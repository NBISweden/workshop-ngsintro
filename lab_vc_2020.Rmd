---
title: "Short Variant Calling"
subtitle: "NGS workflow"
---

```{r,child="assets/header-lab.Rmd"}
```

```{r,eval=TRUE,include=FALSE}
library(yaml)
upid <- yaml::read_yaml("_site.yml")$uppmax_project
upres3 <- yaml::read_yaml("_site.yml")$uppmax_res_3
upres4 <- yaml::read_yaml("_site.yml")$uppmax_res_4
```

# Introduction
Rapidly dropping sequencing costs and the ability to obtain information about the entire genetic code has made whole genome sequening (WGS) a powerful research tool for detecting genetic variation. This workshop will take you thourgh the process of calling short variants (SNPs and indels) in whole geonome sequence data from three samples. 
  
1. The main part of the workshop will guide you through a basic short variant calling workflow for just one sample. The aim is to get familiar with the bam and vcf file formats, and how to interpret the results of variant calling in Integrative Genomics Viewer (IGV).
2. Next, if you have time, we will expand the workflow and perform joint variant calling on three samples. Here we will also give you an idea of how you can combine individual linux commands into a workflow that can be started as an SBATCH script.
3. The last part of the exercise, if you have time, will take you thourgh some additional steps that are recommended by GATK best practise short variant detection. 
  
## Samples
The 1000 Genomes Project was the first project to sequence the entire genomes of a large number of people, to provide a comprehensive resource on human genetic variation. Data from the 1000 Genomes Project available through freely accessible public databases, and in this workshop we will use 3 samples from the low converage phase of the 1000 Genomes project.

Sample        | Population | Coverage
------------- | ---------- | --------
HG00097       | British in England and Scotland    | Low  
HG00100       | British in England and Scotland    | Low  
HG00100       | British in England and Scotland    | Low  
  
## The *LCT* locus
The *LCT* gene on chromosome 2 encodes the lactase protein, which is responible for the metabolism of lactose in mammals. Most mammals can not digest lactose as adults, but some humans tolerate lactose also in adulthood. Genetic variants upstream of the human *LCT* control how lactose is tolerated in adults, and the variant **rs 4988235** (located at at position chr2:136608596 in HG19) has been shown to lead to lactose persistence.  
  
In this workshop we will work with sequencing data (fastq files) for a small region on chromosome 2 that covers the LCT gene and upstream region: 
chr2:136545000-136617000

We will use this region as an example case to illustrate how you can look for genetic variation in NGS data. 
To learn more about the genetic bases for lactose tolerance please read the first three pages of this publication by Mattar et al. The variant (rs4988235) is here referred to as LCT-13910C>T. [Lactose intolerance: diagnosis, genetic, and clinical factors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3401057/pdf/ceg-5-113.pdf).
  
## General guide
<div class="boxy boxy-lightbulb">
* In paths, please replace `<username>` with your actual UPPMAX username.
* In commands, please replace `<parameter>` with the correct parameter, for example your input file name, output file name, directory name, etc.
* Running a command without parameters will often return a help message on how to run the command.
* Copying and pasting commands from the exercise to terminal can result in formatting errors. 
* Use tab completion.
* A line starting with `#` is a comment
* If you change the node you are working on you will need to reload the tool modules. Please see [Accessing programs](#Acessingprograms) below.
* Check that the output file exists and is a reasonable size (use `ls -l`) after a command is completed as a quick way to see that the command went well. 
* A common mistake is to attempt to load input files that do not exist, or create output files where they cannot write.
* Giving good names to your output files that describes what has been done will help you.
* Google errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere.
</p>
</div>
  
# Data {#Data}
All input data for this exercise is located in the folder on Rackham:
```bash
/sw/courses/ngsintro/reseq/data
```
The fastq files are located in this folder:
```bash
/sw/courses/ngsintro/reseq/data/fastq
```
You will also need the reference genome in fasta format. This, and a few other refence files are located in this folder: 
```bash
/sw/courses/ngsintro/reseq/data/ref
```
      
# Main exercise

## Prepare your local computed
You will copy stuff from Uppmax during this exercies, so we need a good workspace in your local computer. 
It is up to you where you want to put this, but it can be for example a folder called "ngsworkflows" on Desktop. 
You need to be able to write to this workspace.
We will use `SCP`to copy files that we have generated on Rackham to this folder. 

## Using Uppmax
Connect to UPPMAX.
```bash
$ ssh -Y username@rackham.uppmax.uu.se
```
Book a node. Make sure you only do this once depeding on the day.  
Use this reservation on day 1 of variant-calling:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("salloc -A ",upid," -t 04:00:00 -p core -n 1 --no-shell --reservation=",upres3," &"))
```
Use this reservation on day 2 of variant-calling:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("salloc -A ",upid," -t 04:00:00 -p core -n 1 --no-shell --reservation=",upres4," &"))
```
Once your job allocation has been granted (should not take long) you can connect to the node using `ssh`. To find out the name of your node, use:
```bash
squeue -u <username>
# Node name is found under nodelist header. You should only see one node here.
```
and to connect to the node:
```bash
ssh -Y <nodename>
```
### Creating a workspace
During this lab you should work in your folder under the course's nobackup folder, just like you have done during the previous labs. Start by creating a workspace for this exercise in your folder, and then move into it.

```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("mkdir /proj/",upid,"/nobackup/<username>/ngsworkflow\n"))
cat(paste0("cd /proj/",upid,"/nobackup/<username>/ngsworkflow"))
```
### Create symbolic links 
The raw fastq files are located in the [Data](#Data) folder described above. In stead of copying the files to your workspace you should create symbolic links (soft-links) to them. Soft-linking files and folders allows us to work them as if they were in in your work directory, but we avoid multiplying them.   
Create a symbolic link to the reference in your workspace:
```bash
ln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta
```
Do the same with the fastq files:
```bash
ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq
ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq
```
  
### Accessing programs {#Accessingprograms}
We will use several programs that are installed in the module system on Uppmax. First load the `bioinfo-tools` module:
```bash
module load bioinfo-tools
```
This makes it possible to load the individual programs we need:
```bash
module load FastQC/0.11.8
module load bwa/0.7.17
module load samtools/1.10
module load picard/2.20.4
module load GATK/4.1.4.1

# You don't HAVE to specify version number when you load a tool, but it is recommended for reproducability if you want to rerun the exact same commands later.
# Picard and GATK are java programs, which means that we need the path to the program file, therefore UPPMAX sets a variable when you load these modules ($GATK_HOME or $PICARD_HOME).
```
  
### Index the reference genome
Tools that compare (short) reads with a (large) reference genome needs genome index files to allow efficient random access to the reference genome. Before we can begin analysing our samples we therefore need to create index files for each tool.  
  
To generate several BWA index files:
```bash
bwa index -a bwtsw human_g1k_v37_chr2.fasta
```
Check to see that several new files have been created using `ls -l`.
  
Generate a Samtool index: 
```bash
samtools faidx human_g1k_v37_chr2.fasta
```
Check to see what file(s) were created using `ls -lrt`.
  
Generate a sequence dictionary for Picard:
```bash
java -Xmx7g -jar $PICARD_HOME/picard.jar CreateSequenceDictionary R=human_g1k_v37_chr2.fasta O=human_g1k_v37_chr2.dict
#You will get a warning about upcoming command line syntax changes for Picard, but you can ignore them.
```
Again, check that the last command generated a new index file using `ls -lrt`
  
### Quality control
![](data/ngs-workflow/main_qc.png)

We will use FastQC to check the qualiy scores of the reads in the fastq. The output is a .html document that shows the quality score along the reads, and other information. 
First create a folder where the output from FastQC will be stored:
```bash
mkdir fastqc 
```
Then run FastQC on the first sample, and direct the output to your new fastqc folder:
```bash
fastqc -q HG00097_1.fq HG00097_2.fq -o fastqc
```

### Download data from Uppmax to your local computer
Uppmax is a compute cluster with great analysis capacity, but when it comes to displaying data graphically it is usually more efficient to dowload the data to your local computer. We will therefore download the .html files generated by FastQC using SCP, and look at them in using a web-browser on your local computer. First create a suitable workspace in your local computer and go into that. 

In your LOCAL computer:
```bash
scp <username>@rackham.uppmax.uu.se:</path/to/fastqc/>*.html .
#Replace <username> with your uppmax user name  and  
#/paht/to/fastqc/ with the full path to the fastqc folder generated in previous step.
```

Open a webbrowser in your local computer and open the two html files that you just dowloaded.
**Questions:**   
* Look at the section "Sequence Length Distribution". How long are the reads?
* Look at the "per base sequence quality". How many bases in the reads have a median phred-score above 28?
* Are the quality scores higher for the first strand reads or the second strand reads?
Phred scores 

### Aligning reads
![](data/ngs-workflow/main_align.png)


We will use `BWA mem` to align the reads to the reference genome. At the same time we add something called *read groups* to the reads in the fastq files. Read groups allow us to trace various technical features, such as which flowcell that was used to generate the reads, and read gruop info needs to be in the file when we perform variant calling with HaplotypeCaller. For a detailed description of read groups, please read this article at [GATK-forum](https://gatkforums.broadinstitute.org/gatk/discussion/6472/read-groups).  
   
The samples in this workshop comes from the 1000 Genomes project, and we don't have all the information needed to create *real* read groups. However, we assume that each fastq file was generated from one library preparation (called "libraryx" below) derived from one biological sample (called "HG00097" below) that was run on one lane of a flowcell (flowcellx_lanex) in the Illumina machine, and we call this "readgroup x". 
  
We parse the output from BWA to samtools sort, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. This saves space since no intermediary sam file is created!  
  
```bash
bwa mem -R "@RG\\tID:readgroupx\\tPU:flowcellx_lanex\\tSM:HG00097\\tLB:libraryx\\tPL:illumina" -t 1 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort > HG00097.bam
# -t 1 is the number of threads to use (the number of cores you booked). If you would have analysed the entire genome more cores and threads would have been necessary.
# You have to use a file redirect ">" for the output, otherwise it will be written to stdout (your screen).
```
Please check that the expected output file was generated and that it has content. 

We also need to index the output bam file so that programs can randomly access the sorted data without reading the whole file. This creates an index file with the same name as the input bam file, except with a .bai extension.

```bash
samtools index HG00097.bam
```
Please check what output file was generated this time. 


### Inspect the bam file with samtools
The bam file is binary so we can not direct read it, but we can view it with `samtools view`. 
The header section of the bam file can be viewed separatedly with the `-H` flag:
```bash
samtools view -H HG00097.bam 
```
Please look at the sam/bam format definition at [Sequence Alignment/Map Format Specification](https://samtools.github.io/hts-specs/SAMv1.pdf).  
**Questions**
* How is your bam file sorted and what does it mean?
* What is encoded in the @SQ tag?
* What is encoded in the @RG tag?

To look at the reads in the bam file just use `samtools view` without the `-H`. This will display the entire bam file whcih is quite large, so if you just want to look at the first 5 lines (for example) you can combine `samtools view` with `head`:
```bash
samtools view HG00097.bam | head -n 5 
```

### Inspect the bam file in IGV
Looking at the aligned reads in samformat can be good in some cases, for example when inspecting reads marked with a specific sam flag, but to get an overview of the data we will use `Integrative Genomics Viewer`. IGV provides a graphical representation of the bam file, and it is best used on your local computer. download the generated bam and bam.bai files to your local computer using `SCP` as above. 




## Variant Calling
![](data/ngs-workflow/main_vc.png)



Now we'll run the GATK `HaplotypeCaller` on our BAM file and output a gVCF file.

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T HaplotypeCaller \
  -R human_g1k_v37_chr2.fasta \
  --emitRefConfidence GVCF \
  -I <input_bam> \
  -o <output>.g.vcf

# The output gVCF file needs to have the ending .g.vcf
# You will have one g.vcf file per sample, so make sure the sample id is in the file name
```

## Further samples

Rerun the workflow for additional samples. If you want to repeat the procedure you can now use your saved commands and rerun the mapping and variant calling steps for at least one more sample (HG00100 or HG00101) from the course directory before continuing with the next step. Alternatively, you can copy the .g.vcf files of the two additional samples.

```bash
cp /sw/courses/ngsintro/vc/data/vcf/<sample>.g.vcf .
```

## Joint genotyping

Now you will call genotypes from all the gVCF files produced in the previous step with `GenotypeGVCFs`. This will create one file with unfiltered variants for all samples (HG00097, HG00100 and HG00101).

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T GenotypeGVCFs \
  -R human_g1k_v37_chr2.fasta \
  --variant <sample1>.g.vcf \
  --variant <sample2>.g.vcf \
  --variant <sample3>.g.vcf \
  -o <raw_variants>.vcf
```

## Filtering Variants

![](data/ngs-workflow/wf_filtration.png)

The next thing we will do is filter the variants. For a large dataset with many variants it is recommended to use GATK's VQSR technique for training filter thresholds on known variants. Since we are working with a small data set we're going to use hard filters and [parameters suggested by GATK.](https://software.broadinstitute.org/gatk/documentation/article?id=2806)

The parameters are slightly different for SNPs and INDELs, which we have called together and are both in our VCF. You therefore need to perform the filtering in two separate steps: first select all SNPs and filter them, then select all indels and filter them.

#### SNPs

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T SelectVariants \
  -R human_g1k_v37_chr2.fasta \
  -V <raw_variants>.vcf \
  -selectType SNP \
  -o <raw_snps>.vcf

java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T VariantFiltration \
  -R human_g1k_v37_chr2.fasta \
  -V <raw_snps>.vcf \
  -o <filtered_snps>.vcf \
  --filterExpression "QD < 2.0" --filterName QDfilter \
  --filterExpression "MQ < 40.0" --filterName MQfilter \
  --filterExpression "FS > 60.0" --filterName FSfilter

# Each filterName option has to immediately follow the filterExpression it matches.
# Do not worry about the warnings here, but if you have time, try to find out why you get these warnings!
```

#### Indels

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T SelectVariants \
  -R human_g1k_v37_chr2.fasta \
  -V <raw_variants>.vcf \
  -selectType INDEL \
  -o <raw_indels>.vcf

java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T VariantFiltration \
  -R human_g1k_v37_chr2.fasta \
  -V <raw_indels>.vcf \
  -o <filtered_indels>.vcf \
  --filterExpression "QD < 2.0" \
  --filterName QDfilter \
  --filterExpression "FS > 200.0" \
  --filterName FSfilter
```

### Merge SNPs and Indels

Once you have the filtered calls for both SNPs and INDELs, you can combine these using `CombineVariants`:

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T CombineVariants \
  -R human_g1k_v37_chr2.fasta \
  --variant:snp <filtered_snps>.vcf \
  --variant:indel <filtered_indels>.vcf \
  -o <my_variants_filtered>.vcf \
  -genotypeMergeOptions PRIORITIZE -priority snp,indel

```
Open your filtered VCF with `less` and page through it. It still has all the variant lines, but the FILTER column that was blank before is now filled in, with PASS or a list of the filters it failed. Note also that the filters that were run are described in the header section.

# Analyse your results

There are several tools you can use to analyse your results, once you have your multi-sample vcf file. You can get various statistics, add annotations, view the alignments etc.

## vcftools

[Vcftools](https://vcftools.github.io/man_latest.html) can be used to work with vcf files, either for manipulation or getting statistics. We can use vcftools to remove all variants that did not pass our filters:

```bash
module load vcftools/0.1.15

vcftools --vcf <my_variants_filtered>.vcf \
  --out <output_prefix> \
  --remove-filtered-all \
  --recode --recode-INFO-all
# --recode is used to create a new vcf file as output, otherwise only statistics are calculated.
```

<i class="fas fa-comments"></i> You can now try to answer these questions:

* How many variants passed the filters? (Check the log file)  
* Try to find out how many variants that are indels (Hint: `man vcftools` will list available options.)

## Variant annotation

![](data/ngs-workflow/wf_annotation.png)

There are several tools for variant annotation. Some of the most frequently used are VEP (Variant Effect Predictor), SnpEff and Annovar. We will try [Annovar](http://annovar.openbioinformatics.org/en/latest/). Note that these tools are predictors and may give slightly different results. The output is also dependent on your choice of gene set.

Load the module.

```bash
module load annovar/2017.07.16
```

Annovar uses downloaded annotation tracks to annotate the input file. These can be downloaded using Annovar, however many annotation tracks are already downloaded in the uppmax installation of annovar, and will be used in this lab.

```bash
#$ANNOVAR_HOME is set when annovar module is loaded. To see which datasets are already downloaded
# you can use
ls $ANNOVAR_HOME/humandb/
```

All datasets available through Annovar can be found [here](http://annovar.openbioinformatics.org/en/latest/user-guide/download/).

Annovar can use a vcf file as input and also output the results as a vcf file. However, to make the results easier to read we will first create an annovar formatted file and produce tab delimited results.

```bash
convert2annovar.pl -format vcf4 -allsample -withfreq <vcffile> > <annovarfile>
```

Now you are ready to annotate your variants. Here, we will use RefSeq genes, dbSNP-ids and allele frequencies from 1000 genomes.

```bash
# Use table_annovar.pl -h to find all options

table_annovar.pl <annovarfile> $ANNOVAR_HOME/humandb -buildver hg19 -outfile <myanno> -remove -protocol refGene,snp138,1000g2015aug_all,1000g2015aug_eur,1000g2015aug_afr -operation g,f,f,f,f

# buildver: Should be the same build as the reference used for alignment.
# protocol: Datasets used for annotation
# operation: Are they gene (g), filter (f) or region (r) based annotations?
```

Your results can be found in `<myanno>.hg19_multianno.txt.`

<i class="fas fa-comments"></i> You can now try to answer these questions:

* What is the position of the variant that leads to retained lactase activity in adults? It is refered to as rs4988235 in dbSNP.
* Is the variant located within a gene, if so, which gene? (What effect could this have on the LCT gene?)
* What are the allele frequencies of the variant in different populations?
* What is the allele frequency in the Swedish population? See the [SweGen Variant Frequency Browser](https://swegen-exac.nbis.se)
* Are there any coding (exonic) variants in the LCT gene?
* Add one or more datasets and rerun the annotation!

## Using IGV

Next, we want to look at the data. For that, we will use IGV (Integrative Genomics Viewer). You can launch it from UPPMAX using your graphical forwarding, `-X` or `-Y`, as you may have already tried, see [here](http://www.uppmax.uu.se/support-sv/user-guides/integrative-genomics-viewer--igv--guide/). However, here we will show how to run it on your local machine.

Go to the IGV [download page](https://software.broadinstitute.org/software/igv/download), and follow the instructions. If you have not downloaded IGV before, it will prompt you to fill in some information and agree to license. Launch the viewer through webstart. The 1.2 Gb version should be sufficient for our data.

**Download data**

We need to download some data to our local machines so IGV can find it. Open a new terminal, but do not log in to UPPMAX. You may want to create a new folder for the exercise (cd to the new folder). Now we’re going to use the command `scp` (secure copy) to copy .bam (and .bai) and vcf files. **\<username\> is your UPPMAX username.** It will prompt you for your UPPMAX password, then it should download your files.
You will need the bam files of all 3 samples: HG00097, HG00100 and HG00101.

```{r,echo=FALSE,comment="",class.output="bash"}
cat("# copy bam files\n")
cat(paste0("scp <username>@rackham.uppmax.uu.se:/sw/courses/ngsintro/vc/HG00* ./\n"))
cat("# copy vcf file\n")
cat(paste0("scp <username>@rackham.uppmax.uu.se:/sw/courses/ngsintro/vc/filtered_variants.vcf ./\n"))

```

If you are missing some of the BAM files or the VCF you can do this instead:

```bash
# Copy the files from the course directory:
scp <username>@rackham.uppmax.uu.se:/sw/courses/ngsintro/vc/data/bam/<sample>.dedup.recal.ba* ./
scp <username>@rackham.uppmax.uu.se:/sw/courses/ngsintro/vc/data/vcf/my_variants_filtered.vcf ./
```

**Choose reference**

First we want to make sure we have the right reference, i.e. the same reference used for mapping the reads.

* In IGV, go to the popup menu in the upper left and set it to **Human hg19**.

**Prepare BAMs**

* Go under the Tools menu and select **Run igvtools…**. Choose the command **Count** and then use the Browse button next to the Input File line to select the BAMs (not the bai) that you just downloaded. It will autofill the output file.
* Now hit the Run button. This generates a .tdf file for each BAM. This allows us to see the coverage value for our BAM file even at zoomed out views.

**Load BAM & VCF**

Close the **igvtools** window and go back to the File menu, select **Load from File…** and select your BAMs (not the .bai or the .tdf), which should appear in the tracks window. Next, load the vcf file. You will have to zoom in before you can see any reads. You can either select a region by click and drag, or by typing a region (chr:from-to) or gene name in the text box at the top.

<i class="fas fa-comments"></i> You can now try to answer these questions:

* What is the read length?
* Can you find the variant shown to be associated with lactose tolerance?
* Do you have coverage in this region in all samples?
* Which samples do you think should avoid drinking milk? Any heterozygotes?
* Does this agree with the genotype in the vcf file?

**Customizing the view**

* Change the track features. If you control-click (or right click) on the track name at the left, you will get a popup menu with several options. For example, click the gene track and play with the view.

* If you go to a read alignment track, you can control some useful features of the display. One is how you color the reads and another is the grouping. Are there any colored reads? Why?

* Go under the View menu and select Preferences. Click on the Alignments tab. There are a number of things we can configure. Feel free to play with them.

# Extra labs

Please continue here if you have time left!

## UCSC Genome Browser

The UCSC Genome Browser is a collection of tools for viewing and manipulating genomic data from public databases. In this extra lab we will upload our filtered variants to the UCSC genome browser, and look at evolutionary conservation of the genomic region surrounding the lactase persistent allele. We also encourage you to explore other types of data available in the UCSC Genome Browser. Plenty of documentation is available about UCSC Genome Browser, so please look at these sites to learn more:  

* http://www.genome.ucsc.edu/training/index.html  
* http://www.genome.ucsc.edu/training/ucscGeneFishing.pdf  

1. Go to https://genome.ucsc.edu.

2. Click on **Genomes** in the upper menu and select **Human GRCh37/hg19**. This will take you to the version of the human reference assembly that we have been working with.

3. Click on **My Data** in the upper menu, select **Custom Tracks**, and click on **add custom tracks** on the next page.

4. Click on the **Choose File** button to upload data, browse to the vcf file with filtered variants on your local computer, and press **Submit**. Then click **go** to view the uploaded variants in the Genome Browser.

5. Zoom in to the relevant part of the genome. We only have data for a small region on chromosome 2 surrounding the *LCT* gene. An easy way to find the relevant region is to type **LCT** in the search window on the top of the page, press **go**, and then select **LCT (uc002tuu.1) at chr2:136545415-136594750 - Homo sapiens lactase (LCT), mRNA** under **UCSC Genes**.Then use the zoom out buttons to view both the *LCT* gene and the *MCM6* gene.

6. Zoom in to the variant that leads to retained lactase activity in adults (rs4988235).  Look at the **Multiz Alignment of 100 Vertebrates** track that shows evolutionary conservation of this genomic region across 100 vertebrates. (It is possible to directly enter the rs-number in the search window.)  

<div class="boxy boxy-comments">
You can try to answer these questions:

* Which species appear to have the DNA sequence surrounding the rs4988235 variant in their genomes?  
* What allele does these species have at the variant position?  

</div>

7. Play around with the various annotations tracks that are available in the UCSC genome browser and explore what is known about this particular region of the human genome. All tracks can be displayed in different modes, from **full** to **hide** and this regulates at what detail the track will be displayed.

## BEDTools

[BEDTools](http://bedtools.readthedocs.org/en/latest/) is a [collection of  tools](http://bedtools.readthedocs.io/en/latest/content/bedtools-suite.html) used for many different tasks in genomics. It supports different input formats, including .bam, .bed, .gff or vcf. The [bed format](http://genome.ucsc.edu/FAQ/FAQformat) is central; each line describes a feature, where the three first columns are required (chromosome, start coord, end coord) and the rest describe the feature (e.g. a gene). We will try the **intersect** tool:

```bash
module load BEDTools/2.27.1

#Copy this bed file from the course directory and look at it using e.g. less
cp /sw/courses/ngsintro/vc/data/genes.bed .

bedtools intersect -wo -a <vcffile> -b <bedfile> > <subset.vcf>
```

<i class="fas fa-comments"></i> You can now try to answer these questions:

* How many variants are found within the given gene regions? (Hint: use `wc -l` to count the lines in your output file.)
* How would you find all variants that are *NOT* in genes? (Look at the available options using bedtools intersect `-h`)

## Samtools & bit flag

Viewing duplicates and understanding the bit flag. We can look at the duplicates that we marked with Picard above, using a filter on the bit flag. The mark for duplicates is the bit for 1024, we can use samtools view to look at them. First go to [this online utility](https://broadinstitute.github.io/picard/explain-flags.html) that is helpful for decoding sam flags. Notice that it can also be used in reverse to find the appropriate flag. Now look at your data.

```bash
# We can count the marked reads, with samtools and the -c option.
samtools view -f 1024 -c <marked_bam>

# Or if we want to look at these reads:
samtools view -f 1024 <marked_bam> | less
```

<i class="fas fa-comments"></i> You can now try to answer these questions:

* Do you get the same number of duplicates as in the metrics file?
* Why do we use samtools to look at the BAM file? Could we have looked at it with just less?

## VEP

Variant Effect Predictor ([VEP](https://www.ensembl.org/info/docs/tools/vep/index.html)) is another popular tool for variant annotation.

```bash
module load vep/92
#To see which cache is default use
#echo $VEP_CACHE

vep --cache --dir_cache $VEP_CACHE --assembly GRCh37 --offline --force_overwrite --symbol --af_1kg -i <vcffile> -o <outfile>
# To list all options see: http://www.ensembl.org/info/docs/tools/vep/script/vep_options.html
```

Do you get the same annotations as with annovar?

# Additional information

* Read e.g. the first three pages of [Matter *el.al.*, 2012](http://dx.doi.org/10.2147/CEG.S32368) to find out more details about the lactase persistence allele. The variant (rs4988235) is here referred to as *LCT*-13910C>T.

* Here is a technical documentation of [Illumina Quality Scores](https://www.illumina.com/documents/products/technotes/technote_understanding_quality_scores.pdf)

* Tools used or referenced

  * [BWA](http://bio-bwa.sourceforge.net/bwa.shtml)
  * [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
  * [MultiQC](http://multiqc.info/)
  * [Picard](https://broadinstitute.github.io/picard/command-line-overview.html)
  * [GATK](https://software.broadinstitute.org/gatk/)
  * [samtools](http://www.htslib.org/)

***
