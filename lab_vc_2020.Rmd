---
title: "Short Variant Calling"
subtitle: "NGS workflow"
---

```{r,child="assets/header-lab.Rmd"}
```

```{r,eval=TRUE,include=FALSE}
library(yaml)
upid <- yaml::read_yaml("_site.yml")$uppmax_project
upres3 <- yaml::read_yaml("_site.yml")$uppmax_res_3
upres4 <- yaml::read_yaml("_site.yml")$uppmax_res_4
```

# Introduction
Rapidly dropping sequencing costs and the ability to obtain information about the entire genetic code has made whole genome sequening (WGS) a powerful research tool for detecting genetic variation. This workshop will take you thourgh the process of calling short variants (SNPs and indels) in whole geonome sequence data from three samples. 
  
1. The first part of the workshop will guide you through a basic short variant calling workflow for just one sample. The aim is to get familiar with the bam and vcf file formats, and how to interpret the results of variant calling in Integrative Genomics Viewer (IGV).
2. Next, if you have time, we will expand the workflow and perform joint variant calling on three samples. Here we will also give you an idea of how you can combine individual linux commands into a workflow that can be started as an SBATCH script.
3. The last part of the exercise, if you have time, will take you thourgh some additional steps that are recommended by GATK best practise short variant detection. 
  
## Samples
The 1000 Genomes Project was the first project to sequence the entire genomes of a large number of people, to provide a comprehensive resource on human genetic variation. Data from the 1000 Genomes Project available through freely accessible public databases, and in this workshop we will use 3 samples from the low converage phase of the 1000 Genomes project.

Sample        | Population | Coverage
------------- | ---------- | --------
HG00097       | British in England and Scotland    | Low  
HG00100       | British in England and Scotland    | Low  
HG00100       | British in England and Scotland    | Low  
  
## The *LCT* locus
The *LCT* gene on chromosome 2 encodes the lactase protein, which is responible for the metabolism of lactose in mammals. Most mammals can not digest lactose as adults, but some humans tolerate lactose also in adulthood. Genetic variants upstream of the human *LCT* control how lactose is tolerated in adults, and the variant **rs 4988235** (located at at position chr2:136608596 in HG19) has been shown to lead to lactose persistence.  
  
In this workshop we will work with sequencing data (fastq files) for a small region on chromosome 2 that covers the LCT gene and upstream region: 
chr2:136545000-136617000

We will use this region as an example case to illustrate how you can look for genetic variation in NGS data. 
To learn more about the genetic bases for lactose tolerance please read the first three pages of this publication by Mattar et al. The variant (rs4988235) is here referred to as LCT-13910C>T. [Lactose intolerance: diagnosis, genetic, and clinical factors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3401057/pdf/ceg-5-113.pdf).
  
## General guide
<div class="boxy boxy-lightbulb">
* In paths, please replace `<username>` with your actual UPPMAX username.
* In commands, please replace `<parameter>` with the correct parameter, for example your input file name, output file name, directory name, etc.
* Running a command without parameters will often return a help message on how to run the command.
* Copying and pasting commands from the exercise to terminal can result in formatting errors. 
* Use tab completion.
* A line starting with `#` is a comment
* If you change the node you are working on you will need to reload the tool modules. Please see [Accessing programs](#Acessingprograms) below.
* Check that the output file exists and is a reasonable size (use `ls -l`) after a command is completed as a quick way to see that the command went well. 
* A common mistake is to attempt to load input files that do not exist, or create output files where they cannot write.
* Giving good names to your output files that describes what has been done will help you.
* Google errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere.
</p>
</div>
  
# Data {#Data}
All input data for this exercise is located in the folder on Rackham:
```bash
/sw/courses/ngsintro/reseq/data
```
The fastq files are located in this folder:
```bash
/sw/courses/ngsintro/reseq/data/fastq
```
You will also need the reference genome in fasta format. This, and a few other refence files are located in this folder: 
```bash
/sw/courses/ngsintro/reseq/data/ref
```
      
# Preparations

## Create a local workspace on your laptop {#preparelaptop}
The majority of the analyses in this workshop will be done on Uppmax, but we will copy some of the resulting files to our laptops so that we can look at them in graphical browsers. Therefore, please start by creating a folder for this workshop on your local laptop. 
It is up to you where you want to put this, but it can for example be a folder called "ngsworkflow" on Desktop. You need to have write permission in this folder. The folder you create here will be refered to as **local workspace** throughout this workshop. 

## Using Uppmax
Connect to UPPMAX.
```bash
$ ssh -Y username@rackham.uppmax.uu.se
```
Book a node. Make sure you only do this once depeding on the day.  
Use this reservation on day 1 of variant-calling:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("salloc -A ",upid," -t 04:00:00 -p core -n 1 --no-shell --reservation=",upres3," &"))
```
Use this reservation on day 2 of variant-calling:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("salloc -A ",upid," -t 04:00:00 -p core -n 1 --no-shell --reservation=",upres4," &"))
```
Once your job allocation has been granted (should not take long) you can connect to the node using `ssh`. To find out the name of your node, use:
```bash
squeue -u <username>
# Node name is found under nodelist header. You should only see one node here.
```
and to connect to the node:
```bash
ssh -Y <nodename>
```
  
### Create a workspace on Uppmax
During this lab you should work in your folder under the course's nobackup folder, just like you have done during the previous labs. Start by creating a workspace for this exercise in your folder, and then move into it. This folder will be refered to as your *uppmax workspace* throughout this workshop.  

```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("mkdir /proj/",upid,"/nobackup/<username>/ngsworkflow\n"))
cat(paste0("cd /proj/",upid,"/nobackup/<username>/ngsworkflow"))
```
  
### Create symbolic links to data
The raw fastq files are located in the [Data](#Data) folder described above. In stead of copying the files to your workspace you should create symbolic links (soft-links) to them. Soft-linking files and folders allows us to work them as if they were in in your work directory, but we avoid multiplying them.   
Create a symbolic link to the reference in your workspace:
```bash
ln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta
```
Do the same with the fastq files:
```bash
ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq
ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq
```
  
### Accessing programs {#Accessingprograms}
We will use several programs that are installed in the module system on Uppmax. First load the `bioinfo-tools` module:
```bash
module load bioinfo-tools
```
This makes it possible to load the individual programs we need:
```bash
module load FastQC/0.11.8
module load bwa/0.7.17
module load samtools/1.10
module load picard/2.20.4
module load GATK/4.1.4.1

# You don't HAVE to specify version number when you load a tool, but it is recommended for reproducability if you want to rerun the exact same commands later.
# Picard and GATK are java programs, which means that we need the path to the program file, therefore UPPMAX sets a variable when you load these modules ($GATK_HOME or $PICARD_HOME).
```
  
## Index the reference genome
Tools that compare (short) reads with a (large) reference genome needs genome index files to allow efficient random access to the reference genome. Before we can begin analysing our samples we therefore need to create index files for each tool.  
  
To generate several BWA index files:
```bash
bwa index -a bwtsw human_g1k_v37_chr2.fasta
```
Check to see that several new files have been created using `ls -l`.
  
Generate a Samtool index: 
```bash
samtools faidx human_g1k_v37_chr2.fasta
```
Check to see what file(s) were created using `ls -lrt`.
  
Generate a sequence dictionary for Picard:
```bash
java -Xmx7g -jar $PICARD_HOME/picard.jar CreateSequenceDictionary R=human_g1k_v37_chr2.fasta O=human_g1k_v37_chr2.dict
#You will get a warning about upcoming command line syntax changes for Picard, but you can ignore them.
```
Again, check that the last command generated a new index file using `ls -lrt`

# Worshop part one
## Quality control
![](data/ngs-workflow/main_qc.png)

We will use FastQC to check the qualiy scores of the reads in the fastq. The output is a .html document that shows the quality score along the reads, and other information. 
First create a folder where the output from FastQC will be stored:
```bash
mkdir fastqc 
```
Then run FastQC on the first sample, and direct the output to your new fastqc folder:
```bash
fastqc -q HG00097_1.fq HG00097_2.fq -o fastqc
```
  
### Look at the data on your laptop
Uppmax is a compute cluster with great analysis capacity, but when it comes to displaying data graphically it is usually more efficient to dowload the data to your local computer. We will therefore download the .html files generated by FastQC using SCP, and look at them in using a web-browser on your laptop. 

Open a new terminal and navigate to the the workspace you created on your laptop [above](#preparelaptop). Do not log in to UPPMAX. Copy the .html files generated above with this command:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("scp <username>@rackham.uppmax.uu.se:/proj/",upid,"/nobackup/<username>/ngsworkflow/fastqc/*.html ."))
#Replace <username> with your uppmax user name
```
Check that the files are now present in the workspace folder on your laptop downladed using `ls -lrt`. 

Open a webbrowser in your local computer and open the two html files that you just dowloaded.  
**Questions:**   
* Look at the section "Sequence Length Distribution". How long are the reads?
* Look at the "per base sequence quality". How many bases in the reads have a median phred-score above 28?
* Are the quality scores higher for the first strand reads or the second strand reads?
Phred scores 

## Aligning reads
![](data/ngs-workflow/main_align.png)


We will use `BWA mem` to align the reads to the reference genome. At the same time we add something called *read groups* to the reads in the fastq files. Read groups allow us to trace various technical features, such as which flowcell that was used to generate the reads, and read gruop info needs to be in the file when we perform variant calling with HaplotypeCaller. For a detailed description of read groups, please read this article at [GATK-forum](https://gatkforums.broadinstitute.org/gatk/discussion/6472/read-groups).  
   
The samples in this workshop comes from the 1000 Genomes project, and we don't have all the information needed to create *real* read groups. However, we assume that each fastq file was generated from one library preparation (called "libraryx" below) derived from one biological sample (called "HG00097" below) that was run on one lane of a flowcell (flowcellx_lanex) in the Illumina machine, and we call this "readgroup x". 
  
We parse the output from BWA to samtools sort, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. This saves space since no intermediary sam file is created!  
  
```bash
bwa mem -R "@RG\\tID:readgroupx\\tPU:flowcellx_lanex\\tSM:HG00097\\tLB:libraryx\\tPL:illumina" -t 1 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort > HG00097.bam
# -t 1 is the number of threads to use (the number of cores you booked). If you would have analysed the entire genome more cores and threads would have been necessary.
# You have to use a file redirect ">" for the output, otherwise it will be written to stdout (your screen).
```
Please check that the expected output file was generated and that it has content. 

We also need to index the output bam file so that programs can randomly access the sorted data without reading the whole file. This creates an index file with the same name as the input bam file, except with a .bai extension.

```bash
samtools index HG00097.bam
```
Please check what output file was generated this time. 

### Inspect the bam file with samtools
The bam file is binary so we can not direct read it, but we can view it with `samtools view`. 
The header section of the bam file can be viewed separatedly with the `-H` flag:
```bash
samtools view -H HG00097.bam 
```
Please look at the sam/bam format definition at [Sequence Alignment/Map Format Specification](https://samtools.github.io/hts-specs/SAMv1.pdf).  
**Questions**  
* How is your bam file sorted and what does it mean?  
* What is encoded in the @SQ tag?  
* What is encoded in the @RG tag?  
  
To look at the reads in the bam file just use `samtools view` without the `-H`. This will display the entire bam file whcih is quite large, so if you just want to look at the first 5 lines (for example) you can combine `samtools view` with `head`:
```bash
samtools view HG00097.bam | head -n 5 
```
  
### Inspect the bam file in IGV on your laptop
#### Install IGV
Integrative Genomics Viewer (IGV) provides an interactive visualisation of the reads in a bam file. Here we will show how to run IGV on your laptop. If you have not used IGV on your laptop before, plase go to the IGV [download page](https://software.broadinstitute.org/software/igv/download), and follow the instructions to dowload it. It will prompt you to fill in some information and agree to license. Launch the viewer through webstart. The 1.2 Gb version should be sufficient for our data.

##### Dowload the bam file
You also need to download the bam file to your laptop. 
Navigate to your *local workspace*, but do not log in to UPPMAX. Copy the .bam file generated in your *uppmax workspace* with this command:
```{r,echo=FALSE,comment="",class.output="bash"}
cat(paste0("scp <username>@rackham.uppmax.uu.se:/proj/",upid,"/nobackup/<username>/ngsworkflow/HG00097.bam ."))
#Replace <username> with your uppmax user name
```
Check that the files are now present in your *local workspace* using `ls -lrt`. 

#### Look at the bam file in IGV
* In IGV, go to the popup menu in the upper left and set it to **Human hg19**.
* Go under the Tools menu and select **Run igvtools…**. Choose the command **Count** and then use the Browse button next to the Input File line to select the BAMs (not the bai) that you just downloaded. It will autofill the output file. Hit the Run button. This generates a .tdf file for each BAM. This allows us to see the coverage value for our BAM file even at zoomed out views.
* Close the **igvtools** window and go back to the File menu, select **Load from File…** and select your BAMs (not the .bai or the .tdf), which should appear in the tracks window. You will have to zoom in before you can see any reads. You can either select a region by click and drag, or by typing a region (chr:from-to) or gene name in the text box at the top.
*Questions*
* What is the read length?
* Can you estimate the coverage by looking at the bam file in IGV? 
* 




## Variant Calling
![](data/ngs-workflow/main_vc.png)



Now we'll run the GATK `HaplotypeCaller` on our BAM file and output a gVCF file.

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T HaplotypeCaller \
  -R human_g1k_v37_chr2.fasta \
  --emitRefConfidence GVCF \
  -I <input_bam> \
  -o <output>.vcf

# The output gVCF file needs to have the ending .g.vcf
# You will have one g.vcf file per sample, so make sure the sample id is in the file name
```

## Further samples

Rerun the workflow for additional samples. If you want to repeat the procedure you can now use your saved commands and rerun the mapping and variant calling steps for at least one more sample (HG00100 or HG00101) from the course directory before continuing with the next step. Alternatively, you can copy the .g.vcf files of the two additional samples.

```bash
cp /sw/courses/ngsintro/vc/data/vcf/<sample>.g.vcf .
```

## Joint genotyping

Now you will call genotypes from all the gVCF files produced in the previous step with `GenotypeGVCFs`. This will create one file with unfiltered variants for all samples (HG00097, HG00100 and HG00101).

```bash
java -Xmx7g -jar $GATK_HOME/GenomeAnalysisTK.jar -T GenotypeGVCFs \
  -R human_g1k_v37_chr2.fasta \
  --variant <sample1>.g.vcf \
  --variant <sample2>.g.vcf \
  --variant <sample3>.g.vcf \
  -o <raw_variants>.vcf
```



# Additional information


* Here is a technical documentation of [Illumina Quality Scores](https://www.illumina.com/documents/products/technotes/technote_understanding_quality_scores.pdf)

* Tools used or referenced

  * [BWA](http://bio-bwa.sourceforge.net/bwa.shtml)
  * [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)
  * [MultiQC](http://multiqc.info/)
  * [Picard](https://broadinstitute.github.io/picard/command-line-overview.html)
  * [GATK](https://software.broadinstitute.org/gatk/)
  * [samtools](http://www.htslib.org/)

