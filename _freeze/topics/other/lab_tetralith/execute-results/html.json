{
  "hash": "bbaaaf3e1101aea23eff88f929176f51",
  "result": {
    "markdown": "---\ntitle: \"Alternative cluster instructions\"\nsubtitle: \"\"\nauthor: \"Malin Larsson\"\nformat: html\n---\n\n\n\n\n\n\n\n\n## Connect to NSC via ThinLinc\n\nThe ThinLinc client can be downloaded for free from http://www.cendio.com/downloads/clients/. It is available for Windows, Mac OS X, Linux and Solaris.\n\nTo use ThinLinc to connect to Tetralith:\n\n1. If you haven't already done so, download the ThinLinc client matching your local computer (i.e Windows, Linux, MacOS X or Solaris) and install it.\n2. Start the client.\n3. Change the \"Server\" setting to \"tetralith.nsc.liu.se\".\n4. Change the \"Name\" setting to your Tetralith username (e.g x_abcde).\n5. You do not need to change any other settings.\n6. Enter your cluster Tetralith password in the \"Password\" box.\n7. Press the \"Connect\" button.\n8. Enter the 6-digit code generated by the two-factor authentication app on your phone.\n9. If you connect for the first time, you will see the \"The server's host key is not cached ...\" dialog. Verify that the fingerprint shown on your screen matches the one listed below! If it does not match, press Abort and then contact NSC Support!\n\n**Tetralith SSH server host key fingerprint: 20:19:f4:6b:38:d6:e7:ac:e6:7c:8e:38:0a:7f:34:dc**\n\nAfter a few seconds, a window with a simple desktop session in it will appear. From the Applications menu, start a Terminal Window.\n\n## Interactive session\n\nStart an interactive session with three compute cores for todays lab:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\ninteractive -A snic2023-xx-xxxx -t 04:00:00 -n 3\n```\n:::\n:::\n\n\nPlease adjust the requested time depending on what lab you will be working on. After a minute or so you should have gotten your interactive job.\n\n::: {.callout-tip}\nNote that your terminal prompt changed from `<nsc_username>@tetralith$` to something like `<nsc_username>@n424$` (or another node name), which means that you are now running on one of the compute nodes.\n:::\n\n## UPPMAX singularity container\n\nWe will use a singularity container (a virtual computer) that mimics the UPPMAX computing environment. Once you have started the singularity container your environment will look exactly as on UPPMAX, and the software used in this workshop will be available through the module system inside the container.\nUse this command to start the singularity container:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\nsingularity shell -B /proj/snic2023-xx-xxxx/users:/proj/snic2023-xx-xxxx/nobackup -B /proj/snic2023-xx-xxxx /proj/snic2023-xx-xxxx/ngsintro.sif\n```\n:::\n:::\n\n\n::: {.callout-tip}\nYour terminal prompt changed to something like `<nsc_username>@offline-uppmax$`. This means that you have moved into a \"virtual computer\" that mimics the UPPMAX environment.\n:::\n\nIn the singularity container type this to make the module system behave properly:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\nsource /uppmax_init\n```\n:::\n:::\n\n\nEverything from this point and onwards should be identical to running the exercise on UPPMAX.\nTo close the singularity container  later on just type `exit`in the terminal, but don't do that now.\n\n::: {.callout-note}\n**Note:** Since this is not actually running on uppmax, none of the queue system commands (`squeue`, `sbatch`, `jobinfo` etc) will work. \n:::\n\n## Cluster workspace\n\nWhile running the UPPMAX singularity container, create a workspace for this exercise. This will be the \"cluster workspace\" in which you should perform the analyses, as if you would have worked on UPPMAX.\n\nThe name of the workspace depends on what lab you are working on. For example, if you are working on the introduction to Linux lab then call the workspace \"linux_tutorial\". Once the workspace is created you should go into it.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\nmkdir /proj/snic2023-xx-xxxx/nobackup/<nsc_username>/linux_tutorial\ncd /proj/snic2023-xx-xxxx/nobackup/<nsc_username>/linux_tutorial\n```\n:::\n:::\n\n\nWhen you have the UPPMAX singularity container up and running you can follow regular lab instructions, except that you should not connect to UPPMAX or book a node, and you should work in the cluster workspace defined here. Also, IGV is started with a special command,see below.\n\n## IGV\n\nTo start IGV at NSC type this in the command:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n/proj/snic2023-xx-xxxx/igv/igv.sh\n```\n:::\n:::\n\n\nYou can use the same command both when running the UPPMAX singularity container and from a normal terminal window at NSC.\n\n## Exit the UPPMAX singularity container\n\nWhen you are done with the lab just type exit to close the singularity container:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\noffline-uppmax$ exit\n```\n:::\n:::\n\n\nAll files and folders that you create in /proj/snic2023-xx-xxxx/nobackup/nsc_username/ while running the singularity container can be reached also from outside of the container, in this folder on Tetralith:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n/proj/snic2023-xx-xxxx/users/<nsc_username>/\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}