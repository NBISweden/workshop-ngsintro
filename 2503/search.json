[
  {
    "objectID": "topics/vc/lab_vc_tetralith.html",
    "href": "topics/vc/lab_vc_tetralith.html",
    "title": "Variant Calling Workflow",
    "section": "",
    "text": "Whole genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. This workshop will take you through the process of calling germline short variants (SNVs and INDELs) in WGS data from three human samples.\n\nThe first part of the workshop will guide you through a basic variant calling workflow in one sample. The goals are that you should get familiar with the bam and vcf file formats, and learn how to interpret the results of variant calling in Integrative Genomics Viewer (IGV).\nIf you have time, the next part of the workshop will show you how to perform joint variant calling in three samples. The goals here is that you should be able to interpret multi-sample vcf files and explain the differences between the g.vcf and vcf file formats. Also, if you are interested in programming, another goal is that you should learn to combine individual Linux commands into an SBATCH script.\n\nIf you have time, the last part of the workshop will take you through the GATK best practices for germline short variant detection in three samples. The goal here is that you should learn how to use GATK’s documentation so that you can analyze your own samples in the future.\n\nGeneral guide\n\n\n\n\n\n\nNote\n\n\n\n\nIn this workshop you will work on the computing cluster Tetralith at NSC.\nYou will use a singularity container (a virtual computer) that mimics the UPPMAX environment.\nIn paths, please replace &lt;nsc_username&gt; with your NSC username.\nIn commands, please replace &lt;parameter&gt; with the correct parameter, for example your input file name, output file name, directory name, etc.\nDo not copy and paste commands from the exercise to terminal, as this can result in formatting errors.\nUse tab completion.\nA line starting with # is a comment\nRunning a command without parameters will often return a help message on how to run the command.\nAfter a command is completed, please check that the desired output file was generated and that it has a reasonable size (use ls -l).\nA common mistake is to attempt to load input files that do not exist, or create output files where you don’t have permission to write.\nUse output file names that describes what was done in the command.\nIf you change the node you are working on you will need to reload the tool modules.\nGoogle errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#samples",
    "href": "topics/vc/lab_vc_tetralith.html#samples",
    "title": "Variant Calling Workflow",
    "section": "Samples",
    "text": "Samples\nThe 1000 Genomes Project ran between 2008 and 2015, creating the largest public catalogue of human variation and genotype data. In this workshop we will use low coverage whole genome sequence data from three individuals, generated in the first phase of the 1000 Genomes Project.\n\n\n\nSample\nPopulation\nSequencing technology\n\n\n\n\nHG00097\nBritish in England and Scotland\nLow coverage WGS\n\n\nHG00100\nBritish in England and Scotland\nLow coverage WGS\n\n\nHG00101\nBritish in England and Scotland\nLow coverage WGS"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#Genomicregion",
    "href": "topics/vc/lab_vc_tetralith.html#Genomicregion",
    "title": "Variant Calling Workflow",
    "section": "Genomic region",
    "text": "Genomic region\nThe LCT gene on chromosome 2 encodes the enzyme lactase, which is responsible for the metabolism of lactose in mammals. Most mammals can not digest lactose as adults, but some humans can. Genetic variants upstream of the LCT gene lead to lactase persistence, which means that lactase is expressed also in adulthood and the carrier can continue to digest lactose. The variant rs4988235, located at position chr2:136608646 in the HG19 reference genome, has been shown to lead to lactose persistence. The alternative allele (A on the forward strand and T on the reverse strand) creates a new transcription factor binding site that enables continued expression of the gene after weaning.\nIn this workshop we will use sequencing data for the region chr2:136545000-136617000 chr2:136,039,147-136,662,073 in the 3 samples listed above to illustrate variant calling in NGS data. We will use chromosome 2 from hg19 as reference genome.\nFor those interested in the details of the genetic bases for lactose tolerance, please read the first three pages of Lactose intolerance: diagnosis, genetic, and clinical factors by Mattar et al. The variant rs4988235 is here referred to as LCT-13910C&gt;T."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#Data",
    "href": "topics/vc/lab_vc_tetralith.html#Data",
    "title": "Variant Calling Workflow",
    "section": "Data folder on UPPMAX",
    "text": "Data folder on UPPMAX\nAll input data for this exercise is located in this folder:\n\n\n/sw/courses/ngsintro/reseq/reseq/data\n\n\nThe fastq files are located in this folder:\n\n\n/sw/courses/ngsintro/reseq/data/fastq\n\n\nReference files, such as the reference genome in fasta format, are located in this folder:\n\n\n/sw/courses/ngsintro/reseq/data/ref"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#preparelaptop",
    "href": "topics/vc/lab_vc_tetralith.html#preparelaptop",
    "title": "Variant Calling Workflow",
    "section": "Local workspace",
    "text": "Local workspace\nThe majority of the analyses in this workshop will be done on the computing cluster, but you will copy some of the resulting files to your laptop. Therefore, please start by creating a folder for this workshop on your laptop. It is up to you where you want to put this, but it can for example be a folder called vc on Desktop. You need to have write permission in this folder. The folder you create here will be referred to as local workspace throughout this workshop."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#the-nsc-cluster",
    "href": "topics/vc/lab_vc_tetralith.html#the-nsc-cluster",
    "title": "Variant Calling Workflow",
    "section": "The NSC cluster",
    "text": "The NSC cluster\nPlease connect to the Tetralith cluster at NSC using ssh:\nssh -Y nsc_username@tetralith.nsc.liu.se\n\n\n\n\n\n\nNote\n\n\n\nNote that your terminal prompt changed to something like nsc_username@tetralith1 $ which means that you have entered Tetralith.\n\n\n\nInteractive session\nOn Tetralith you enter the login node, which is not intended for compute intensive tasks. You should therefore book a compute node, or in this case three cores:\n\n\ntetralith1$ interactive -A naissXXXX-XX-XXXX -t 04:00:00 -n 3\n\n\n\n\n\n\n\n\nNote\n\n\n\nYour terminal prompt changed to something like &lt;nsc_username&gt;@n424 $ (or another node name), which means that you are now running on one of the compute nodes.\n\n\n\n\nUPPMAX singularity container\nWe will use a singularity container (a virtual computer) that mimics the UPPMAX computing environment. Once you have started the singularity container your environment will look exactly as on UPPMAX, and the software used in this workshop will be available through the module system.\nUse this command to start the singularity container:\n\n\nn424$ singularity shell -B ~/ngsintro:~/ngsintro ~/ngsintro/ngsintro.sif\n\n\n\n\n\n\n\n\nNote\n\n\n\nNote that your terminal prompt changed to something like &lt;nsc_username&gt;@offline-uppmax$. This means that you have moved into a “virtual computer” that mimics the UPPMAX environment.\n\n\nIn the singularity container type this to make modules based on Java to load properly:\n\n\nsource /uppmax_init\n\n\nTo close the singularity container later on just type exit in the terminal, but don’t do that now.\n\n\nCluster workspace\nWhile running the UPPMAX singularity container, create a workspace folder for this exercise and move into it:\n\n\noffline-uppmax$ mkdir ~/ngsintro/vc\noffline-uppmax$ cd ~/ngsintro/vc\n\n\nThe folder you just created will be referred to as your cluster workspace throughout this workshop.\nThe cluster workspace can be reached also from outside of the UPPMAX singularity container, in this folder on Tetralith:\n\n\ntetralith$ ~/ngsintro/vc\n\n\nThis is the path you must use when downloading files to your local workspace later on.\n\n\n\n\n\n\n\nNote\n\n\n\nNote: The rest of the instructions assumes that you are running the UPPMAX singularity container from an interactive session on Tetralith, and that you are located in your cluster workspace, unless noted otherwise.\n\n\n\n\nSymbolic links to data\nThe raw data files are located in the Data folder described above. Instead of copying the files to your workspace you should create symbolic links (soft-links) to them. Soft-linking files and folders allows you to work with them as if they were in your current directory, but without multiplying them.\nCreate a symbolic link to the reference genome in your workspace:\n\n\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta\n\n\nDo the same with the fastq files:\n\n\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_1.fq\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_2.fq\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_1.fq\noffline-uppmax$ ln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_2.fq\n\n\n\n\nAccessing programs\nWe will use several programs that are installed in the module system on UPPMAX. These modules must be loaded every time you start the singularity container.\nFirst load the bioinfo-tools module:\noffline-uppmax$ module load bioinfo-tools\nThis makes it possible to load the individual programs:\noffline-uppmax$ module load FastQC/0.11.8\noffline-uppmax$ module load bwa/0.7.17\noffline-uppmax$ module load samtools/1.10\noffline-uppmax$ module load GATK/4.1.4.1\n\n\n\n\n\n\nNote\n\n\n\nYou don’t have to specify which versions of the tools to use, but is recommended to do so for reproducibility if you want to rerun the exact same analyses later.\n\n\nWhen loading the module GATK/4.1.4.1 you may get a warning message about the fact that GATK commands have been updated since the previous version of GATK. This is fine and you don’t have to do anything about it."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#index-the-genome",
    "href": "topics/vc/lab_vc_tetralith.html#index-the-genome",
    "title": "Variant Calling Workflow",
    "section": "Index the genome",
    "text": "Index the genome\nTools that compare short reads with a large reference genome needs indexes of the reference genome to work efficiently. You therefore need to create index files for each tool.\nGenerate BWA index files:\noffline-uppmax$ bwa index -a bwtsw human_g1k_v37_chr2.fasta\nCheck to see that several new files have been created using ls -l.\nGenerate a samtools index:\noffline-uppmax$ samtools faidx human_g1k_v37_chr2.fasta\nCheck to see what file(s) were created using ls -lrt.\nGenerate a GATK sequence dictionary:\noffline-uppmax$ gatk --java-options -Xmx7g CreateSequenceDictionary -R human_g1k_v37_chr2.fasta -O human_g1k_v37_chr2.dict\nAgain, check what file(s) were created using ls -lrt."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#aligning-reads",
    "href": "topics/vc/lab_vc_tetralith.html#aligning-reads",
    "title": "Variant Calling Workflow",
    "section": "1.1 Aligning reads",
    "text": "1.1 Aligning reads\n\n1.1.1 BWA mem\nIn the Cluster workspace you should now use BWA mem to align the reads to the reference genome.\nAt the same time, you should add something called read groups to the reads in the fastq files. Read groups allow us to trace various technical features, such as which flow cell was used to generate the reads, and this is needed in order to perform variant calling with HaplotypeCaller. You are not expected to learn all the details of read groups here, but if you are interested in more information please read this article at GATK-forum. The samples in this workshop come from the 1000 Genomes project, and we don’t know how these reads were generated. Let’s assume that each fastq file was generated from one library preparation (called libraryx), derived from one biological sample (called HG00097), that was run on one lane of a flow cell (called lanex_flowcellx) in the Illumina machine, and create a read group with id readgroupx that contains this information.\nThe output from BWA should be parsed to samtools sort, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. This saves space since no intermediary sam file is created.\nPlease use this command for all of this:\noffline-uppmax$ bwa mem -R \"@RG\\\\tID:readgroupx\\\\tPU:lanex_flowcellx\\\\tSM:HG00097\\\\tLB:libraryx\\\\tPL:illumina\" -t 1 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort &gt; HG00097.bam\nWhere -t 1 is the number of threads, which should be equal to the number of cores you booked. If you would have analysed the entire genome more threads would have been necessary.\nYou have to use a file redirect &gt; for the output, otherwise it will be written to stdout (your screen).\nPlease check that the expected output file was generated and that it has content using ls -lrt.\nNext you need to index the output bam file so that programs can randomly access the sorted data without reading the whole file. This command creates an index file with the same name as the input bam file, except with a .bai extension:\noffline-uppmax$ samtools index HG00097.bam\nPlease check what output file was generated this time.\n\n\n1.1.2 Check bam with samtools\nThe bam file is binary so we cannot read it, but we can view it with samtools view. The header section of the bam file can be viewed separately with the -H flag:\noffline-uppmax$ samtools view -H HG00097.bam \nTo look at the reads in the bam file just use samtools view without the -H. This will display the entire bam file which is quite large, so if you just want to look at the first 5 lines (for example) you can combine samtools view with head:\noffline-uppmax$ samtools view HG00097.bam | head -n 5 \nFor help with interpreting the bam file, please look at the sam/bam format definition at Sequence Alignment/Map Format Specification.\n\n\n1.1.3 Questions\n\nWhat does “SO:coordinate” in the @HD tag on the first line of the bam file mean?\n\nWhat does “SN:2” and “LN:243199373” in the @SQ tag mean?\nWhat is encoded in the @RG tag?\n\nWhat is the leftmost mapping position of the first read in the bamfile?\n\n\n\n1.1.4 Check bam in IGV\nInstall IGV\nIntegrated Genomics Viewer (IGV) provides an interactive visualisation of the reads in a bam file. Here we will show you how to run IGV on your laptop. If you have not used IGV on your laptop before, then go to the IGV download page, and follow the instructions to download it. It will prompt you to fill in some information and agree to license. Launch the viewer through web start. The 1.2 Gb version should be sufficient for our data.\nDownload the bam file\nOpen a new terminal window on your laptop and navigate to your local workspace, but do not log in to NSC. Download the .bam and .bam.bai files you just generated with this command:\n\n\nscp &lt;nsc_username&gt;@tetralith.nsc.uu.se:~/ngsintro/vc/HG00097.bam* .\n\n\nCheck that the files are now present in your local workspace using ls -lrt.\nLook at the bam file in IGV\n\nIn IGV, go to the popup menu in the upper left and set it to Human hg19.\n\nIn the Tools menu, select Run igvtools. Choose the command Count and then use the Browse button next to the Input File line to select the bam file (not the bai) that you just downloaded. It will autofill the output file. Hit the Run button. This generates a .tdf file that allows you to see the coverage value for our bam file even at zoomed out views. Close the igvtools window.\n\nIn the File menu, select Load from File and select your BAMs (not the .bai or the .tdf), which should appear in the tracks window. You will have to zoom in before you can see any reads. You can either select a region by click and drag, or by typing a region or a gene name in the text box at the top. Remember that we have data for the region chr2:136545000-136617000.\n\n\n\n1.1.5 Questions\n\nWhat is the read length?\n\nHow can you estimate the coverage at a specific position in IGV?\nWhich RefSeq Genes are located within the region chr2:136545000-136617000?"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#variant-calling",
    "href": "topics/vc/lab_vc_tetralith.html#variant-calling",
    "title": "Variant Calling Workflow",
    "section": "1.2 Variant Calling",
    "text": "1.2 Variant Calling\n\n1.2.1 HaplotypeCaller\nNow we will detect short variants in the bam file using GATK’s HaplotypeCaller. Go to your Cluster workspace and run:\noffline-uppmax$ gatk --java-options -Xmx7g HaplotypeCaller -R human_g1k_v37_chr2.fasta -I HG00097.bam -O HG00097.vcf \nCheck what new files were generated with ls -lrt.\n\n\n1.2.2 Explore the vcf file\nNow you have your first vcf file containing the raw variants in the region chr2:136545000-136617000 in sample HG00097. Please look at the vcf file with less and try to understand its structure.\nVCF files contains meta-information lines starting with ##, a header line starting with #CHROM, and then data lines each containing information about one variant position in the genome. The header line defines the columns of the data lines, and to view the header line you can type this command:\noffline-uppmax$ grep '#CHROM' HG00097.vcf\nThe meta-information lines starting with ##INFO defines how the data in the INFO column is encoded, and the meta-information lines starting with ##FORMAT defines how the data in the FORMAT column is encoded. To view the meta-information lines describing the INFO column use:\noffline-uppmax$ grep '##INFO' HG00097.vcf\nTo view the meta-information lines describing the FORMAT column use:\noffline-uppmax$ grep '##FORMAT' HG00097.vcf\nNow lets look at the details of one specific genetic variant at position 2:136545844:\noffline-uppmax$ grep '136545844' HG00097.vcf\nFor more detailed information about vcf files please have a look at The Variant Call Format specification.\n\n\n1.2.3 Questions\n\nWhat column of the VCF file contains genotype information for the sample HG00097?\nWhat does GT in the FORMAT column of the data lines mean?\nWhat genotype does the sample HG00097 have at position 2:136545844?\nWhat does AD in the FORMAT column of the data lines mean?\nWhat are the allelic depths for the reference and alternative alles in sample HG00097 at position 2:136545844?\nHow many genetic variants was detected in the sample? The linux command grep -v \"#\" HG00097.vcf | wc -l extracts all lines in HG00097.vcf that don’t start with “#”, and counts these lines.\n\n\n\n1.2.4 Check vcf in IGV\nDownload the vcf file and its index to the local workspace on your laptop, just like you did with the bam file and its index earlier. Open a new terminal window on your laptop and navigate to your local workspace, but do not log in to NSC. Download the files that you just generated with:\n\n\nscp &lt;nsc_username&gt;@tetralith.nsc.uu.se:~/ngsintro/vc/HG00097.vcf* .\n\n\nPlease replace &lt;nsc_nsc_username&gt; with your NSC user name.\nNote that the * in the end of the file name means that you will download all files that start with HG00097.vcf, so you will also download the vcf index.\nCheck that the files were properly downloaded to your local workspace using ls -lrt.\nIn IGV, in the File menu, select Load from File and select your vcf file (not the .idx file) and the bam file (not the .bai file) that you downloaded earlier. The vcf and bam files should appear in the tracks window. You will now see all the variants called in HG00097. You can view all variants in the LCT gene by typing the gene name in the search box, and you can look specifically at the variant at position chr2:136545844 by typing that position in the search box.\n\n\n1.2.5 Questions\n\nHover the mouse over the upper row of the vcf track. What is the reference and alternative alleles of the variant at position chr2:136545844?\nHover the mouse over the lower row of the vcf track and look under “Genotype Information”. What genotype does HG00097 have at position chr2:136545844? Is this the same as you found by looking directly in the vcf file in question 10?\nLook in the bam track and count the number of reads that have “T” and “C”, respectively, at position chr2:136545844. How is this information captured under “Genotype Attributes”? (Again, hoover the mouse over the lower row of the vcf track.)"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#bwa-mem",
    "href": "topics/vc/lab_vc_tetralith.html#bwa-mem",
    "title": "Variant Calling Workflow",
    "section": "2.1 BWA mem",
    "text": "2.1 BWA mem\nRun BWA mem for all three samples in the data set. BWA mem should be run exactly as above but with the new sample names. Please note that you also need to adjust the SM field in the read group so that it matches the new sample name, otherwise the joint genotyping step will not work properly.\nIf you run out of time, please click below to get paths to precomputed bam files.\n\n\n/sw/courses/ngsintro/reseq/data/bam/HG00097.bam\n/sw/courses/ngsintro/reseq/data/bam/HG00100.bam\n/sw/courses/ngsintro/reseq/data/bam/HG00101.bam"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#generategvcf",
    "href": "topics/vc/lab_vc_tetralith.html#generategvcf",
    "title": "Variant Calling Workflow",
    "section": "2.2 Generate g.vcf files",
    "text": "2.2 Generate g.vcf files\nHaplotypeCaller should also be run for all three samples, but this time the output for each sample needs to be in g.vcf format. This is accomplished with a small change in the HaploteypCaller command:\noffline-uppmax$ gatk --java-options -Xmx7g HaplotypeCaller -R human_g1k_v37_chr2.fasta -ERC GVCF -I &lt;sample.bam&gt; -O &lt;sample&gt;.g.vcf \nPlease replace  with the real sample names.\nIf you run out of time, please click below to get paths to the precomputed g.vcf files.\n\n\n/sw/courses/ngsintro/reseq/data/vcf/HG00097.g.vcf\n/sw/courses/ngsintro/reseq/data/vcf/HG00100.g.vcf\n/sw/courses/ngsintro/reseq/data/vcf/HG00101.g.vcf"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#jointgenotyping",
    "href": "topics/vc/lab_vc_tetralith.html#jointgenotyping",
    "title": "Variant Calling Workflow",
    "section": "2.3 Joint genotyping",
    "text": "2.3 Joint genotyping\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. To do this you first need to combine all individual .g.vcf files to one file using CombineGVCFs:\noffline-uppmax$ gatk --java-options -Xmx7g CombineGVCFs -R human_g1k_v37_chr2.fasta -V &lt;sample1&gt;.g.vcf -V &lt;sample2&gt;.g.vcf -V &lt;sample3&gt;.g.vcf -O cohort.g.vcf\nPlease replace &lt;sample1&gt;, &lt;sample2&gt;, &lt;sample3&gt; with the real sample names.\nThen run GATK’s GenoteypeGVC to generate a vcf file:\noffline-uppmax$ gatk --java-options -Xmx7g GenotypeGVCFs -R human_g1k_v37_chr2.fasta -V cohort.g.vcf -O cohort.vcf\nIf you run out of time, please click below to get paths to the precomputed cohort.g.vcf and cohort.vcf files.\n\n\n/sw/courses/ngsintro/reseq/data/vcf/cohort.g.vcf\n/sw/courses/ngsintro/reseq/data/vcf/cohort.vcf\n\n\n\n2.3.1 Questions\n\nHow many data lines do the cohort.g.vcf file have? You can use the Linux command grep -v \"#\" cohort.g.vcf to extract all lines in “cohort.g.vcf” that don’t start with “#”, then |, and then wc -l to count those lines.\n\nHow many data lines do the cohort.vcf file have?\n\nExplain the difference in number of data lines.\n\nLook at the header line of the cohort.vcf file. What columns does it have?\nWhat is encoded in the last three columns of the data lines?"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#sbatchscript",
    "href": "topics/vc/lab_vc_tetralith.html#sbatchscript",
    "title": "Variant Calling Workflow",
    "section": "2.4 SBATCH script",
    "text": "2.4 SBATCH script\n\n\n\n\n\n\nOptional\n\n\n\nThis section is only for those of you who want to try to run all steps automatically in bash scripts. Below is a skeleton script that can be used as a template. Please modify it to run all the steps in part two of this workshop. You might find it helpful to try each step for one sample in the interactive terminal, so that you know that a particular command is working, before you incorporate it in the script.\n#!/bin/bash\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"$sample\n  #Fill in the code for running bwa-mem for each sample here\n  #Fill in the code for samtools index for each sample here \n  #Fill in the code for HaplotypeCaller for each sample here\ndone\n#Fill in the code for CombineGVCFs for all samples here\n#Fill in the code for GenotypeGVCFs here\nPlease save the sbatch script in your cluster folder and call it “joint_genotyping.sh” or similar. Make the script executable by this command:\nchmod u+x joint_genotyping.sh\nUse this command to run the script in the singularity container:\n./joint_genotyping.sh\nIf you would like more help with creating the bash script, please look at our example solution:\n\n\n#!/bin/bash\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"$sample\n  bwa mem -R \"@RG\\tID:readgroupx\\tPU:flowcellx_lanex\\tSM:\"$sample\"\\tLB:libraryx\\tPL:illumina\" -t 1 human_g1k_v37_chr2.fasta $sample\"_1.fq\" $sample\"_2.fq\" | samtools sort &gt; $sample\".bam\"\n  samtools index $sample\".bam\"\n  gatk --java-options -Xmx7g HaplotypeCaller -R human_g1k_v37_chr2.fasta -ERC GVCF -I $sample\".bam\" -O $sample\".g.vcf\"\ndone\ngatk --java-options -Xmx7g CombineGVCFs -R human_g1k_v37_chr2.fasta -V HG00097.g.vcf -V HG00100.g.vcf -V HG00101.g.vcf -O cohort.g.vcf\ngatk --java-options -Xmx7g GenotypeGVCFs -R human_g1k_v37_chr2.fasta -V cohort.g.vcf -O cohort.vcf\n\n\nPlease answer the questions above."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#check-combined-vcf-file-in-igv",
    "href": "topics/vc/lab_vc_tetralith.html#check-combined-vcf-file-in-igv",
    "title": "Variant Calling Workflow",
    "section": "2.5 Check combined vcf file in IGV",
    "text": "2.5 Check combined vcf file in IGV\nDownload the file cohort.vcf and its index, as well as HG00100.bam and HG00101.bam and their indexes to your local workspace as described above. Then open cohort.vcf, HG00097.bam, HG00100.bam and HG00101.bam in IGV as described above. This time lets look at the variant rs4988235, located at position chr2:136608646 in the HG19 reference genome, that has been shown to lead to lactose persistence.\n\n2.5.1 Questions\n\nWhat is the reference and alternative alleles at chr2:136608646?\nWhat genotype do the three samples have at chr2:136608646? Note how genotypes are color coded in IGV.\nShould any of the individuals avoid drinking milk?\n\nNow compare the data shown in IGV with the data in the VCF file. Extract the row for the chr2:136608646 variant in the cohort.vcf file, for example using grep '136608646' cohort.vcf. What columns of the vcf file contain the information shown in the upper part of the vcf track in IGV?\nWhat columns of the vcf file contain the information shown in the lower part of the vcf track?\nZoom out so that you can see the MCM6 and LCT genes. Is the variant at chr2:136608646 locate within the LCT gene?\nIf you are interested in how this variant affects lactose tolerance please read the article by Mattar et al presented above, or in OMIM."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#bwa-mem-1",
    "href": "topics/vc/lab_vc_tetralith.html#bwa-mem-1",
    "title": "Variant Calling Workflow",
    "section": "3.1 BWA mem",
    "text": "3.1 BWA mem\nRun BWA mem for all three samples in the data set. BWA mem should be run exactly as above but with the new sample names. Please note that you also need to adjust the SM field in the read group so that it matches the new sample name."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#mark-duplicates",
    "href": "topics/vc/lab_vc_tetralith.html#mark-duplicates",
    "title": "Variant Calling Workflow",
    "section": "3.2 Mark Duplicates",
    "text": "3.2 Mark Duplicates\nSometimes the same DNA fragment is sequenced multiple times, which leads to multiple reads from the same fragment in the fastq file. This can occur due to PCR amplification in the library preparation, or if one read cluster is incorrectly detected as multiple clusters by the sequencing instrument. If a duplicated read contains a genetic variant, the ratio of the two alleles might be obscured, which can lead to incorrect genotyping. It is therefore recommended (in most cases) to mark duplicate reads so that they are counted as one during genotyping.\nPlease read about Picard’s MarkDuplicates here. Picard’s MarkDuplicates has recently been incorporated into the GATK suite, but the usage example in GATKs documentation still describes how to call it via the stand alone Picard program. A usage example for the version of MarkDuplicates that is incorporated in GATK is (with this you don’t have to load the Picard module):\noffline-uppmax$ gatk --java-options -Xmx7g MarkDuplicates \\\n      -I input.bam \\\n      -O marked_duplicates.bam \\\n      -M marked_dup_metrics.txt\nIf you would like more help you can sneak peek at our example solution for HG00097 below.\n\n\noffline-uppmax$ gatk --java-options -Xmx7g MarkDuplicates -I HG00097.bam -O HG00097.md.bam -M HG00097_mdmetrics.txt"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#recalibrate-base-quality-scores",
    "href": "topics/vc/lab_vc_tetralith.html#recalibrate-base-quality-scores",
    "title": "Variant Calling Workflow",
    "section": "3.3 Recalibrate Base Quality Scores",
    "text": "3.3 Recalibrate Base Quality Scores\nAnother source of error is systematic biases in the assignment of base quality scores by the sequencing instrument. This can be corrected by GATK’s Base Quality Score Recalibration. In short, you first use BaseRecalibrator to build a recalibration model, and then ApplyBQSR to recalibrate the base qualities in your bam file.\nBaseRecalibrator requires a file with known SNPs as input. This file is available in the data folder on UPPMAX:\n\n\n/sw/courses/ngsintro/reseq/data/ref/1000G_phase1.snps.high_confidence.b37.chr2.vcf\n\n\nPlease use GATK’s documentation to recalibrate the base quality scores in your data. If you would like more help you can sneak peek at our example solution for HG00097 below.\n\n\noffline-uppmax$ gatk --java-options -Xmx7g BaseRecalibrator -R human_g1k_v37_chr2.fasta -I HG00097.md.bam&gt; --known-sites /sw/courses/ngsintro/reseq/data/ref/1000G_phase1.snps.high_confidence.b37.chr2.vcf -O HG00097.recal.table\noffline-uppmax$ gatk --java-options -Xmx7g ApplyBQSR -R human_g1k_v37_chr2.fasta -I HG00097.md.bam --bqsr-recal-file HG00097.recal.table -O HG00097.recal.bam"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#generate-g.vcf-files",
    "href": "topics/vc/lab_vc_tetralith.html#generate-g.vcf-files",
    "title": "Variant Calling Workflow",
    "section": "3.4 Generate g.vcf files",
    "text": "3.4 Generate g.vcf files\nHaplotypeCaller should also be run for all three samples, and the output should be in g.vcf exactly as described above. This time use recalibrated bam files as input."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#joint-genotyping",
    "href": "topics/vc/lab_vc_tetralith.html#joint-genotyping",
    "title": "Variant Calling Workflow",
    "section": "3.5 Joint genotyping",
    "text": "3.5 Joint genotyping\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. This should be done with the commands CombineGVCFs and GenotypeGVCFs exactly as described above, but you should use the g.vcf files generated from the recalibrated bam files as input."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#variant-filtering",
    "href": "topics/vc/lab_vc_tetralith.html#variant-filtering",
    "title": "Variant Calling Workflow",
    "section": "3.6 Variant Filtering",
    "text": "3.6 Variant Filtering\nHaplotypeCaller is designed to be very sensitive, which is good because it minimises the chance of missing real variants. However, it means that the number of false positives can be quite large, so we need to filter the raw callset. GATK offers two ways to filter variants:\n\nThe variant quality score recalibration (VQSR) method uses machine learning to identify variants that are likely to be real. This is the best method if you have a lot of data, for example one whole genome sequence sample or several whole exome samples.\nIf you have less data you can use hard filters as described here.\n\nSince we have very little data we will use hard filters. The parameters are slightly different for SNVs and INDELs, so you need to first select all SNVs using SelectVariants and filter them using VariantFiltration with the parameters suggested for SNVs. Then select all INDELs and filter them with the parameters suggested for INDELs. Finally merge the SNVs and INDELs to get all variants in one file using MergeVCFs. If you would like more help you can sneak peek at our example solution for HG00097 below.\nFilter SNVs:\n\n\noffline-uppmax$ gatk --java-options -Xmx7g SelectVariants \n  -R human_g1k_v37_chr2.fasta \n  -V cohort.vcf \n  --select-type-to-include SNP \n  -O cohort.snvs.vcf\n  \noffline-uppmax$ gatk --java-options -Xmx7g VariantFiltration \n  -R human_g1k_v37_chr2.fasta \n  -V cohort.snvs.vcf \n  -O cohort.snvs.filtered.vcf \n  --filter-name QDfilter --filter-expression \"QD &lt; 2.0\"  \n  --filter-name MQfilter --filter-expression \"MQ &lt; 40.0\"  \n  --filter-name FSfilter --filter-expression \"FS &gt; 60.0\"\n\n\nFilter INDELs:\n\n\noffline-uppmax$ gatk --java-options -Xmx7g SelectVariants \n  -R human_g1k_v37_chr2.fasta \n  -V cohort.vcf \n  --select-type-to-include INDEL \n  -o cohort.indels.vcf\n\noffline-uppmax$ gatk --java-options -Xmx7g VariantFiltration \n  -R human_g1k_v37_chr2.fasta \n  -V cohort.indels.vcf \n  -O cohort.indels.filtered.vcf \n  --filter-name QDfilter --filter-expression \"QD &lt; 2.0\" \n  --filter-name FSfilter --filter-expression \"FS &gt; 200.0\"\n\n\nMerge filtered SNVs and INDELs:\n\n\noffline-uppmax$ gatk --java-options -Xmx7g MergeVcfs -I cohort.snvs.filtered.vcf -I cohort.indels.filtered.vcf -O cohort.filtered.vcf\n\n\nOpen your filtered vcf with less and page through it. It still has all the variant lines, but the FILTER column that was blank before is now filled in, with PASS or a list of the filters it failed. Note also that the filters that were run are described in the header section."
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#precomputed-files",
    "href": "topics/vc/lab_vc_tetralith.html#precomputed-files",
    "title": "Variant Calling Workflow",
    "section": "3.7 Precomputed files",
    "text": "3.7 Precomputed files\nIf you run out of time, please click below to get the path to precomputed bam and vcf files for the GATK’s best practices section.\n\n\nPath to intermediary and final bam files: /sw/courses/ngsintro/reseq/data/best_practise_bam\nPath to intermediary and final vcf files: /sw/courses/ngsintro/reseq/data/best_practise_vcf"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#bpscript",
    "href": "topics/vc/lab_vc_tetralith.html#bpscript",
    "title": "Variant Calling Workflow",
    "section": "3.8 SBATCH script",
    "text": "3.8 SBATCH script\nWe recommend you to incorporate the new steps into an SBATCH script similar to the one you created above for joint variant calling. Please try to complete the script for GATK’s best practices workflow. If you run out of time you can sneak peek at our example solution below.\n\n\n#!/bin/bash\n\n## load modules\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\n# define path to reference genome\nref=\"/sw/courses/ngsintro/reseq/data/ref\"\n\n# make symbolic links\nln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_2.fq\n\n# index reference genome\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\nsamtools faidx human_g1k_v37_chr2.fasta\ngatk --java-options -Xmx7g CreateSequenceDictionary -R human_g1k_v37_chr2.fasta -O human_g1k_v37_chr2.dict\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: ${sample}\"\n  # map the reads\n  bwa mem -R \"@RG\\tID:readgroupx\\tPU:flowcellx_lanex\\tSM:\"$sample\"\\tLB:libraryx\\tPL:illumina\" -t 1 human_g1k_v37_chr2.fasta $sample\"_1.fq\" $sample\"_2.fq\" | samtools sort &gt; $sample\".bam\"\n  samtools index $sample\".bam\"\n  # mark duplicates\n  gatk --java-options -Xmx7g MarkDuplicates -I $sample\".bam\" -O $sample\".md.bam\" -M $sample\"_mdmetrics.txt\"\n  # base quality score recalibration\n  gatk --java-options -Xmx7g BaseRecalibrator -R human_g1k_v37_chr2.fasta -I $sample\".md.bam\" --known-sites $ref\"/1000G_phase1.snps.high_confidence.b37.chr2.vcf\" -O $sample\".recal.table\"\n  gatk --java-options -Xmx7g ApplyBQSR -R human_g1k_v37_chr2.fasta -I $sample\".md.bam\" --bqsr-recal-file $sample\".recal.table\" -O $sample\".recal.bam\"\n  # haplotypeCaller in -ERC mode\n  gatk --java-options -Xmx7g HaplotypeCaller -R human_g1k_v37_chr2.fasta -ERC GVCF -I $sample\".bam\" -O $sample\".g.vcf\" \ndone\n\n# joint genotyping\ngatk --java-options -Xmx7g CombineGVCFs -R human_g1k_v37_chr2.fasta -V HG00097.g.vcf -V HG00100.g.vcf -V HG00101.g.vcf -O cohort.g.vcf\ngatk --java-options -Xmx7g GenotypeGVCFs -R human_g1k_v37_chr2.fasta -V cohort.g.vcf -O cohort.vcf\n# variant filtration SNPs\ngatk --java-options -Xmx7g SelectVariants -R human_g1k_v37_chr2.fasta -V cohort.vcf --select-type-to-include SNP -O cohort.snvs.vcf\ngatk --java-options -Xmx7g VariantFiltration -R human_g1k_v37_chr2.fasta -V cohort.snvs.vcf -O cohort.snvs.filtered.vcf --filter-name QDfilter --filter-expression \"QD &lt; 2.0\" --filter-name MQfilter --filter-expression \"MQ &lt; 40.0\"  --filter-name FSfilter --filter-expression \"FS &gt; 60.0\"\n# variant filtration indels\ngatk --java-options -Xmx7g SelectVariants -R human_g1k_v37_chr2.fasta -V cohort.vcf --select-type-to-include INDEL -O cohort.indels.vcf\ngatk --java-options -Xmx7g VariantFiltration -R human_g1k_v37_chr2.fasta -V cohort.indels.vcf -O cohort.indels.filtered.vcf --filter-name QDfilter --filter-expression \"QD &lt; 2.0\" --filter-name FSfilter --filter-expression \"FS &gt; 200.0\"\n# merge filtered SNPs and indels\ngatk --java-options -Xmx7g MergeVcfs -I cohort.snvs.filtered.vcf -I cohort.indels.filtered.vcf -O cohort.filtered.vcf"
  },
  {
    "objectID": "topics/vc/lab_vc_tetralith.html#questions-5",
    "href": "topics/vc/lab_vc_tetralith.html#questions-5",
    "title": "Variant Calling Workflow",
    "section": "3.9 Questions",
    "text": "3.9 Questions\n\nHow many variants are present in the cohort.filtered.vcf file?\n\nHow many variants have passed the filters?\n\nLook at the variants that did not pass the filters using grep -v 'PASS' cohort.filtered.vcf. Try to understand why these variants didn’t pass the filter."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#what-is-rna",
    "href": "topics/rnaseq/slide_rnaseq.html#what-is-rna",
    "title": "Bulk RNASeq Analysis",
    "section": "What is RNA?",
    "text": "What is RNA?\n\n\nThe transcriptome is spatially and temporally dynamic\nData comes from functional units (coding regions)\nOnly a tiny fraction of the genome\n\n\n\nCentral dogma of molecular biology. DNA -&gt; RNA -&gt; Protein.\nWe have gene models to describe the organisation of a genome into functional units.\nVery tiny portion of the genome is transcribed into RNA.\nThere are many types of RNA. Commonly protein coding RNA, also called mRNA. There are tRNA, sRNA, miRNA, siRNA, lincRNA, piRNA, snRNA etc.\nWhat is the most abundant RNA? rRNA.\nWhile DNA is mostly considered constant in all cells for an organism, expressed RNA varies from cell to cell and from time to time.\nWhen we say RNA sequencing, we are not sequencing RNA molecules directly. RNA is first converted to DNA through reverse transcription followed by regular DNA sequencing."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#applications",
    "href": "topics/rnaseq/slide_rnaseq.html#applications",
    "title": "Bulk RNASeq Analysis",
    "section": "Applications",
    "text": "Applications\n\nIdentify gene sequences in genomes (annotation)\nLearn about gene function\nDifferential gene expression\nExplore isoform and allelic expression\nUnderstand co-expression, pathways and networks\nGene fusion\nRNA editing"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#workflow",
    "href": "topics/rnaseq/slide_rnaseq.html#workflow",
    "title": "Bulk RNASeq Analysis",
    "section": "Workflow",
    "text": "Workflow\n\n\n\n\n\n\nConesa et al. (2016)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#de-novo-assembly",
    "href": "topics/rnaseq/slide_rnaseq.html#de-novo-assembly",
    "title": "Bulk RNASeq Analysis",
    "section": "De-Novo assembly",
    "text": "De-Novo assembly\n\nWhen no reference genome available\nTo identify novel genes/transcripts/isoforms\nIdentify fusion genes\nAssemble transcriptome from short reads\nAccess quality of assembly and refine\nMap reads back to assembled transcriptome\n\n\nTrinity, Hsieh et al. (2019), Wang & Gribskov (2017)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#experimental-design",
    "href": "topics/rnaseq/slide_rnaseq.html#experimental-design",
    "title": "Bulk RNASeq Analysis",
    "section": "Experimental design",
    "text": "Experimental design\n\n\n\nBiological replicates: 6 - 12 Schurch et al. (2016)\nSample size estimation Hart et al. (2013)\nPower analysis rnaseq-power web app, Zhao et al. (2018)\nBalanced design to avoid batch effects\nRIN values have strong effect Gallego Romero et al. (2014)\n\n\n\n\n\n\nTechnical replicates are not necessary Marioni et al. (2008)\nPrevious publications can be useful in experimental planning to avoid repeating the same mistakes"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#library-sequencing",
    "href": "topics/rnaseq/slide_rnaseq.html#library-sequencing",
    "title": "Bulk RNASeq Analysis",
    "section": "Library & Sequencing",
    "text": "Library & Sequencing\n\n\n\n\n\n\npolyA selection / Ribosomal RNA depletion\nSingle-end / Paired-end"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#library-prep",
    "href": "topics/rnaseq/slide_rnaseq.html#library-prep",
    "title": "Bulk RNASeq Analysis",
    "section": "Library prep",
    "text": "Library prep\n\n80% of the RNA in a cell is ribosomal RNA (rRNA)\nrRNA can be eliminated using polyA selection or rRNA depletion\n\nPolyA selection mostly captures only protein-coding genes / mRNA but gives cleaner results\nDepletion of rRNA is the solution if it’s important to retain all RNA species\n\nsmallRNAs are purified through size selection\nPCR amplification may be needed for low quantity input (See section PCR duplicates)\nUse stranded (directional) libraries Zhao et al. (2015), Levin et al. (2010)\n\nAccurately identify sense/antisense transcript\nResolve overlapping genes\n\nExome capture\nLibrary normalisation to concentrate specific transcripts\nLibraries should have high complexity / low duplication. Daley & Smith (2013)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#sequencing",
    "href": "topics/rnaseq/slide_rnaseq.html#sequencing",
    "title": "Bulk RNASeq Analysis",
    "section": "Sequencing",
    "text": "Sequencing\n\nShort reads vs long reads (Illumina/PacBio)\nRead length Chhangawala et al. (2015)\n\nGreater than 50bp does not improve DGE\nLonger reads are better for isoforms\n\nPooling samples\nSequencing depth (Coverage / Reads per sample)\nSingle-end reads (Cheaper?)\nUse paired-end reads\n\nIncreased mappable reads\nIncreased power in assemblies\nBetter for structural variation and isoforms\nDecreased false-positives for DGE\n\nMore replicates are better than more depth Liu et al. (2014)\n\nCorley et al. (2017)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#workflow-dge",
    "href": "topics/rnaseq/slide_rnaseq.html#workflow-dge",
    "title": "Bulk RNASeq Analysis",
    "section": "Workflow • DGE",
    "text": "Workflow • DGE"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#read-qc",
    "href": "topics/rnaseq/slide_rnaseq.html#read-qc",
    "title": "Bulk RNASeq Analysis",
    "section": "Read QC",
    "text": "Read QC\n\n\n\nNumber of reads\nPer base sequence quality\nPer sequence quality score\nPer base sequence content\nPer sequence GC content\nPer base N content\nSequence length distribution\nSequence duplication levels\nOverrepresented sequences\nAdapter content\nKmer content\n\nFastQC, MultiQC, https://sequencing.qcfail.com/"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#fastqc",
    "href": "topics/rnaseq/slide_rnaseq.html#fastqc",
    "title": "Bulk RNASeq Analysis",
    "section": "FastQC",
    "text": "FastQC\n\n\nGood quality\n\n\nPoor quality"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#read-qc-pbsq-psqs",
    "href": "topics/rnaseq/slide_rnaseq.html#read-qc-pbsq-psqs",
    "title": "Bulk RNASeq Analysis",
    "section": "Read QC • PBSQ, PSQS",
    "text": "Read QC • PBSQ, PSQS\nPer base sequence quality\n\nPer sequence quality scores"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#read-qc-pbsc-psgc",
    "href": "topics/rnaseq/slide_rnaseq.html#read-qc-pbsc-psgc",
    "title": "Bulk RNASeq Analysis",
    "section": "Read QC • PBSC, PSGC",
    "text": "Read QC • PBSC, PSGC\nPer base sequence content\n\nPer sequence GC content"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#read-qc-sdl-ac",
    "href": "topics/rnaseq/slide_rnaseq.html#read-qc-sdl-ac",
    "title": "Bulk RNASeq Analysis",
    "section": "Read QC • SDL, AC",
    "text": "Read QC • SDL, AC\nSequence duplication level\n\nAdapter content"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#trimming",
    "href": "topics/rnaseq/slide_rnaseq.html#trimming",
    "title": "Bulk RNASeq Analysis",
    "section": "Trimming",
    "text": "Trimming\n\n\n\nTrimming reads to remove adapter/readthrough or low quality bases\nRelated options are hard clipping, filtering reads\nSliding window trimming\nFilter by min/max read length\n\nRemove reads less than ~18nt\n\nDemultiplexing/Splitting\n\nWhen to avoid trimming?\n\nRead trimming may not always be necessary Liao & Shi (2020)\nFixed read length may sometimes be more important\nExpected insert size distribution may be more important for assemblers\n\nCutadapt, fastp, Prinseq"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#mapping",
    "href": "topics/rnaseq/slide_rnaseq.html#mapping",
    "title": "Bulk RNASeq Analysis",
    "section": "Mapping",
    "text": "Mapping\n\n\nAligning reads back to a reference sequence\nMapping to genome vs transcriptome\nSplice-aware alignment (genome) (STAR, HISAT2 etc)\n\nSTAR, HiSat2, Baruzzo et al. (2017)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#aligners-metrics",
    "href": "topics/rnaseq/slide_rnaseq.html#aligners-metrics",
    "title": "Bulk RNASeq Analysis",
    "section": "Aligners • Metrics",
    "text": "Aligners • Metrics\n\n\n\nBaruzzo et al. (2017)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#aligners-time-and-ram",
    "href": "topics/rnaseq/slide_rnaseq.html#aligners-time-and-ram",
    "title": "Bulk RNASeq Analysis",
    "section": "Aligners time and RAM",
    "text": "Aligners time and RAM\n\n\n\nProgram\nTime_Min\nMemory_GB\n\n\n\n\nHISATx1\n22.7\n4.3\n\n\nHISATx2\n47.7\n4.3\n\n\nHISAT\n26.7\n4.3\n\n\nSTAR\n25\n28\n\n\nSTARx2\n50.5\n28\n\n\nGSNAP\n291.9\n20.2\n\n\nTopHat2\n1170\n4.3"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#section",
    "href": "topics/rnaseq/slide_rnaseq.html#section",
    "title": "Bulk RNASeq Analysis",
    "section": "",
    "text": "Reads (FASTQ)\n\n@ST-E00274:179:HHYMLALXX:8:1101:1641:1309 1:N:0:NGATGT\nNCATCGTGGTATTTGCACATCTTTTCTTATCAAATAAAAAGTTTAACCTACTCAGTTATGCGCATACGTTTTTTGATGGCATTTCCATAAACCGATTTTTTTTTTATGCACGTACCCAAAACGTGCAGAAAAATACGCTGCTAGAAATGTA\n+\n#AAAFAFA&lt;-AFFJJJAFA-FFJJJJFFFAJJJJ-&lt;FFJJJ-A-F-7--FA7F7-----FFFJFA&lt;FFFFJ&lt;AJ--FF-A&lt;A-&lt;JJ-7-7-&lt;FF-FFFJAFFAA--A--7FJ-7----77-A--7F7)---7F-A----7)7-----7&lt;&lt;-\n@instrument:runid:flowcellid:lane:tile:xpos:ypos read:isfiltered:controlnumber:sampleid\n\nReference Genome/Transcriptome (FASTA)\n\n&gt;1 dna:chromosome chromosome:GRCz10:1:1:58871917:1 REF\nGATCTTAAACATTTATTCCCCCTGCAAACATTTTCAATCATTACATTGTCATTTCCCCTC\n\nAnnotation (GTF/GFF)\n\n#!genome-build GRCz10\n4       ensembl_havana  gene    6732    52059   .       -       .       gene_id \"ENSDARG00000104632\"; gene_version \"2\"; gene_name \"rerg\"; gene_source \"ensembl_havana\"; gene_biotype \"protein_coding\"; havana_gene \"OTTDARG00000044080\"; havana_gene_version \"1\";\nseq source feature start end score strand frame attribute\nIllumina FASTQ format, GTF format"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment",
    "text": "Alignment\n\nSAM/BAM (Sequence Alignment Map format)\n\nST-E00274:188:H3JWNCCXY:4:1102:32431:49900      163     1       1       60      8S139M4S      =       385     535     TATTTAGAGATCTTAAACATCCATTCCCCCTGCAAACATTTTCAATCATTACATTGTCATTTTCCCTCCAAATTAAATTTAGCCAGAGGCGCACAACATACGACCTCTAAAAAAGGTGCTGGAACATGTACCTATATGCAGCACCACCATC     AAAFAFFAFFFFJ7FFFFJ&lt;JAFA7F-&lt;AJ7JJ&lt;FFFJ--&lt;FAJF&lt;7&lt;7FAFJ-&lt;AFA&lt;-JJJ-AF-AJ-FF&lt;F--A&lt;FF&lt;-7777-7JA-77A---F-7AAFF-FJA--77FJ&lt;--77)))7&lt;JJA&lt;J77&lt;-------&lt;7--))7)))7-    NM:i:4   MD:Z:12T0T40C58T25      AS:i:119        XS:i:102        XA:Z:17,-53287490,4S33M4D114M,11;     MQ:i:60 MC:Z:151M       RG:Z:ST-E00274_188_H3JWNCCXY_4\nquery flag ref pos mapq cigar mrnm mpos tlen seq qual opt\nNever store alignment files in raw SAM format. Always compress it! SAM format\n\n\n\nFormat\nSize_GB\n\n\n\n\nSAM\n7.4\n\n\nBAM\n1.9\n\n\nCRAM lossless Q\n1.4\n\n\nCRAM 8 bins Q\n0.8\n\n\nCRAM no Q\n0.26"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#visualisation-igv",
    "href": "topics/rnaseq/slide_rnaseq.html#visualisation-igv",
    "title": "Bulk RNASeq Analysis",
    "section": "Visualisation • IGV",
    "text": "Visualisation • IGV\n\nIGV, UCSC Genome Browser, SeqMonk, More"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#visualisation-tview",
    "href": "topics/rnaseq/slide_rnaseq.html#visualisation-tview",
    "title": "Bulk RNASeq Analysis",
    "section": "Visualisation • tview",
    "text": "Visualisation • tview\nsamtools tview alignment.bam genome.fasta"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#visualisation-seqmonk",
    "href": "topics/rnaseq/slide_rnaseq.html#visualisation-seqmonk",
    "title": "Bulk RNASeq Analysis",
    "section": "Visualisation • SeqMonk",
    "text": "Visualisation • SeqMonk\n\nSeqMonk"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC",
    "text": "Alignment QC\n\nNumber of reads mapped/unmapped/paired etc\nUniquely mapped\nInsert size distribution\nCoverage\nGene body coverage\nBiotype counts / Chromosome counts\nCounts by region: gene/intron/non-genic\nSequencing saturation\nStrand specificity\n\nSTAR (final log file), samtools stats, bamtools stats, QoRTs, RSeQC, Qualimap"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc-star-log",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc-star-log",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC • STAR Log",
    "text": "Alignment QC • STAR Log\nMultiQC can be used to summarise and plot STAR log files."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc-features",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc-features",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC • Features",
    "text": "Alignment QC • Features\nQoRTs was run on all samples and summarised using MultiQC."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc-qorts",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc-qorts",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC • QoRTs",
    "text": "Alignment QC • QoRTs"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc-examples",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc-examples",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC • Examples",
    "text": "Alignment QC • Examples\n\n\nRead mapping profile \n\nGene body coverage \nSigurgeirsson et al. (2014)\n\n\nThe read mapping profile shows how well bases along a read mapped to the reference for all reads. Mapability usually decreases towards the 5’ and 3’ ends due to soft-clipping. This is is more pronounced in untrimmed reads.\nGene body coverage shows read coverage over the gene for all genes. An even coverage is ideally expected. A bias in either direction could be indicative of library quality. A bias toward the 3’ end is usually seen in polyA selected libraries."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#alignment-qc-examples-1",
    "href": "topics/rnaseq/slide_rnaseq.html#alignment-qc-examples-1",
    "title": "Bulk RNASeq Analysis",
    "section": "Alignment QC • Examples",
    "text": "Alignment QC • Examples\n\n\nInsert size\n\n\nSaturation curve\n\nFrancis et al. (2013)\n\n\nNegative insert size implies overlapping mate pairs.\nConserved genes in the mouse transcriptome. Saturation curves of discovery of genes in the mouse heart from a set of a subset of 248 conserved orthologs; genes which have any blast hit are shown in circles; genes which the translated protein was within the expected size range of the conserved gene are in squares; proteins which are 100% identical to a canonical protein in Uniprot/Swissprot mouse database are shown in triangles."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#quantification-counts",
    "href": "topics/rnaseq/slide_rnaseq.html#quantification-counts",
    "title": "Bulk RNASeq Analysis",
    "section": "Quantification • Counts",
    "text": "Quantification • Counts\n\n\n\nRead counts = gene expression\nIntersection on gene models\nReads can be quantified on any feature (gene, transcript, exon etc)\n\n\nfeatureCounts, HTSeq"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#quantification",
    "href": "topics/rnaseq/slide_rnaseq.html#quantification",
    "title": "Bulk RNASeq Analysis",
    "section": "Quantification",
    "text": "Quantification\n\n\nPCR duplicates\n\nComputational deduplication not recommended Klepikova et al. (2017), Parekh et al. (2016)\nUse PCR-free library-prep kits\nUse UMIs during library-prep Fu et al. (2018)\n\nMulti-mapping\n\nAdded (BEDTools multicov)\nDiscard (featureCounts, HTSeq)\nDistribute counts (Cufflinks, featureCounts)\nRescue\n\nProbabilistic assignment (Rcount, Cufflinks)\nPrioritise features (Rcount)\nProbabilistic assignment with EM (RSEM)\n\n\n\n\n\n\nTools that detect duplicate reads (like Picard MarkDuplicates) is looking for reads that start and end at the same coordinates. Such reads are expected to be PCR derived and can be collapsed down to 1 read. This makes sense in whole genome sequencing, but in RNA-Seq, this is naturally expected since highly expressed genes create huge number of identical transcripts which may fragment at hotspots leading to fragmentation bias Roberts et al. (2011). Therefore, removing PCR duplicates may remove real duplicates as well."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#quantification-abundance",
    "href": "topics/rnaseq/slide_rnaseq.html#quantification-abundance",
    "title": "Bulk RNASeq Analysis",
    "section": "Quantification • Abundance",
    "text": "Quantification • Abundance\n\nCount methods\n\nProvide no inference on isoforms\nCannot accurately measure fold change\n\n\n\n\nProbabilistic assignment\n\nDeconvolute ambiguous mappings\nTranscript-level\ncDNA reference\n\n\nKallisto, Salmon\n\nUltra-fast & alignment-free\nBootstrapping & quantification confidence\nTranscript-level counts\nTranscript-level estimates improves gene-level estimates Soneson et al. (2015), tximport\nEvaluation and comparison of isoform quantification tools Zhang et al. (2017)\n\nRSEM, Kallisto, Salmon\n\nGene expression counts may be literal counts (reads overlapping to gene models) or probabilistically estimated based on the data. Proponents of probabilistically estimation claims that it is a better estimate that corrects for sampling biases.\nTools such as RSEM, Kallisto and Salmon produce probabilistic abundance estimates. Soneson et al. (2015) claims that estimating abundance using Kallisto at the transcript levels and then summarising down to gene-level counts results in improved estimates of gene expression.\nKallisto/Salmon -&gt; transcript-counts -&gt; tximport() -&gt; gene-counts"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#quantification-qc",
    "href": "topics/rnaseq/slide_rnaseq.html#quantification-qc",
    "title": "Bulk RNASeq Analysis",
    "section": "Quantification QC",
    "text": "Quantification QC\nENSG00000000003    140   242   188   143   287   344   438   280   253\nENSG00000000005    0     0     0     0     0     0     0     0     0\nENSG00000000419    69    98    77    55    52    94    116   79    69\nENSG00000000457    56    75    104   79    157   205   183   178   153\nENSG00000000460    33    27    23    19    27    42    69    44    40\n\nPairwise correlation between samples must be high (&gt;0.9)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#multiqc",
    "href": "topics/rnaseq/slide_rnaseq.html#multiqc",
    "title": "Bulk RNASeq Analysis",
    "section": "MultiQC",
    "text": "MultiQC"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#normalization",
    "href": "topics/rnaseq/slide_rnaseq.html#normalization",
    "title": "Bulk RNASeq Analysis",
    "section": "Normalization",
    "text": "Normalization\n\n\n\nControl for Sequencing depth, compositional bias and more\nMedian of Ratios (DESeq2) and TMM (edgeR) perform the best\n\n\n\nFor DGE using DGE packages, use raw counts\nFor clustering, heatmaps etc use VST, VOOM or RLOG\nFor own analysis, plots etc, use TPM\nOther solutions: spike-ins/house-keeping genes\n\nDillies et al. (2013), Evans et al. (2018), Wagner et al. (2012)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#exploratory",
    "href": "topics/rnaseq/slide_rnaseq.html#exploratory",
    "title": "Bulk RNASeq Analysis",
    "section": "Exploratory",
    "text": "Exploratory\n\n\n\nRemove lowly expressed genes\nHeatmaps, MDS, PCA etc.\n\n\npheatmap\n\n\nTransform raw counts to VST, VOOM, RLOG, TPM etc"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#batch-correction",
    "href": "topics/rnaseq/slide_rnaseq.html#batch-correction",
    "title": "Bulk RNASeq Analysis",
    "section": "Batch correction",
    "text": "Batch correction\n\nEstimate variation explained by variables (PVCA)\n\n\n\nFind confounding effects as surrogate variables (SVA)\nModel known batches in the LM/GLM model\nCorrect known batches (ComBat from SVA)(Can overcorrect! Zindler et al. (2020))\nInteractively evaluate batch effects and correction (BatchQC) Manimaran et al. (2016)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#differential-expression",
    "href": "topics/rnaseq/slide_rnaseq.html#differential-expression",
    "title": "Bulk RNASeq Analysis",
    "section": "Differential expression",
    "text": "Differential expression\n\n\n\n\nUnivariate testing gene-by-gene\nMore descriptive, less predictive"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#differential-expression-1",
    "href": "topics/rnaseq/slide_rnaseq.html#differential-expression-1",
    "title": "Bulk RNASeq Analysis",
    "section": "Differential expression",
    "text": "Differential expression\n\nDESeq2, edgeR (Neg-binom &gt; GLM &gt; Test)\nLimma-Voom (Neg-binom &gt; Voom-transform &gt; LM &gt; Test)\nDESeq2 ~age+condition\n\nEstimate size factors estimateSizeFactors()\nEstimate gene-wise dispersion estimateDispersions()\nFit curve to gene-wise dispersion estimates\nShrink gene-wise dispersion estimates\nGLM fit for each gene\nWald test nbinomWaldTest()\n\n\n\nDESeq2, edgeR, limma, Seyednasrollah et al. (2015)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#dge",
    "href": "topics/rnaseq/slide_rnaseq.html#dge",
    "title": "Bulk RNASeq Analysis",
    "section": "DGE",
    "text": "DGE\n\nResults results()\n\nlog2 fold change (MLE): type type2 vs control\nWald test p-value: type type2 vs control\nDataFrame with 1 row and 6 columns\n                        baseMean     log2FoldChange             lfcSE\n                       &lt;numeric&gt;          &lt;numeric&gt;         &lt;numeric&gt;\nENSG00000000003 242.307796723287 -0.932926089608546 0.114285150312285\n                             stat               pvalue                 padj\n                        &lt;numeric&gt;            &lt;numeric&gt;            &lt;numeric&gt;\nENSG00000000003 -8.16314356729037 3.26416150242775e-16 1.36240609998527e-14\n\nSummary summary()\n\nout of 17889 with nonzero total read count\nadjusted p-value &lt; 0.1\nLFC &gt; 0 (up)       : 4526, 25%\nLFC &lt; 0 (down)     : 5062, 28%\noutliers [1]       : 25, 0.14%\nlow counts [2]     : 0, 0%\n(mean count &lt; 3)"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#section-1",
    "href": "topics/rnaseq/slide_rnaseq.html#section-1",
    "title": "Bulk RNASeq Analysis",
    "section": "",
    "text": "MA plot plotMA()\n\n\n\nVolcano plot\n\n\n\n\nNormalised counts plotCounts()"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#functional-analysis-gene-ontology",
    "href": "topics/rnaseq/slide_rnaseq.html#functional-analysis-gene-ontology",
    "title": "Bulk RNASeq Analysis",
    "section": "Functional analysis • Gene Ontology",
    "text": "Functional analysis • Gene Ontology\n\n\n\nGene set analysis (GSA)\nGene set enrichment analysis (GSEA)\nGene ontology / Reactome databases"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#functional-analysis-kegg",
    "href": "topics/rnaseq/slide_rnaseq.html#functional-analysis-kegg",
    "title": "Bulk RNASeq Analysis",
    "section": "Functional analysis • Kegg",
    "text": "Functional analysis • Kegg\n\nPathway analysis (Kegg)\n\n\nWebgestalt, EnrichR, clusterProfiler, ClueGO, pathview"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#summary",
    "href": "topics/rnaseq/slide_rnaseq.html#summary",
    "title": "Bulk RNASeq Analysis",
    "section": "Summary",
    "text": "Summary\n\nSound experimental design to avoid confounding\nPlan carefully about lib prep, sequencing etc based on experimental objective\nFor DGE, biological replicates may be more important than other considerations (paired-end, sequencing depth, long reads etc)\nDiscard low quality bases, reads, genes and samples\nVerify that tools and methods align with data assumptions\nExperiment with multiple pipelines and tools\nQC! QC everything at every step\n\n\nConesa, A., Madrigal, P., Tarazona, S., Gomez-Cabrero, D., Cervera, A., McPherson, A., … & Mortazavi, A. (2016). A survey of best practices for RNA-seq data analysis. Genome biology, 17(1), 1-19."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#further-learning",
    "href": "topics/rnaseq/slide_rnaseq.html#further-learning",
    "title": "Bulk RNASeq Analysis",
    "section": "Further learning",
    "text": "Further learning\n\n\n\nGriffith lab RNA-Seq using HiSat & StringTie tutorial\nHBC Training DGE using DeSeq2 tutorial\nRNA-Seq Blog\nSciLifeLab courses"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#section-2",
    "href": "topics/rnaseq/slide_rnaseq.html#section-2",
    "title": "Bulk RNASeq Analysis",
    "section": "",
    "text": "Thank you. Questions?"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#references",
    "href": "topics/rnaseq/slide_rnaseq.html#references",
    "title": "Bulk RNASeq Analysis",
    "section": "References",
    "text": "References\n\n\nBaruzzo, G., Hayer, K. E., Kim, E. J., Di Camillo, B., FitzGerald, G. A., & Grant, G. R. (2017). Simulation-based comprehensive benchmarking of RNA-seq aligners. Nature Methods, 14(2), 135–139. https://www.nature.com/articles/nmeth.4106\n\n\nChhangawala, S., Rudy, G., Mason, C. E., & Rosenfeld, J. A. (2015). The impact of read length on quantification of differentially expressed genes and splice junction detection. Genome Biology, 16(1), 1–10. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4531809/\n\n\nConesa, A., Madrigal, P., Tarazona, S., Gomez-Cabrero, D., Cervera, A., McPherson, A., Szcześniak, M. W., Gaffney, D. J., Elo, L. L., Zhang, X., et al. (2016). A survey of best practices for RNA-seq data analysis. Genome Biology, 17(1), 1–19.\n\n\nCorley, S. M., MacKenzie, K. L., Beverdam, A., Roddam, L. F., & Wilkins, M. R. (2017). Differentially expressed genes from RNA-seq and functional enrichment results are affected by the choice of single-end versus paired-end reads and stranded versus non-stranded protocols. BMC Genomics, 18(1), 1–13. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5442695/\n\n\nDaley, T., & Smith, A. D. (2013). Predicting the molecular complexity of sequencing libraries. Nature Methods, 10(4), 325–327. https://www.nature.com/articles/nmeth.2375\n\n\nDillies, M.-A., Rau, A., Aubert, J., Hennequet-Antier, C., Jeanmougin, M., Servant, N., Keime, C., Marot, G., Castel, D., Estelle, J., et al. (2013). A comprehensive evaluation of normalization methods for illumina high-throughput RNA sequencing data analysis. Briefings in Bioinformatics, 14(6), 671–683.\n\n\nEvans, C., Hardin, J., & Stoebel, D. M. (2018). Selecting between-sample RNA-seq normalization methods from the perspective of their assumptions. Briefings in Bioinformatics, 19(5), 776–792.\n\n\nFrancis, W. R., Christianson, L. M., Kiko, R., Powers, M. L., Shaner, N. C., & D Haddock, S. H. (2013). A comparison across non-model animals suggests an optimal sequencing depth for de novotranscriptome assembly. BMC Genomics, 14(1), 1–12. https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-14-167\n\n\nFu, Y., Wu, P.-H., Beane, T., Zamore, P. D., & Weng, Z. (2018). Elimination of PCR duplicates in RNA-seq and small RNA-seq using unique molecular identifiers. Bmc Genomics, 19, 1–14.\n\n\nGallego Romero, I., Pai, A. A., Tung, J., & Gilad, Y. (2014). RNA-seq: Impact of RNA degradation on transcript quantification. BMC Biology, 12(1), 1–13. https://bmcbiol.biomedcentral.com/articles/10.1186/1741-7007-12-42\n\n\nHart, S. N., Therneau, T. M., Zhang, Y., Poland, G. A., & Kocher, J.-P. (2013). Calculating sample size estimates for RNA sequencing data. Journal of Computational Biology, 20(12), 970–978. https://www.liebertpub.com/doi/10.1089/cmb.2012.0283\n\n\nHsieh, P.-H., Oyang, Y.-J., & Chen, C.-Y. (2019). Effect of de novo transcriptome assembly on transcript quantification. Scientific Reports, 9(1), 8304. https://www.nature.com/articles/s41598-019-44499-3\n\n\nKlepikova, A. V., Kasianov, A. S., Chesnokov, M. S., Lazarevich, N. L., Penin, A. A., & Logacheva, M. (2017). Effect of method of deduplication on estimation of differential gene expression using RNA-seq. PeerJ, 5, e3091. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5357343/\n\n\nLevin, J. Z., Yassour, M., Adiconis, X., Nusbaum, C., Thompson, D. A., Friedman, N., Gnirke, A., & Regev, A. (2010). Comprehensive comparative analysis of strand-specific RNA sequencing methods. Nature Methods, 7(9), 709–715. https://www.nature.com/articles/nmeth.1491\n\n\nLiao, Y., & Shi, W. (2020). Read trimming is not required for mapping and quantification of RNA-seq reads at the gene level. NAR Genomics and Bioinformatics, 2(3), lqaa068. https://pubmed.ncbi.nlm.nih.gov/33575617/\n\n\nLiu, Y., Zhou, J., & White, K. P. (2014). RNA-seq differential expression studies: More sequence or more replication? Bioinformatics, 30(3), 301–304. https://academic.oup.com/bioinformatics/article/30/3/301/228651\n\n\nManimaran, S., Selby, H. M., Okrah, K., Ruberman, C., Leek, J. T., Quackenbush, J., Haibe-Kains, B., Bravo, H. C., & Johnson, W. E. (2016). BatchQC: Interactive software for evaluating sample and batch effects in genomic data. Bioinformatics, 32(24), 3836–3838.\n\n\nMarioni, J. C., Mason, C. E., Mane, S. M., Stephens, M., & Gilad, Y. (2008). RNA-seq: An assessment of technical reproducibility and comparison with gene expression arrays. Genome Research, 18(9), 1509–1517. https://genome.cshlp.org/content/18/9/1509.long\n\n\nParekh, S., Ziegenhain, C., Vieth, B., Enard, W., & Hellmann, I. (2016). The impact of amplification on differential expression analyses by RNA-seq. Scientific Reports, 6(1), 25533. https://www.nature.com/articles/srep25533\n\n\nRoberts, A., Trapnell, C., Donaghey, J., Rinn, J. L., & Pachter, L. (2011). Improving RNA-seq expression estimates by correcting for fragment bias. Genome Biology, 12(3), 1–14.\n\n\nSchurch, N. J., Schofield, P., Gierliński, M., Cole, C., Sherstnev, A., Singh, V., Wrobel, N., Gharbi, K., Simpson, G. G., Owen-Hughes, T., et al. (2016). How many biological replicates are needed in an RNA-seq experiment and which differential expression tool should you use? Rna, 22(6), 839–851. https://rnajournal.cshlp.org/content/early/2016/03/30/rna.053959.115.abstract\n\n\nSeyednasrollah, F., Laiho, A., & Elo, L. L. (2015). Comparison of software packages for detecting differential expression in RNA-seq studies. Briefings in Bioinformatics, 16(1), 59–70.\n\n\nSigurgeirsson, B., Emanuelsson, O., & Lundeberg, J. (2014). Sequencing degraded RNA addressed by 3’tag counting. PloS One, 9(3), e91851. https://pubmed.ncbi.nlm.nih.gov/24632678/\n\n\nSoneson, C., Love, M. I., & Robinson, M. D. (2015). Differential analyses for RNA-seq: Transcript-level estimates improve gene-level inferences. F1000Research, 4.\n\n\nWagner, G. P., Kin, K., & Lynch, V. J. (2012). Measurement of mRNA abundance using RNA-seq data: RPKM measure is inconsistent among samples. Theory in Biosciences, 131, 281–285.\n\n\nWang, S., & Gribskov, M. (2017). Comprehensive evaluation of de novo transcriptome assembly programs and their effects on differential gene expression analysis. Bioinformatics, 33(3), 327–333. https://academic.oup.com/bioinformatics/article/33/3/327/2580374\n\n\nZhang, C., Zhang, B., Lin, L.-L., & Zhao, S. (2017). Evaluation and comparison of computational tools for RNA-seq isoform quantification. BMC Genomics, 18(1), 1–11.\n\n\nZhao, S., Li, C.-I., Guo, Y., Sheng, Q., & Shyr, Y. (2018). RnaSeqSampleSize: Real data based sample size estimation for RNA sequencing. BMC Bioinformatics, 19(1), 1–8. https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2191-5\n\n\nZhao, S., Zhang, Y., Gordon, W., Quan, J., Xi, H., Du, S., Schack, D. von, & Zhang, B. (2015). Comparison of stranded and non-stranded RNA-seq transcriptome profiling and investigation of gene overlap. BMC Genomics, 16(1), 1–14. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4559181/\n\n\nZindler, T., Frieling, H., Neyazi, A., Bleich, S., & Friedel, E. (2020). Simulating ComBat: How batch correction can lead to the systematic introduction of false positive results in DNA methylation microarray studies. BMC Bioinformatics, 21, 1–15."
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#hands-on-tutorial",
    "href": "topics/rnaseq/slide_rnaseq.html#hands-on-tutorial",
    "title": "Bulk RNASeq Analysis",
    "section": "Hands-On tutorial",
    "text": "Hands-On tutorial\nMain exercise\n\n01 Check the quality of the raw reads with FastQC\n02 Map the reads to the reference genome using HISAT2\n03 Assess the post-alignment quality using QualiMap\n04 Count the reads overlapping with genes using featureCounts\n05 Find differentially expressed genes using DESeq2 in R\n\nBonus exercises\n\n01 Functional annotation of DE genes using GO/Reactome/Kegg databases\n02 RNA-Seq figures and plots using R\n03 Visualisation of RNA-seq BAM files using IGV genome browser\n\nData: /sw/courses/ngsintro/rnaseq/dardel\nWork: ~/ngsintro/rnaseq/"
  },
  {
    "objectID": "topics/rnaseq/slide_rnaseq.html#hands-on-tutorial-1",
    "href": "topics/rnaseq/slide_rnaseq.html#hands-on-tutorial-1",
    "title": "Bulk RNASeq Analysis",
    "section": "Hands-On tutorial",
    "text": "Hands-On tutorial\n\n\n\nCourse data directory\n\n/sw/courses/ngsintro/rnaseq/dardel\ndardel/\n├── bonus\n│   ├── assembly\n│   ├── exon\n│   ├── funannot\n│   └── plots\n├── main\n│   ├── 1_raw\n│   ├── 2_fastqc\n│   ├── 3_mapping\n│   ├── 4_qorts\n│   ├── 4_qualimap\n│   ├── 5_dge\n│   ├── 6_multiqc\n│   ├── reference\n│   │   └── mouse_chr19_hisat2\n│   └── scripts\n├── main_full\n│   └── nextflow\n├── r\n└── README.md\n\n\nYour work directory\n\n~/ngsintro/rnaseq/\nrnaseq/\n├── 1_raw\n├── 2_fastqc\n├── 3_mapping\n├── 4_picard\n├── 4_qualimap\n├── 5_dge\n├── 6_multiqc\n├── funannot\n├── plots\n├── reference\n└── scripts"
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html",
    "href": "topics/rnaseq/lab_rnaseq.html",
    "title": "RNASeq Workflow",
    "section": "",
    "text": "RNA-seq has become a powerful approach to study the continually changing cellular transcriptome. Here, one of the most common questions is to identify genes that are differentially expressed between two conditions, e.g. controls and treatment. The main exercise in this tutorial will take you through a basic bioinformatic analysis pipeline to answer just that, it will show you how to find differentially expressed (DE) genes.\nMain exercise\n\nCheck the quality of the raw reads with FastQC\nMap the reads to the reference genome using HISAT2\nAssess the post-alignment quality using Qualimap or Picard\nCount the reads overlapping with genes using featureCounts\nFind DE genes using DESeq2 in R\n\nRNA-seq experiment does not necessarily end with a list of DE genes. If you have time after completing the main exercise, try one (or more) of the bonus exercises. The bonus exercises can be run independently of each other, so choose the one that matches your interest. Bonus sections are listed below.\nBonus exercises\n\n01 Functional annotation of DE genes using GO/Reactome databases\n02 RNA-Seq figures and plots using R\n03 Visualisation of RNA-seq BAM files using IGV genome browser\n\n\n\n\n\n\n\nGeneral guide\n\n\n\nIn code and paths, remember to change username with your actual HPC username.\nYou are welcome to try your own solutions to the problems, before checking the solution. Click the button to see the suggested solution. There is more than one way to complete a task. Discuss with person next to you and ask us when in doubt.\nInput code blocks are displayed like shown below.\n\ncommand"
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#introduction",
    "href": "topics/rnaseq/lab_rnaseq.html#introduction",
    "title": "RNASeq Workflow",
    "section": "",
    "text": "RNA-seq has become a powerful approach to study the continually changing cellular transcriptome. Here, one of the most common questions is to identify genes that are differentially expressed between two conditions, e.g. controls and treatment. The main exercise in this tutorial will take you through a basic bioinformatic analysis pipeline to answer just that, it will show you how to find differentially expressed (DE) genes.\nMain exercise\n\nCheck the quality of the raw reads with FastQC\nMap the reads to the reference genome using HISAT2\nAssess the post-alignment quality using Qualimap or Picard\nCount the reads overlapping with genes using featureCounts\nFind DE genes using DESeq2 in R\n\nRNA-seq experiment does not necessarily end with a list of DE genes. If you have time after completing the main exercise, try one (or more) of the bonus exercises. The bonus exercises can be run independently of each other, so choose the one that matches your interest. Bonus sections are listed below.\nBonus exercises\n\n01 Functional annotation of DE genes using GO/Reactome databases\n02 RNA-Seq figures and plots using R\n03 Visualisation of RNA-seq BAM files using IGV genome browser\n\n\n\n\n\n\n\nGeneral guide\n\n\n\nIn code and paths, remember to change username with your actual HPC username.\nYou are welcome to try your own solutions to the problems, before checking the solution. Click the button to see the suggested solution. There is more than one way to complete a task. Discuss with person next to you and ask us when in doubt.\nInput code blocks are displayed like shown below.\n\ncommand"
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#data-description",
    "href": "topics/rnaseq/lab_rnaseq.html#data-description",
    "title": "RNASeq Workflow",
    "section": "2 Data description",
    "text": "2 Data description\nThe data used in this exercise is from the paper: Poitelon, Yannick, et al. YAP and TAZ control peripheral myelination and the expression of laminin receptors in Schwann cells. Nature neuroscience 19.7 (2016): 879. In this study, YAP and TAZ genes were knocked-down in Schwann cells to study myelination, using mice as a model.\nMyelination is essential for nervous system function. Schwann cells are a type of glial cell that interact with neurons and the basal lamina to myelinate axons. Myelinated axons transfer signals up to 10x faster. Hippo pathway is a conserved pathway involved in cell contact inhibition, and it acts to promote cell proliferation and inhibits apoptosis. Transcription co-activators YAP and TAZ are two major downstream effectors of the Hippo pathway, and have redundant roles in transcriptional activation. TAZ and YAP genes were knocked-down to study their effect on neurons.\nThe material for RNA-seq was collected from 2 conditions (Wt and KO), each with 3 biological replicates.\n\n\n\n\n\n\nAccession\nCondition\nReplicate\n\n\n\n\nSRR3222409\nKO\n1\n\n\nSRR3222410\nKO\n2\n\n\nSRR3222411\nKO\n3\n\n\nSRR3222412\nWt\n1\n\n\nSRR3222413\nWt\n2\n\n\nSRR3222414\nWt\n3\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purpose of this tutorial, to shorten the time needed to run various bioinformatics steps, we have picked reads for a single chromosome (Chr 19) and downsampled the reads. We randomly sampled, without replacement, 25% reads from each sample, using fastq-sample from the toolset fastq-tools."
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#main-exercise",
    "href": "topics/rnaseq/lab_rnaseq.html#main-exercise",
    "title": "RNASeq Workflow",
    "section": "3 Main exercise",
    "text": "3 Main exercise\nThe main exercise covers Differential Gene Expression (DGE) workflow from raw reads to a list of differentially expressed genes.\n\n3.1 Using HPC\n\n\n\n\n\n\nUse ThinLinc\n\n\n\nIf you have issues opening GUI windows from HPC through the terminal, it is recommended to use ThinLinc.\n\n\nIf you are not on ThinLinc, connect to HPC first.  Remember to replace username.\nssh -XY username@dardel.pdc.kth.se\nBook a node.\nFor the RNA-Seq part of the course, we will work on the HPC. The code below is valid to run at the start of the day. If you are running it in the middle of a day, you need to decrease the time (-t). Do not run this twice and also make sure you are not running computations on a login node.\nBook compute resources for RNA-Seq lab.\n\n\n\n\n\n\nTip\n\n\n\nThe --reservation is only used during the workshop and not needed when working on your projects. Use edu25-03-27 for Thursday and edu25-03-28 for Friday.\n\n\nsalloc -A edu25.uppmax --reservation=edu25-03-27 -t 03:00:00 -p shared -c 4 --no-shell\nCheck allocation.  Remember to replace username.\nsqueue -u username\nOnce allocation is granted, log on to the compute node.  Remember to replace nodename.\nssh -Y nodename\n\n3.1.1 Set-up directory\nSetting up the directory structure is an important step as it helps to keep our raw data, intermediate data and results in an organised manner. We suggest a location in your home directory for the whole workshop (~/ngsintro) and within that a directory for this lab (~/ngsintro/rnaseq).\nCreate a directory named rnaseq.\ncd ~/ngsintro\nmkdir rnaseq\n Create the directory structure as shown below.\n~/ngsintro/\nrnaseq/\n  +-- 1_raw/\n  +-- 2_fastqc/\n  +-- 3_mapping/\n  +-- 4_qualimap/\n  +-- 5_dge/\n  +-- 6_multiqc/\n  +-- reference/\n  |   +-- mouse_chr19_hisat2/\n  +-- scripts/\n  +-- funannot/\n  +-- plots\ncd rnaseq\nmkdir 1_raw 2_fastqc 3_mapping 4_qualimap 5_dge 6_multiqc reference scripts funannot plots\ncd reference\nmkdir mouse_chr19_hisat2\ncd ..\nThe 1_raw directory will hold the raw fastq files (soft-links). 2_fastqc will hold FastQC outputs. 3_mapping will hold the mapping output files. 4_qualimap will hold the post alignment QC output. 5_dge will hold the counts from featureCounts and all differential gene expression related files. 6_multiqc will hold MultiQC outputs. reference directory will hold the reference genome, annotations and aligner indices. The funannot and plots directory are optional for bonus steps.\n It might be a good idea to open an additional terminal window. One to navigate through directories and another for scripting in the scripts directory.\n\n\n3.1.2 Create symbolic links\nWe have the raw fastq files in this remote directory: /sw/courses/ngsintro/rnaseq/dardel/main/1_raw/. We are going to create symbolic links (soft-links) for these files from our 1_raw directory to the remote directory. We do this because fastq files tend to be large files and simply copying them would use up a lot of storage space. Soft-linking files and folders allows us to work with those files as if they were actually there.\nChange to 1_raw directory. Use pwd to check if you are standing in the correct directory.\ncd 1_raw\npwd\n~/ngsintro/rnaseq/1_raw\nRun below to create softlinks. Note that the command ends in a space followed by a period.\nln -s /sw/courses/ngsintro/rnaseq/dardel/main/1_raw/*.gz .\nCheck if your files have linked correctly. You should be able to see as below.\nls -l\nSRR3222409-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222409-19_1.fq.gz\nSRR3222409-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222409-19_2.fq.gz\nSRR3222410-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222410-19_1.fq.gz\nSRR3222410-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222410-19_2.fq.gz\nSRR3222411-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222411-19_1.fq.gz\nSRR3222411-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222411-19_2.fq.gz\nSRR3222412-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222412-19_1.fq.gz\nSRR3222412-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222412-19_2.fq.gz\nSRR3222413-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222413-19_1.fq.gz\nSRR3222413-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222413-19_2.fq.gz\nSRR3222414-19_1.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222414-19_1.fq.gz\nSRR3222414-19_2.fq.gz -&gt; /sw/courses/ngsintro/rnaseq/main/1_raw/SRR3222414-19_2.fq.gz\n\n\n\n3.2 Read QC\nQuality check using FastQC\nAfter receiving raw reads from a high throughput sequencing centre it is essential to check their quality. FastQC provides a simple way to do some quality control check on raw sequence data. It provides a modular set of analyses which you can use to get a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\n Change into the 2_fastqc directory. Use pwd to check if you are standing in the correct directory.\ncd ../2_fastqc\npwd\n~/ngsintro/rnaseq/2_fastqc\nLoad modules bioinfo-tools and FastQC fastqc/0.12.1.\nmodule load bioinfo-tools\nmodule load fastqc/0.12.1\nOnce the module is loaded, FastQC program is available through the command fastqc. Use fastqc --help to see the various parameters available to the program. We will use -o to specify the output directory path and finally, the name of the input fastq file to analyse. The syntax to run one file will look like below.\n Don’t run this. It’s just a template.\nfastqc -o . ../1_raw/filename.fq.gz\nBased on the above command, we will write a bash loop script to process all fastq files in the directory. Writing multi-line commands through the terminal can be a pain. Therefore, we will run larger scripts from a bash script file. Move to your scripts directory and create a new file named fastqc.sh.\ncd ../scripts\npwd\n~/ngsintro/rnaseq/scripts\nThe command below creates a new file in the current directory.\ntouch fastqc.sh\nUse a text editor (nano,Emacs, gedit etc.) to edit fastqc.sh.\n gedit behaves like a regular text editor with a standard graphical interface.\n\ngedit fastqc.sh &\n\nThen add the lines below and save the file.\n\n#!/bin/bash\n\nmodule load bioinfo-tools\nmodule load fastqc/0.12.1\n\nfor i in ../1_raw/*.gz\ndo\n    echo \"Running $i ...\"\n    fastqc -o . \"$i\"\ndone\n\nThis script loops through all files ending in .gz. In each iteration of the loop, it executes fastqc on the file. The -o . flag to fastqc indicates that the output must be exported in this current directory (ie; the directory where this script is run).\nChange to the 2_fastqc directory. Use pwd to check if you are standing in the correct directory. Run the script file fastqc.sh\ncd ../2_fastqc\npwd\n~/ngsintro/rnaseq/2_fastqc\nbash ../scripts/fastqc.sh\nAfter the fastqc run, there should be a .zip file and a .html file for every fastq file. The .html file is the report that you need. Download the .html files to your computer and open them in a web browser. You do not need to necessarily look at all files now. We will do a comparison with all samples when using the MultiQC tool.\n Run this step in a LOCAL terminal and NOT on HPC. Open a terminal locally on your computer, move to a suitable download directory and run the command below to download one html report.\n Remember to replace username.\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/2_fastqc/SRR3222409-19_1_fastqc.html .\nor download the whole directory.\nscp -r username@dardel.pdc.kth.se:~/ngsintro/rnaseq/2_fastqc .\n\n\n\n\n\n\nDownloading using a GUI\n\n\n\nYou can optionally use an SFTP browser like Filezilla or Cyberduck for a GUI interface. Those using MobaXterm, it has an embedded SFTP file browser to drag and drop.\n\n\n Go back to the FastQC website and compare your report with the sample report for Good Illumina data and Bad Illumina data.\n Discuss based on your reports, whether your data is of good enough quality and/or what steps are needed to fix it.\n\n\n3.3 Mapping\nMapping reads using HISAT2\nAfter verifying that the quality of the raw sequencing reads is acceptable, we will map the reads to the reference genome. There are many mappers/aligners available, so it may be good to choose one that is adequate for your type of data. Here, we will use a software called HISAT2 (Hierarchical Indexing for Spliced Alignment of Transcripts) as it is a good general purpose splice-aware aligner. It is fast and does not require a lot of RAM. Before we begin mapping, we need to obtain genome reference sequence (.fasta file) and build an aligner index. Due to time constraints, we will build an index only on chromosome 19.\n\n3.3.1 Get reference\nIt is best if the reference genome (.fasta) and annotation (.gtf) files come from the same source to avoid potential naming conventions problems. It is also good to check in the manual of the aligner you use for hints on what type of files are needed to do the mapping. We will not be using the annotation (.gtf) during mapping, but we will use it during quantification.\n What is the idea behind building an aligner index? What files are needed to build one? Where do we take them from? Could one use an index that was generated for another project? Check out the HISAT2 manual indexer section for answers. Browse through Ensembl and try to find the files needed. Note that we are working with Mouse (Mus musculus).\n Move into the reference directory and download the Chr 19 genome (.fasta) file and the genome-wide annotation file (.gtf) from Ensembl.\nYou should be standing here to run this:\n~/ngsintro/rnaseq/reference\nYou are most likely to use the latest version of ensembl release genome and annotations when starting a new analysis. For this exercise, we will choose ensembl version 99.\nwget ftp://ftp.ensembl.org/pub/release-99/fasta/mus_musculus/dna/Mus_musculus.GRCm38.dna.chromosome.19.fa.gz\nwget ftp://ftp.ensembl.org/pub/release-99/gtf/mus_musculus/Mus_musculus.GRCm38.99.gtf.gz\nDecompress the files for use.\ngunzip Mus_musculus.GRCm38.dna.chromosome.19.fa.gz\ngunzip Mus_musculus.GRCm38.99.gtf.gz\nFrom the full gtf file, we will also extract chr 19 alone to create a new gtf file for use later.\ncat Mus_musculus.GRCm38.99.gtf | grep -E \"^#|^19\" &gt; Mus_musculus.GRCm38.99-19.gtf\n The grep command is set to use regular expression with the -E argument. And it’s looking for lines starting with # or 19. # fetches the header and comments while 19 denotes all rows with annotations for chromosome 19.\nCheck what you have in your directory.\nls -l\ndrwxrwsr-x 2 user gXXXXXXX 4.0K Jan 22 21:59 mouse_chr19_hisat2\n-rw-rw-r-- 1 user gXXXXXXX  26M Jan 22 22:46 Mus_musculus.GRCm38.99-19.gtf\n-rw-rw-r-- 1 user gXXXXXXX 771M Jan 22 22:10 Mus_musculus.GRCm38.99.gtf\n-rw-rw-r-- 1 user gXXXXXXX  60M Jan 22 22:10 Mus_musculus.GRCm38.dna.chromosome.19.fa\n\n\n3.3.2 Build index\nMove into the reference directory if not already there. Load module hisat2. Remember to load bioinfo-tools if you haven’t done so already.\nmodule load bioinfo-tools\nmodule load hisat2/2.2.1\n To search for the tool or other versions of a tool, use module spider hisat.\nCreate a new bash script in your scripts directory named hisat2_index.sh and add the following lines:\n#!/bin/bash\n\n# load module\nmodule load bioinfo-tools\nmodule load hisat2/2.2.1\n\nhisat2-build \\\n  -p 1 \\\n  Mus_musculus.GRCm38.dna.chromosome.19.fa \\\n  mouse_chr19_hisat2/mouse_chr19_hisat2\nThe above script builds a HISAT2 index using the command hisat2-build. It should use 1 core for computation. The paths to the FASTA (.fa) genome file is specified. And the output path is specified. The output describes the output directory and the prefix for output files. The output directory must be present before running this script. Check hisat2-build --help for the arguments and descriptions.\nUse pwd to check if you are standing in the correct directory. Then, run the script from the reference directory.\nbash ../scripts/hisat2_index.sh\nOnce the indexing is complete, move into the mouse_chr19_hisat2 directory and make sure you have all the files.\nls -l mouse_chr19_hisat2/\n-rw-rw-r-- 1 user gXXXXXXX 23M Sep 19 12:30 mouse_chr19_hisat2.1.ht2\n-rw-rw-r-- 1 user gXXXXXXX 14M Sep 19 12:30 mouse_chr19_hisat2.2.ht2\n-rw-rw-r-- 1 user gXXXXXXX  53 Sep 19 12:29 mouse_chr19_hisat2.3.ht2\n-rw-rw-r-- 1 user gXXXXXXX 14M Sep 19 12:29 mouse_chr19_hisat2.4.ht2\n-rw-rw-r-- 1 user gXXXXXXX 25M Sep 19 12:30 mouse_chr19_hisat2.5.ht2\n-rw-rw-r-- 1 user gXXXXXXX 15M Sep 19 12:30 mouse_chr19_hisat2.6.ht2\n-rw-rw-r-- 1 user gXXXXXXX  12 Sep 19 12:29 mouse_chr19_hisat2.7.ht2\n-rw-rw-r-- 1 user gXXXXXXX   8 Sep 19 12:29 mouse_chr19_hisat2.8.ht2\nThe index for the whole genome would be created in a similar manner. It just requires more time to run.\n\n\n3.3.3 Map reads\nNow that we have the index ready, we are ready to map reads. Move to the directory 3_mapping. Use pwd to check if you are standing in the correct directory.\nYou should be standing here to run this:\n~/ngsintro/rnaseq/3_mapping\nWe will create softlinks to the fastq files from here to make things easier.\ncd 3_mapping\nln -s ../1_raw/* .\nThese are the parameters that we want to specify for the mapping run:\n\nSpecify the number of threads\nSpecify the full genome index path\nSpecify a summary file\nIndicate read1 and read2 since we have paired-end reads\nDirect the output (SAM) to a file\n\nHISAT2 aligner arguments can be obtained by running hisat2 --help. We should end with a script that looks like below to run one of the samples.\nhisat2 \\\n  -p 1 \\\n  -x ../reference/mouse_chr19_hisat2/mouse_chr19_hisat2 \\\n  --summary-file \"SRR3222409-19.summary\" \\\n  -1 SRR3222409-19_1.fq.gz \\\n  -2 SRR3222409-19_2.fq.gz \\\n  -S SRR3222409-19.sam\nLet’s break this down a bit. -p options denotes that it will use 1 thread. In a real world scenario, we might want to use 8 to 12 threads. -x denotes the full path (with prefix) to the aligner index we built in the previous step. --summary-file specifies a summary file to write alignments metrics such as mapping rate etc. -1 and -2 specifies the input fastq files. Both are used here because this is a paired-end sequencing. -S denotes the output SAM file.\nBut, we will not run it as above. We will make some more changes to it. We want to make the following changes:\n\nRather than writing the output as a SAM file, we want to direct the output to another tool samtools to order alignments by coordinate and to export in BAM format\nGeneralise the above script to be used as a bash script to read any two input files and to automatically create the output filename.\n\n Now create a new bash script file named hisat2_align.sh in your scripts directory and add the script below to it.\n#!/bin/bash\n\nmodule load bioinfo-tools\nmodule load hisat2/2.2.1\nmodule load samtools/1.20\n\n# get output filename prefix\nprefix=$( basename $1 | sed -E 's/_.+$//' )\n\nhisat2 \\\n  -p 1 \\\n  -x ../reference/mouse_chr19_hisat2/mouse_chr19_hisat2 \\\n  --summary-file \"${prefix}.summary\" \\\n  -1 $1 \\\n  -2 $2 | samtools sort -O BAM &gt; \"${prefix}.bam\"\nIn the above script, the two input fasta files are not hard coded, rather we use $1 and $2 because they will be passed in as arguments.\nThe output filename prefix is automatically created using this line prefix=$( basename $1 | sed -E 's/_.+$//' ) from input filename of $1. For example, a file with path /bla/bla/sample_1.fq.gz will have the directory stripped off using the function basename to get sample_1.fq.gz. This is piped (|) to sed where all text starting from _ to end of string (specified by this regular expression _.+$ matching _1.fq.gz) is removed and the prefix will be just sample. This approach will work only if your filenames are labelled suitably.\nLastly, the SAM output is piped (|) to the tool samtools for sorting and written in BAM format.\nNow we can run the bash script like below while standing in the 3_mapping directory.\nbash ../scripts/hisat2_align.sh sample_1.fq.gz sample_2.fq.gz\n Similarly run the other samples.\n\n\n\n\n\n\nOptional\n\n\n\nTry to create a new bash loop script (hisat2_align_batch.sh) to iterate over all fastq files in the directory and run the mapping using the hisat2_align.sh script. Note that there is a bit of a tricky issue here. You need to use two fastq files (_1 and _2) per run rather than one file. There are many ways to do this and here is one.\n## find only files for read 1 and extract the sample name\nlines=$(find *_1.fq.gz | sed \"s/_1.fq.gz//\")\n\nfor i in ${lines}\ndo\n  ## use the sample name and add suffix (_1.fq.gz or _2.fq.gz)\n  echo \"Mapping ${i}_1.fq.gz and ${i}_2.fq.gz ...\"\n  bash ../scripts/hisat2_align.sh ${i}_1.fq.gz ${i}_2.fq.gz\ndone\nRun the hisat2_align_batch.sh script in the 3_mapping directory.\nbash ../scripts/hisat2_align_batch.sh\n\n\nAt the end of the mapping jobs, you should have the following list of output files for every sample:\nls -l\n-rw-rw-r-- 1 user gXXXXXXX 16M Sep 19 12:30 SRR3222409-19.bam\n-rw-rw-r-- 1 user gXXXXXXX 604 Sep 19 12:30 SRR3222409-19.summary\nThe .bam file contains the alignment of all reads to the reference genome in binary format. BAM files are not human readable directly. To view a BAM file in text format, you can use samtools view functionality.\nmodule load bioinfo-tools\nmodule load samtools/1.20\nsamtools view SRR3222409-19.bam | head\nSRR3222409.13658290 163 19  3084385 60  95M3S   =   3084404 120 CTTTAAGATAAGTGCCGGTTGCAGCCAGCTGTGAGAGCTGCACTCCCTTCTCTGCTCTAAAGTTCCCTCTTCTCAGAAGGTGGCACCACCCTGAGCTG  DB@D@GCHHHEFHIIG&lt;CHHHIHHIIIIHHHIIIIGHIIIIIFHIGHIHIHIIHIIHIHIIHHHHIIIIIIHFFHHIIIGCCCHHHH1GHHIIHHIII  AS:i:-3 ZS:i:-18    XN:i:0  XM:i:0  XO:i:0  XG:i:0  NM:i:0  MD:Z:95 YS:i:-6 YT:Z:CP NH:i:1\nSRR3222409.13658290 83  19  3084404 60  101M    =   3084385 -120    TGCAGCCAGCTGTGAGAGCTGCACTCCCTTCTCTGCTCTAAAGTTCCCTCTTCTCAGAAGGTGGCACCACCCTGAGCTGCTGGCAGTGAGTCTGTTCCAAG   IIIIHECHHH?IHHHIIIHIHIIIHEHHHCHHHIHIIIHHIHIIIHHHHHHIHEHIIHIIHHIIHHIHHIGHIGIIIIIIIHHIIIHHIHEHCHHG@&lt;&lt;BD   AS:i:-6 ZS:i:-16    XN:i:0  XM:i:1  XO:i:0  XG:i:0  NM:i:1  MD:Z:76T24  YS:i:-3 YT:Z:CP NH:i:1\nSRR3222409.13741570 163 19  3085066 60  15M2I84M    =   3085166 201 ATAGTACCTGGCAACAAAAAAAAAAAAGCTTTTGGCTAAAGACCAATGTGTTTAAGAGATAAAAAAAGGGGTGCTAATACAGAAGCTGAGGCCTTAGAAGA   0B@DB@HCCH1&lt;&lt;CGECCCGCHHIDD?01&lt;&lt;G1&lt;/&lt;1&lt;FH1F11&lt;1111&lt;&lt;&lt;&lt;11&lt;CGC1&lt;G1&lt;F//DHHI0/01&lt;&lt;1FG11111111&lt;111&lt;1D1&lt;1D1&lt;   AS:i:-20    XN:i:0  XM:i:3  XO:i:1  XG:i:2  NM:i:5  MD:Z:40T19C27T10    YS:i:0  YT:Z:CP NH:i:1\n Can you identify what some of these columns are? SAM format description is available here. SAM output specifically from HISAT is described here, especially the tags.\n The summary file gives a summary of the mapping run. This file is used by MultiQC later to collect mapping statistics. Inspect one of the mapping log files to identify the number of uniquely mapped reads and multi-mapped reads.\ncat SRR3222409-19.summary\n156992 reads; of these:\n  156992 (100.00%) were paired; of these:\n    7984 (5.09%) aligned concordantly 0 times\n    147178 (93.75%) aligned concordantly exactly 1 time\n    1830 (1.17%) aligned concordantly &gt;1 times\n    ----\n    7984 pairs aligned concordantly 0 times; of these:\n      690 (8.64%) aligned discordantly 1 time\n    ----\n    7294 pairs aligned 0 times concordantly or discordantly; of these:\n      14588 mates make up the pairs; of these:\n        5987 (41.04%) aligned 0 times\n        4085 (28.00%) aligned exactly 1 time\n        4516 (30.96%) aligned &gt;1 times\n98.09% overall alignment rate\nNext, we need to index these BAM files. Indexing creates .bam.bai files which are required by many downstream programs to quickly and efficiently locate reads anywhere in the BAM file.\n Index all BAM files. Write a for-loop to index all BAM files using the command samtools index file.bam.\nmodule load bioinfo-tools\nmodule load samtools/1.20\n\nfor i in *.bam\n  do\n    echo \"Indexing $i ...\"\n    samtools index $i\n  done\nNow, we should have .bai index files for all BAM files.\nls -l\n-rw-rw-r-- 1 user gXXXXXXX 16M Sep 19 12:30 SRR3222409-19.bam\n-rw-rw-r-- 1 user gXXXXXXX 43K Sep 19 12:32 SRR3222409-19.bam.bai\n If you are running short of time or unable to run the mapping, you can copy over results for all samples that have been prepared for you before class. They are available at this location: /sw/courses/ngsintro/rnaseq/dardel/main/3_mapping/.\ncp -r /sw/courses/ngsintro/rnaseq/dardel/main/3_mapping/* ~/ngsintro/rnaseq/3_mapping/\n\n\n\n\n3.4 Alignment QC\nPost-alignment QC using QualiMap\nSome important quality aspects, such as saturation of sequencing depth, read distribution between different genomic features or coverage uniformity along transcripts, can be measured only after mapping reads to the reference genome. One of the tools to perform this post-alignment quality control is QualiMap. QualiMap examines sequencing alignment data in SAM/BAM files according to the features of the mapped reads and provides an overall view of the data that helps to the detect biases in the sequencing and/or mapping of the data and eases decision-making for further analysis.\nThere are other tools with similar functionality such as RNASeqQC or QoRTs.\n Read through QualiMap documentation and see if you can figure it out how to run it to assess post-alignment quality on the RNA-seq mapped samples.\n Load the QualiMap module version 2.2.1 and create a bash script named qualimap.sh in your scripts directory.\nAdd the following script to it. Note that we are using the smaller GTF file with chr19 only.\n#!/bin/bash\n\n# load modules\nmodule load PDC/23.12\nmodule load bioinfo-tools\nmodule load QualiMap/2.2.1\n\n# get output filename prefix\nprefix=$( basename \"$1\" .bam)\n\nunset DISPLAY\n\nqualimap rnaseq -pe \\\n  -bam $1 \\\n  -gtf \"../reference/Mus_musculus.GRCm38.99-19.gtf\" \\\n  -outdir \"../4_qualimap/${prefix}/\" \\\n  -outfile \"$prefix\" \\\n  -outformat \"HTML\" \\\n  --java-mem-size=6G &gt;& \"${prefix}-qualimap.log\"\nThe line prefix=$( basename \"$1\" .bam) is used to remove directory path and .bam from the input filename and create a prefix which will be used to label output. The export unset DISPLAY forces a ‘headless mode’ on the JAVA application, which would otherwise throw an error about X11 display. If that doesn’t work, one can also try export DISPLAY=:0 or export DISPLAY=\"\". The last part &gt;& \"${prefix}-qualimap.log\" saves the standard-out as a log file.\n Create a new bash loop script named qualimap_batch.sh with a bash loop to run the qualimap script over all BAM files. The loop should look like below.\nfor i in ../3_mapping/*.bam\ndo\n    echo \"Running QualiMap on $i ...\"\n    bash ../scripts/qualimap.sh $i\ndone\nRun the loop script qualimap_batch.sh in the directory 4_qualimap.\nbash ../scripts/qualimap_batch.sh\nQualimap should have created a directory for every BAM file.\ndrwxrwsr-x 5 user gXXXXXXX 4.0K Jan 22 22:53 SRR3222409-19\n-rw-rw-r-- 1 user gXXXXXXX  669 Jan 22 22:53 SRR3222409-19-qualimap.log\nInside every directory, you should see:\nls -l\ndrwxrwsr-x 2 user gXXXXXXX 4.0K Jan 22 22:53 css\ndrwxrwsr-x 2 user gXXXXXXX 4.0K Jan 22 22:53 images_qualimapReport\n-rw-rw-r-- 1 user gXXXXXXX  12K Jan 22 22:53 qualimapReport.html\ndrwxrwsr-x 2 user gXXXXXXX 4.0K Jan 22 22:53 raw_data_qualimapReport\n-rw-rw-r-- 1 user gXXXXXXX 1.2K Jan 22 22:53 rnaseq_qc_results.txt\n Download the results.\n When downloading the HTML files, note that you MUST also download the dependency files (ie; css folder and images_qualimapReport folder), otherwise the HTML file may not render correctly. Remember to replace username.\nscp -r username@dardel.pdc.kth.se:~/ngsintro/rnaseq/4_qualimap .\n Check the QualiMap report for one sample and discuss if the sample is of good quality. You only need to do this for one file now. We will do a comparison with all samples when using the MultiQC tool.\n If you are running out of time or were unable to run QualiMap, you can also copy pre-run QualiMap output from this location: /sw/courses/ngsintro/rnaseq/dardel/main/4_qualimap/.\ncp -r /sw/courses/ngsintro/rnaseq/dardel/main/4_qualimap/* ~/ngsintro/rnaseq/4_qualimap/\"))\n\n\n\n3.5 Quantification\nCounting mapped reads using featureCounts\nAfter ensuring mapping quality, we can move on to enumerating reads mapping to genomic features of interest. Here we will use featureCounts, an ultrafast and accurate read summarisation program, that can count mapped reads for genomic features such as genes, exons, promoter, gene bodies, genomic bins and chromosomal locations.\n Read featureCounts documentation and see if you can figure it out how to use paired-end reads using an unstranded library to count fragments overlapping with exonic regions and summarise over genes.\n Load the subread module on HPC. Create a bash script named featurecounts.sh in the directory scripts.\nWe could run featureCounts on each BAM file, produce a text output for each sample and combine the output. But the easier way is to provide a list of all BAM files and featureCounts will combine counts for all samples into one text file.\nBelow is the script that we will use:\n#!/bin/bash\n\n# load modules\nmodule load PDC/23.12\nmodule load bioinfo-tools\nmodule load R/4.4.0\nmodule load subread/2.0.3\n\nfeatureCounts \\\n  -a \"../reference/Mus_musculus.GRCm38.99.gtf\" \\\n  -o \"counts.txt\" \\\n  -F \"GTF\" \\\n  -t \"exon\" \\\n  -g \"gene_id\" \\\n  -p \\\n  -s 0 \\\n  -T 1 \\\n  ../3_mapping/*.bam\nIn the above script, we indicate the path of the annotation file (-a \"../reference/Mus_musculus.GRCm38.99.gtf\"), specify the output file name (-o \"counts.txt\"), specify that that annotation file is in GTF format (-F \"GTF\"), specify that reads are to be counted over exonic features (-t \"exon\") and summarised to the gene level (-g \"gene_id\"). We also specify that the reads are paired-end (-p), the library is unstranded (-s 0) and the number of threads to use (-T 1).\nRun the featurecounts bash script in the directory 5_dge. Use pwd to check if you are standing in the correct directory.\nYou should be standing here to run this:\n~/ngsintro/rnaseq/5_dge\nbash ../scripts/featurecounts.sh\nYou should have two output files:\nls -l\n-rw-rw-r-- 1 user gXXXXXXX 2.8M Sep 15 11:05 counts.txt\n-rw-rw-r-- 1 user gXXXXXXX  658 Sep 15 11:05 counts.txt.summary\n Inspect the files and try to make sense of them.\n\n\n\n\n\n\nImportant\n\n\n\nFor downstream steps, we will NOT use this counts.txt file. Instead we will use counts_full.txt from the back-up folder. This contains counts across all chromosomes. This is located here: /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/. Copy this file to your 5_dge directory.\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/counts_full.txt ~/ngsintro/rnaseq/5_dge/\n\n\n\n\n\n3.6 QC report\nCombined QC report using MultiQC\nWe will use the tool MultiQC to crawl through the output, log files etc from FastQC, HISAT2, QualiMap and featureCounts to create a combined QC report.\nMove to the 6_multiqc directory. You should be standing here to run this:\n~/ngsintro/rnaseq/6_multiqc\nAnd run this in the terminal.\nmodule load bioinfo-tools\nmodule load MultiQC/1.12\n\nmultiqc --interactive ../\n  /// MultiQC 🔍 | v1.12\n\n|           multiqc | Search path : /cfs/klemming/home/r/user/ngsintro/rnaseq\n|         searching | ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% 297/297\n|          qualimap | Found 6 RNASeq reports\n|    feature_counts | Found 6 reports\n|           bowtie2 | Found 6 reports\n|            fastqc | Found 12 reports\n|           multiqc | Compressing plot data\n|           multiqc | Report      : multiqc_report.html\n|           multiqc | Data        : multiqc_data\n|           multiqc | MultiQC complete\nThe output should look like below:\nls -l\ndrwxrwsr-x 2 user gXXXXXXX 4.0K Sep  6 22:33 multiqc_data\n-rw-rw-r-- 1 user gXXXXXXX 1.3M Sep  6 22:33 multiqc_report.html\n Download the MultiQC HTML report to your computer and inspect the report.\n Run this step in a LOCAL terminal and NOT on HPC. Remember to replace username.\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/6_multiqc/multiqc_report.html .\n\n\n\n3.7 DGE\nDifferential gene expression using DESeq2\nThe easiest way to perform differential expression is to use one of the statistical packages, within R environment, that were specifically designed for analyses of read counts arising from RNA-seq, SAGE and similar technologies. Here, we will use one such package called DESeq2. Learning R is beyond the scope of this course so we prepared basic ready to run R scripts to find DE genes between conditions KO and Wt.\nWe need some annotation information, so we will copy this file /sw/courses/ngsintro/rnaseq/dardel/main/reference/mm-biomart99-genes.txt.gz to your rnaseq/reference directory.\ncp /sw/courses/ngsintro/rnaseq/dardel/main/reference/mm-biomart99-genes.txt.gz ~/ngsintro/rnaseq/reference/\nThen uncompress the file. gunzip ../reference/mm-biomart99-genes.txt.gz\nMove to the 5_dge directory and load R module for use.\nmodule load PDC/23.12\nmodule load R/4.4.0\nUse pwd to check if you are standing in the correct directory. Copy the following file /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/dge.r to the 5_dge directory.\nMake sure you have the counts_full.txt. If not, you can copy this file too: /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/counts_full.txt\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/counts_full.txt .\nNow, run the R script from the shell in 5_dge directory.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/dge.r\nIf you are curious what’s inside dge.r, you are welcome to explore it using a text editor.\nThis should have produced the following output files:\nls -l\n-rw-rw-r-- 1 user gXXXXXXX 282K Jan 22 23:16 counts_vst_full.rds\n-rw-rw-r-- 1 user gXXXXXXX 1.7M Jan 22 23:16 counts_vst_full.txt\n-rw-rw-r-- 1 user gXXXXXXX 727K Jan 22 23:16 dge_results_full.rds\n-rw-rw-r-- 1 user gXXXXXXX 1.7M Jan 22 23:16 dge_results_full.txt\nEssentially, we have two outputs: dge_results_full and counts_vst_full. dge_results_full is the list of differentially expressed genes. This is available in human readable tab-delimited .txt file and R readable binary .rds file. The counts_vst_full is variance-stabilised normalised counts, useful for exploratory analyses.\n Copy the results text file (dge_results_full.txt) to your computer and inspect the results. What are the columns? How many differentially expressed genes are present after adjusted p-value of 0.05? How many genes are upregulated and how many are down-regulated? How does this change if we set a fold-change cut-off of 1?\n Open in a spreadsheet editor like Microsoft Excel or LibreOffice Calc.\n If you do not have the results or were unable to run the DGE step, you can copy these two here which will be required for functional annotation (optional).\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/dge_results_full.txt .\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/dge_results_full.rds .\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/counts_vst_full.txt .\ncp /sw/courses/ngsintro/rnaseq/dardel/main/5_dge/counts_vst_full.rds .\n\n\n3.8 RNA-Seq plots\nWe use R to create exploratory plots and investigate the results of differential expression analysis. Depending on your proficiency in reading R code and using R, you can in this section either just call scripts from the command lines with a set of arguments or you can open the R script in a text editor, and run the code step by step from an interactive R session.\nLoad the module R.\nmodule load PDC/23.12\nmodule load R/4.4.0\n\n3.8.1 PCA plot\nA popular way to visualise general patterns of gene expression in your data is to produce either PCA (Principal Component Analysis) or MDS (Multi Dimensional Scaling) plots. These methods aim at summarising the main patterns of expression in the data and display them on a two-dimensional space and still retain as much information as possible. To properly evaluate these kind of results is non-trivial, but in the case of RNA-seq data we often use them to get an idea of the difference in expression between treatments and also to get an idea of the similarity among replicates. If the plots shows clear clusters of samples that corresponds to treatment it is an indication of treatment actually having an effect on gene expression. If the distance between replicates from a single treatment is very large it suggests large variance within the treatment, something that will influence the detection of differentially expressed genes between treatments.\nMove to the 5_dge/ directory. Then run the pca.r script like below.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/scripts/pca.r\nThis generates a file named pca.png in the 5_dge folder. To view it, download it to your local disk.\n\n Do samples cluster as expected? Are there any odd or mislabelled samples? Based on these results, would you expect to find a large number of significant DE genes?\n\n\n3.8.2 MA plot\nAn MA-plot plots the mean expression and estimated log-fold-change for all genes in an analysis.\nRun the ma.r script in the 5_dge directory.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/scripts/ma.r\nThis generates a file named ma.png in the 5_dge folder. To view it, download it to your local disk.\n\n What do you think the blue dots represent?\n\n\n3.8.3 Volcano plot\nA related type of figure will instead plot fold change (on log2 scale) on the x-axis and -log10 p-value on the y-axis. Scaling like this means that genes with lowest p-value will be found at the top of the plot.\nRun the script named volcano.r in the 5_dge directory.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/scripts/volcano.r\nThis generates a file named volcano.png in the 5_dge folder. To view it, download it to your local disk.\n\n Anything noteworthy about the patterns in the plot? What do you think are the different colors? Is there a general trend in the direction of change in gene expression as a consequence of the experiment?\n\n\n3.8.4 Heatmap\nAnother popular plots for genome-wide expression patterns is heatmap for a set of genes. If you run the script called heatmap.r, it will extract the top 50 genes that have the lowest p-value in the experiment and create a heatmap from these. In addition to color-coding the expression levels over samples for the genes it also clusters the samples and genes based on inferred distance between them.\nRun the script named heatmap.r in the 5_dge directory.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/scripts/heatmap.r\nThis generates a file named heatmap.png in the 5_dge folder. To view it, download it to your local disk.\n\n Compare this plot to a similar plot in the paper behind the data."
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#bonus-exercises",
    "href": "topics/rnaseq/lab_rnaseq.html#bonus-exercises",
    "title": "RNASeq Workflow",
    "section": "4 Bonus exercises",
    "text": "4 Bonus exercises\n\n\n\n\n\n\nOptional\n\n\n\nThese exercises are optional and to be run only if you have time and you want to explore this topic further.\n\n\n\n4.1 Gene set analysis\nIn this part of the exercise, we will try some approaches to make sense of the differentially expressed genes. If you are not closely familiar with the genes and it’s relationship to your experimental setup, it’s hard to draw conclusions by simply looking at the gene list. We will use gene set analysis (GSA) and gene set enrichment analysis (GSEA) that ingests the gene list and returns to us functional terms which may be more meaningful in understanding the biological consequences of your experiment. We will use Gene Ontology (GO) database.\nWhen performing this type of analysis, one has to keep in mind that the analysis is only as accurate as the annotation available for your organism. So, if working with non-model organisms which do have experimentally-validated annotations (computationally inferred), the results may not be fully reflecting the actual situation.\nThere are many methods to approach the question as to which biological processes and pathways are over-represented amongst the differentially expressed genes, compared to all the genes included in the DE analysis. They use several types of statistical tests (e.g. hypergeometric test, Fisher’s exact test etc.), and many have been developed with microarray data in mind.\nWe will use the R / Bioconductor package clusterProfiler. This package provides methods for performing gene set analysis and gene set enrichment analysis.\nIn this part, we will use the same data as in the main workflow. The starting point of the exercise is the file with results of the differential expression produced in the main part of the exercise.\nChange to the 5_dge directory in your rnaseq directory.\ncd 5_dge\nLoad R module\nmodule load PDC/23.12\nmodule load R/4.4.0\nRun the GSE script from the linux terminal.\nRscript /sw/courses/ngsintro/rnaseq/dardel/main/scripts/gse-clusterprofiler.r\nIt will read the dge_results_full.rds and filter the table to get the differentially expressed genes and perform over-representation analysis. When completed, you should find the following files:\nls -l\n-rw-r--r--  1 user  staff   3.8M Jan  4 16:00 gse-bp.rds\n-rw-r--r--  1 user  staff   205K Jan  4 16:00 gse-bp.txt\n-rw-r--r--  1 user  staff   1.0M Jan  4 16:00 gse-cc.rds\n-rw-r--r--  1 user  staff    31K Jan  4 16:00 gse-cc.txt\n-rw-r--r--  1 user  staff   841K Jan  4 16:00 gse-mf.rds\n-rw-r--r--  1 user  staff    14K Jan  4 16:00 gse-mf.txt\nThe results are GO terms from three different gene ontology databases: Biological process (BP), cellular component (CC) and molecular function (MF). They are written as text files and RDS files which is an R specific format.\n Take a quick look at some of these files.\nhead gse-bp.txt | cut -f2\nDescription\nstriated muscle cell differentiation\nmuscle system process\ncellular component assembly involved in morphogenesis\nmuscle tissue development\nmuscle contraction\nmuscle organ development\nmuscle cell development\nstriated muscle cell development\nmuscle cell differentiation\nhead gse-cc.txt | cut -f2\nDescription\nmyofibril\nsarcomere\ncontractile fiber\nI band\nZ disc\nstriated muscle thin filament\nmyofilament\nactin cytoskeleton\nsynaptic membrane\n Explore the output files and figure out what the columns mean. Do you think the terms reflects the biology of the experiment we have just analysed?\n\n\n4.2 IGV browser\nData visualisation is important to be able to clearly convey results, but can also be very helpful as tool for identifying issues and note-worthy patterns in the data. In this part you will use the BAM files you created earlier in the RNA-seq lab and use IGV (Integrated Genomic Viewer) to visualise the mapped reads and genome annotations. In addition we will produce high quality plots of both the mapped read data and the results from differential gene expression.\nIf you are already familiar with IGV you can load the mouse genome and at least one BAM file from each of the treatments that you created earlier. The functionality of IGV is the same as if you look at genomic data, but there are a few of the features that are more interesting to use for RNA-seq data.\nIntegrated genomics viewer from Broad Institute is a nice graphical interface to view bam files and genome annotations. It also has tools to export data and some functionality to look at splicing patterns in RNA-seq data sets. Even though it allows for some basic types of analysis it should be used more as a nice way to look at your mapped data. Looking at data in this way might seem like a daunting approach as you can not check more than a few regions, but in in many cases it can reveal mapping patterns that are hard to catch with just summary statistics.\nFor this tutorial you can chose to run IGV directly on your own computer  or on HPC . If you chose to run it on your own computer you will have to download some of the BAM files (and the corresponding index files) from HPC. If you have not yet installed IGV you also have to download the program.\n\n\n\n\n\n\nLocal \n\n\n\nCopy two BAM files (one from each experimental group, for example; SRR3222409-19 and SRR3222412-19) and the associated index (.bam.bai) files to your computer by running the below command in a LOCAL terminal and NOT on HPC.\n Remember to replace username.\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/3_mapping/SRR3222409-19.bam .\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/3_mapping/SRR3222409-19.bam.bai .\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/3_mapping/SRR3222412-19.bam .\nscp username@dardel.pdc.kth.se:~/ngsintro/rnaseq/3_mapping/SRR3222412-19.bam.bai .\nAlternatively, you can use an SFTP browser like Filezilla or Cyberduck for a GUI interface. Windows users can also use the MobaXterm SFTP file browser to drag and drop.\n\n\n\n\n\n\n\n\nHPC \n\n\n\nFor Linux and Mac users, Log in to HPC in a way so that the generated graphics are exported via the network to your screen. This will allow any graphical interface that you start on your compute node to be exported to your computer. However, as the graphics are exported over the network, it can be fairly slow in redrawing windows and the experience can be fairly poor.\nLogin in to HPC with X-forwarding enabled:\nssh -Y username@dardel.pdc.kth.se\nssh -Y computenode\nAn alternative method is to login through ThinLinc. Once you log into this interface you will have a linux desktop interface in a browser window. This interface is running on the login node, so if you want to do any heavy lifting you need to login to your reserved compute node also here. This is done by opening a terminal in the running linux environment and log on to your compute node as before. NB! If you have no active reservation you have to do that first.\nLoad necessary modules and start IGV\nmodule load bioinfo-tools\nmodule load IGV/2.16.0\nigv-core\nThis should start the IGV so that it is visible on your screen. If not please try to reconnect to HPC or consider running IGV locally as that is often the fastest and most convenient solution.\n\n\nOnce we have the program running, you select the genome that you would like to load. Choose Mouse mm10. Note that if you are working with a genome that are not part of the available genomes in IGV, one can create genome files from within IGV. Please check the manual of IGV for more information on that.\nTo open your BAM files, go to File &gt; Load from file... and select your BAM file and make sure that you have a .bai index for that BAM file in the same folder. You can repeat this and open multiple BAM files in the same window, which makes it easy to compare samples. Then select Chr19 since we only have data for that one chromosome.\nFor every file you open a number of panels are opened that visualise the data in different ways. The first panel named Coverage summarises the coverage of base-pairs in the window you have zoomed to. The second panel shows the reads as they are mapped to the genome. If one right click with the mouse on the read panel there many options to group and color reads. The bottom panel named Refseq genes shows the gene models from the annotation.\nTo see actual reads you have to zoom in until the reads are drawn on screen. If you have a gene of interest you can also use the search box to directly go to that gene.\nHere is the list of top 20 differentially expressed genes on Chr19 ordered by absolute fold change. The adjusted p value is shown as padj, the average expression of the gene is shown as baseMean.\n   external_gene_name    baseMean log2FoldChange     lfcSE       stat       pvalue         padj\n1            AA387883    59.94083      -2.784689 0.2706306  -9.076824 1.117894e-19 1.943690e-17\n2              Tm7sf2   947.63388      -2.038574 0.1399445 -14.539498 6.809003e-48 1.677171e-44\n3              Ankrd2    59.28633       1.751990 0.2747018   6.323854 2.551188e-10 1.261004e-08\n4                Nrap   220.59983       1.699389 0.2680529   6.352080 2.124228e-10 1.067822e-08\n5              Slc1a1    63.82987       1.682163 0.2501657   6.651224 2.906652e-11 1.652208e-09\n6                Scd1  6678.59981      -1.447372 0.1767165  -8.190325 2.605209e-16 2.961722e-14\n7               Fads2  3269.79425      -1.415215 0.1201146 -11.779700 4.966970e-32 3.134559e-29\n8             Aldh1a7    65.81205      -1.354443 0.2413361  -5.548387 2.883175e-08 1.002599e-06\n9                Lbx1    26.03640       1.335612 0.2954499   4.503693 6.678261e-06 1.328372e-04\n10             Ms4a4b    23.27050       1.329984 0.2927615   4.729255 2.253447e-06 5.015617e-05\n11             Kcnip2    34.13675      -1.316531 0.2914951  -4.479672 7.475767e-06 1.463369e-04\n12               Pygm  1129.47092       1.245257 0.2912826   4.323631 1.534817e-05 2.736196e-04\n13              Rbm20    51.36261       1.181655 0.2927293   4.051754 5.083520e-05 7.774718e-04\n14            Gm50147    39.53043      -1.179758 0.2754761  -4.239382 2.241359e-05 3.794393e-04\n15            Macrod1   244.02353       1.082442 0.1904105   5.681007 1.339039e-08 4.984802e-07\n16               Scd2 44054.03491      -1.050473 0.1434332  -7.323814 2.410205e-13 1.874759e-11\n17              Fads1  4074.38868      -1.050393 0.1121926  -9.361514 7.860230e-21 1.470460e-18\n18             Plaat3  5284.36470      -1.046273 0.1317594  -7.940261 2.017561e-15 2.114719e-13\n19              Cd274    35.91671       1.017544 0.2938405   3.474263 5.122580e-04 5.632932e-03\n20              Ms4a1    25.80723       1.013036 0.2949268   3.437803 5.864549e-04 6.294275e-03\nHave a look at few of the interesting genes on Chr19 using the external_gene_name identifier. Look into gene Tm7sf2 or Ankrd2. You might have to right-click and change option to Squished to see more reads.\n Do you expect these genes to be differentially expressed?\nTo see some genes with large number of reads, see Scd1 or Scd2.\n\n\n\nIGV view of gene Tm7sf2 across 6 samples.\n\n\n\n\n\nIGV view of gene Ankrd2 across 6 samples.\n\n\n\n\n\nIGV view of gene Scd1 across 6 samples.\n\n\nFor more detailed information on the splice reads you can instead of just looking at the splice panel right click on the read panel and select Sashimi plots. This will open a new window showing in an easy readable fashion how reads are spliced in mapping and you will also be able to see that there are differences in between what locations reads are spliced. This hence gives some indication on the isoform usage of the gene.\n Do you think the reads are from a stranded or unstranded library?\n One can visualise all genes in a given pathway using the gene list option under Regions in the menu. If you need hints for how to proceed, see Gene List tutorial at Broad. But, we only have data from one chromosome, so this is not that useful now."
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#sbatch",
    "href": "topics/rnaseq/lab_rnaseq.html#sbatch",
    "title": "RNASeq Workflow",
    "section": "5 sbatch",
    "text": "5 sbatch\n\n\n\n\n\n\nNote\n\n\n\nYou are not required to run anything practically in this section. This is just to read and understand.\n\n\nWe have throughout this tutorial written bash scripts and run them from the terminal directly. Remember that we are not running on the login node. We have pre-allocated resources, then logged in to a compute node to run tasks. This is called working interactively on HPC. This is fine for tutorials and testing purposes. But, if you were to actually work on HPC, you would follow a slightly different approach.\nThe standard workflow on HPC is to login to the login node and then submit tasks as jobs to something called a Slurm queue. We haven’t used this option, because it involves waiting for an unpredictable amount of time for your submitted job to execute. In this section, we will take a look at how to modify a standard bash script to work with Slurm job submission.\nThis is how our standard bash script for mapping looks like:\n#!/bin/bash\n\n# load modules\nmodule load bioinfo-tools\nmodule load hisat2/2.2.1\nmodule load samtools/1.20\n\n# create output filename prefix\nprefix=$( basename \"$1\" | sed -E 's/_.+$//' )\n\nhisat2 \\\n-p 2 \\\n-x ../reference/mouse_chr19_hisat2/mouse_chr19_hisat2 \\\n--summary-file \"${prefix}.summary\" \\\n-1 $1 \\\n-2 $2 | samtools sort -O BAM &gt; \"${prefix}.bam\"\nWe add SBATCH commands to the above script. The new script looks like this:\n#!/bin/bash\n#SBATCH -A edu25.uppmax\n#SBATCH -p shared\n#SBATCH -c 8\n#SBATCH -t 3:00:00\n#SBATCH -J hisat2-align\n\n# load modules\nmodule load bioinfo-tools\nmodule load hisat2/2.2.1\nmodule load samtools/1.20\n\n# create output filename prefix\nprefix=$( basename \"$1\" | sed -E \"s/_.+$//\" )\n\nhisat2 \\\\\n -p 8 \\\\\n -x ../reference/mouse_chr19_hisat2/mouse_chr19_hisat2 \\\\\n --summary-file \"${prefix}-summary.txt\" \\\\\n -1 $1 \\\\\n -2 $2 | samtools sort -O BAM &gt; \"${prefix}.bam\nThe SBATCH commands in the above script is specifying the account name to use resources from, the required number of cores, the time required for the job and a job name.\n If you run this as a normal bash script like this ./hisat2_align.sh ... or bash ./hisat2_align.sh ..., the SBATCH comments have no effect (they are treated as comments) and the contents of the script will immediately start executing. But if you run this as script as sbatch ./hisat2_align.sh ..., the script is submitted as a job to the HPC Slurm queue. In this case, the SBATCH lines are interpreted and used by Slurm. At some point, your submitted job will reach the top of the queue and then the script will start to be executed.\nYou can check your jobs in the queue by running the following command.\nsqueue -u username\nAnd this gives a list like this:\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5856340    shared no-shell royfranc  R       0:05      1 nid002577\nIf the job is pending, then you will see PD in the ST column. If your job is running, you should see R. Once your job starts running, you will see a file named slurm-XXXX.out in the directory in which you submitted the job. This is the standard-out from that job. ie; everything that you would normally see printed to your screen when running locally, is printed to this file when running as a job. Once the job is over, one would inspect the slurm output file.\nhead slurm-XXXX.out\ntail slurm-XXXX.out\ncat slurm-XXXX.out"
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#nextflow",
    "href": "topics/rnaseq/lab_rnaseq.html#nextflow",
    "title": "RNASeq Workflow",
    "section": "6 Nextflow",
    "text": "6 Nextflow\n\n\n\n\n\n\nNote\n\n\n\nYou are not required to run anything practically in this section. This is just to read and understand.\n\n\nEverything you have done today and more is easily done using structured pipelines. Nextflow is a popular framework for creating bioinformatic pipelines and nf-core is a collection of curated analyses pipelines. Here, we will discuss the steps for analysing our current dataset using the nf-core rnaseq pipeline for bulk RNA-Seq. The nf-core website and pipeline specific pages are good sources of information on what the pipeline does and what parameters can be changed.\n\nWe need a samplesheet.csv to define our samples. In this case, the full size samples are used. For single-end reads, fastq_2 column is left empty.\nsample,fastq_1,fastq_2,strandedness\nko_1,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222409_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222409_2.fq.gz,unstranded\nko_2,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222410_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222410_2.fq.gz,unstranded\nko_3,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222411_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222411_2.fq.gz,unstranded\nwt_1,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222412_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222412_2.fq.gz,unstranded\nwt_2,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222413_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222413_2.fq.gz,unstranded\nwt_3,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222414_1.fq.gz,/sw/courses/ngsintro/rnaseq/rackham/main_full/1_raw/SRR3222414_2.fq.gz,unstranded\nWe need a params.config file to define parameters such as input, output, genome, annotation etc.\nparams.input = \"samplesheet.csv\"\nparams.outdir = \"results\"\nparams.aligner = \"star_salmon\"\n\nparams.fasta = \"/sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Sequence/WholeGenomeFasta/genome.fa\"\nparams.gtf = \"/sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Annotation/Genes/genes.gtf\"\ngene_bed = \"/sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Annotation/Genes/genes.bed\"\nstar_index = \"/sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Sequence/STARIndex/version2.7.x/\"\nAnd lastly we have a nextflow.sh which will be our bash script for launching the job.\n#!/bin/bash\n\n#SBATCH -A edu25.uppmax\n#SBATCH -p shared\n#SBATCH -c 10\n#SBATCH -t 8:00:00\n#SBATCH -J nf-core\n\nmodule load PDC/23.12\nmodule load singularity/4.1.1-cpeGNU-23.12\nmodule load bioinfo-tools\nmodule load nextflow/24.04.2\nmodule load nf-core/2.6\n\nexport NXF_HOME=$PWD\nexport SINGULARITY_CACHEDIR=${PWD}/SINGULARITY_CACHEDIR\nexport SINGULARITY_TMPDIR=${PWD}/SINGULARITY_TMPDIR\nexport NXF_SINGULARITY_CACHEDIR=${SINGULARITY_CACHEDIR}\nmkdir -p SINGULARITY_CACHEDIR SINGULARITY_TMPDIR\n\n# nextflow drop nf-core/rnaseq\nnextflow run nf-core/rnaseq -r 3.17.0 -c params.config -profile pdc_kth --project edu25.uppmax -resume'\nNotice that we use -profile pdc_kth specific to the HPC. The job is then submitted by simply running sbatch nextflow.sh.\nThe slurm output from this job looks like this:\n N E X T F L O W   ~  version 24.04.2\n\nPulling nf-core/rnaseq ...\n downloaded from https://github.com/nf-core/rnaseq.git\nLaunching `https://github.com/nf-core/rnaseq` [suspicious_meucci] DSL2 - revision: 00f924cf92 [3.17.0]\n\nWARN: Nextflow self-contained distribution allows only core plugins -- User config plugins will be ignored: nf-schema@2.1.1\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq 3.17.0\n------------------------------------------------------\nInput/output options\n  input                     : samplesheet.csv\n  outdir                    : results\n\nReference genome options\n  fasta                     : /sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Sequence/WholeGenomeFasta/genome.fa\n  gtf                       : /sw/data/igenomes/Mus_musculus/Ensembl/GRCm38/Annotation/Genes/genes.gtf\n\nAlignment options\n  min_mapped_reads          : 5\n\nInstitutional config options\n  config_profile_description: PDC profile.\n  config_profile_contact    : Pontus Freyhult (@pontus)\n  config_profile_url        : https://www.pdc.kth.se/\n\nCore Nextflow options\n  revision                  : 3.17.0\n  runName                   : suspicious_meucci\n  containerEngine           : singularity\n  launchDir                 : /cfs/klemming/projects/supr/snic2022-22-328/user/ngsintro/nextflow\n  workDir                   : /cfs/klemming/projects/supr/snic2022-22-328/user/ngsintro/nextflow/work\n  projectDir                : /cfs/klemming/home/r/user/proj-nbis/ngsintro/nextflow/assets/nf-core/rnaseq\n  userName                  : user\n  profile                   : pdc_kth\n  configFiles               :\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\n* The pipeline\n    https://doi.org/10.5281/zenodo.1400710\n\n* The nf-core framework\n    https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n    https://github.com/nf-core/rnaseq/blob/master/CITATIONS.md\n\nWARN: The following invalid input values have been detected:\n\n* --project: edu25.uppmax\n* --max_memory: 1.7 TB\n* --max_cpus: 256\n* --max_time: 7d\n* --schema_ignore_params: genomes,input_paths,cluster-options,clusterOptions,project,validationSchemaIgnoreParams\n* --validationSchemaIgnoreParams: genomes,input_paths,cluster-options,clusterOptions,project,schema_ignore_params\n* --validation-schema-ignore-params: genomes,input_paths,cluster-options,clusterOptions,project,schema_ignore_params\n\n[-        ] NFC…:PREPARE_GENOME:GTF_FILTER -\n[-        ] NFC…SEQ:PREPARE_GENOME:GTF2BED -\n[-        ] NFC…OME:MAKE_TRANSCRIPTS_FASTA -\n[-        ] NFC…ENOME:CUSTOM_GETCHROMSIZES -\n[-        ] NFC…AR_GENOMEGENERATE_IGENOMES -\n\n...\nskipping many lines here\n...\n\n[01/165fc2] NFC…QC_READDISTRIBUTION (wt_2) | 6 of 6 ✔\n[5b/1612c9] NFC…EQC_READDUPLICATION (wt_2) | 6 of 6 ✔\n[df/0115fa] NFC…_RNASEQ:RNASEQ:MULTIQC (1) | 1 of 1 ✔\n-[nf-core/rnaseq] Pipeline completed successfully -\nCompleted at: 11-Nov-2024 14:41:08\nDuration    : 1h 38m 43s\nCPU hours   : 57.9\nSucceeded   : 205\nThe output results directory looks like this:\nresults/\n├── fastqc\n│   ├── raw\n│   └── trim\n├── multiqc\n│   └── star_salmon\n├── pipeline_info\n│   ├── nf_core_rnaseq_software_mqc_versions.yml\n│   ├── params_2024-11-08_20-02-55.json\n│   ├── params_2024-11-08_22-29-50.json\n│   ├── params_2024-11-09_12-28-08.json\n│   └── params_2024-11-11_13-02-41.json\n├── star_salmon\n│   ├── bigwig\n│   ├── deseq2_qc\n│   ├── dupradar\n│   ├── featurecounts\n│   ├── ko_1\n│   ├── ko_1.markdup.sorted.bam\n│   ├── ko_1.markdup.sorted.bam.bai\n│   ├── ko_2\n│   ├── ko_2.markdup.sorted.bam\n│   ├── ko_2.markdup.sorted.bam.bai\n│   ├── ko_3\n│   ├── ko_3.markdup.sorted.bam\n│   ├── ko_3.markdup.sorted.bam.bai\n│   ├── log\n│   ├── null.merged.gene_counts_length_scaled.SummarizedExperiment.rds\n│   ├── null.merged.gene_counts_scaled.SummarizedExperiment.rds\n│   ├── null.merged.gene_counts.SummarizedExperiment.rds\n│   ├── null.merged.transcript_counts.SummarizedExperiment.rds\n│   ├── picard_metrics\n│   ├── qualimap\n│   ├── rseqc\n│   ├── salmon.merged.gene_counts_length_scaled.tsv\n│   ├── salmon.merged.gene_counts_scaled.tsv\n│   ├── salmon.merged.gene_counts.tsv\n│   ├── salmon.merged.gene_lengths.tsv\n│   ├── salmon.merged.gene_tpm.tsv\n│   ├── salmon.merged.transcript_counts.tsv\n│   ├── salmon.merged.transcript_lengths.tsv\n│   ├── salmon.merged.transcript_tpm.tsv\n│   ├── samtools_stats\n│   ├── stringtie\n│   ├── tx2gene.tsv\n│   ├── wt_1\n│   ├── wt_1.markdup.sorted.bam\n│   ├── wt_1.markdup.sorted.bam.bai\n│   ├── wt_2\n│   ├── wt_2.markdup.sorted.bam\n│   ├── wt_2.markdup.sorted.bam.bai\n│   ├── wt_3\n│   ├── wt_3.markdup.sorted.bam\n│   └── wt_3.markdup.sorted.bam.bai\n└── trimgalore\n    ├── ko_1_trimmed_1.fastq.gz_trimming_report.txt\n    ├── ko_1_trimmed_2.fastq.gz_trimming_report.txt\n    ├── ko_2_trimmed_1.fastq.gz_trimming_report.txt\n    ├── ko_2_trimmed_2.fastq.gz_trimming_report.txt\n    ├── ko_3_trimmed_1.fastq.gz_trimming_report.txt\n    ├── ko_3_trimmed_2.fastq.gz_trimming_report.txt\n    ├── wt_1_trimmed_1.fastq.gz_trimming_report.txt\n    ├── wt_1_trimmed_2.fastq.gz_trimming_report.txt\n    ├── wt_2_trimmed_1.fastq.gz_trimming_report.txt\n    ├── wt_2_trimmed_2.fastq.gz_trimming_report.txt\n    ├── wt_3_trimmed_1.fastq.gz_trimming_report.txt\n    └── wt_3_trimmed_2.fastq.gz_trimming_report.txt\nIf you want to look into the nextflow results yourself, you can check them out here: /sw/courses/ngsintro/rnaseq/dardel/main_full/nextflow. You can view the multiqc report here.\nThis pipeline takes you from raw reads to counts including read QC, mapping, mapping QC and quantification as well as a complete collated overview of all steps as a MultiQC report (multiqc/star_salmon/multiqc_report.html). The star_salmon/salmon.merged.gene_counts.tsv is the counts file that you would use for downstream differential gene expression using DESeq2.\nFor standard analyses steps, it is recommended to use a pipeline as the tools are up-to-date and the analyses steps are reproducible using exactly the same versions of tools."
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#simons-analysis",
    "href": "topics/rnaseq/lab_rnaseq.html#simons-analysis",
    "title": "RNASeq Workflow",
    "section": "7 Simon’s analysis",
    "text": "7 Simon’s analysis\nWe had Simon Andrews from the Babraham institute with us on one of the workshops and he did his own exploration into the dataset from this paper using his tool SeqMonk. It’s an excellent read and highly recommended. His report is available here."
  },
  {
    "objectID": "topics/rnaseq/lab_rnaseq.html#conclusion",
    "href": "topics/rnaseq/lab_rnaseq.html#conclusion",
    "title": "RNASeq Workflow",
    "section": "8 Conclusion",
    "text": "8 Conclusion\nWe hope that you enjoyed getting your hands wet working on some real-ish data. In this tutorial, we have covered the most important data processing steps that may be sufficient when the libraries are of good quality. If not, there are plenty of troubleshooting procedures to try before discarding the data. And once the count table is in place, the biostatistics and data mining begins. There are no well-defined solutions here, all depends on the experiment and questions to be asked, but we strongly advise learning R and/or Python. Not only to use the specifically designed statistical packages to analyse NGS count data, but also to be able to handle the data and results as well as to generate high-quality plots. There are many available tools and well-written tutorials with examples to learn from.\nFor those interested in RNA-Seq analysis, SciLifeLab offers a more advanced course in RNA-Seq analysis each semester. For more information, see SciLifeLab events."
  },
  {
    "objectID": "topics/other/lab_troubleshooting.html",
    "href": "topics/other/lab_troubleshooting.html",
    "title": "Status & Troubleshooting",
    "section": "",
    "text": "Useful commands for monitoring status and user activity on HPC.\n\n\nList all jobs running an a project\nsqueue -A edu25.uppmax | tail -n +2 | sort -k 4\nSample output\n  23186770      core _interac analopez  R    2:15:00      1 r55\n  23175660      core   (null)   anitav  R    6:52:28      1 r480\n  23188713      core   (null)    annha  R    1:42:59      1 r480\n  23185442      core   (null)    annha  R    2:47:11      1 r480\n  23185433      core   (null) inene424  R    2:47:42      1 r479\n  23185391      core   (null) inene424  R    2:50:04      1 r479\n  23185540      core   (null)  shengyz  R    2:46:20      1 r480\n  23185119      core   (null)    wingf  R    2:53:40      1 r479\n\n\n\nSorted list of jobs per user\nsqueue -A edu25.uppmax | tr -s ' ' | tail -n +2 | cut -d' ' -f5 | sort | uniq -c | sort -k1\nSample output\n      1 tami\n      1 tommal\n      1 valeriia\n      1 vioww\n      1 vishnupk\n      1 ylvafr\n      2 mehran96\n      2 miika\n      3 mariasve\n\n\n\n\nTotal number of cores used on a project\nsqueue -A edu25.uppmax -o %C | awk '{total += $0} END{print total}'\n\n\n\nAmount of storage space used per project\nprojinfo\nSample output\nInformation for compute project: edu24.uppmax (PI: mdahlo)\nUPPMAX intro course\nActive from 2024-09-29 00:00:00 to 2025-02-03 00:00:00\nMembers: asmae,ccollins,dianaek,emmers,eswku,heddaja,larsger,laurea,leonco,malina,mariaio,marziar,mdahlo,muyi,peihung,royfranc,rubencw,shaja23,svelo,szczot,theoseri,tudoran,vibe1827,xuji\ndardel: 2000 corehours/month, used 39.51% (790 corehours) during the past 30 days\nUsage by user royfranc: 36.85% (736 corehours)\n\n\n\nprojinfo\nSample output\nInformation for compute project: edu24.uppmax (PI: mdahlo)\nUPPMAX intro course\nActive from 2024-09-29 00:00:00 to 2025-02-03 00:00:00\nMembers: asmae,ccollins,dianaek,emmers,eswku,heddaja,larsger,laurea,leonco,malina,mariaio,marziar,mdahlo,muyi,peihung,royfranc,rubencw,shaja23,svelo,szczot,theoseri,tudoran,vibe1827,xuji\ndardel: 2000 corehours/month, used 39.51% (790 corehours) during the past 30 days\nUsage by user royfranc: 36.85% (736 corehours)\n\n\n\nList last activity in a directory for all users in a project (not possible on Dardel, as courses don’t have project directories)\nbash /sw/courses/utils/list_modification_times.sh /proj/edu25.uppmax/nobackup/\nSample output\nhkyle           2021-11-22 13:59:59     (/proj/snic2021-22-644/nobackup/hkyle/uppmax_tutorial/job_template)\nmalinh          2021-11-22 14:12:10     (/proj/snic2021-22-644/nobackup/malinh/slurm-23182638.out)\naliraz          2021-11-22 14:23:16     (/proj/snic2021-22-644/nobackup/aliraz/slurm-23184022.out)\nkristaps        2021-11-22 14:25:08     (/proj/snic2021-22-644/nobackup/kristaps/uppmax_tutorial/jobData.sam)\nanapin          2021-11-22 14:56:14     (/proj/snic2021-22-644/nobackup/anapin/uppmax_tutorial/uppmax_tutorial/jobData.sam)\nanalopez        2021-11-22 14:56:16     (/proj/snic2021-22-644/nobackup/analopez/uppmax_tutorial/jobData.sam)\nprivate         Not available     ()\n\n\n\nscontrol show res | grep edu25.uppmax\nSample output\nReservationName=snic2021-22-644_wed StartTime=2021-11-24T12:00:00 EndTime=2021-11-24T18:00:00 Duration=06:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\nReservationName=snic2021-22-644_thu StartTime=2021-11-25T08:30:00 EndTime=2021-11-25T17:30:00 Duration=09:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\nReservationName=snic2021-22-644_fri StartTime=2021-11-26T08:30:00 EndTime=2021-11-26T13:30:00 Duration=05:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\n\n\n\nFetch user information from username\nfinger username"
  },
  {
    "objectID": "topics/other/lab_troubleshooting.html#status",
    "href": "topics/other/lab_troubleshooting.html#status",
    "title": "Status & Troubleshooting",
    "section": "",
    "text": "Useful commands for monitoring status and user activity on HPC.\n\n\nList all jobs running an a project\nsqueue -A edu25.uppmax | tail -n +2 | sort -k 4\nSample output\n  23186770      core _interac analopez  R    2:15:00      1 r55\n  23175660      core   (null)   anitav  R    6:52:28      1 r480\n  23188713      core   (null)    annha  R    1:42:59      1 r480\n  23185442      core   (null)    annha  R    2:47:11      1 r480\n  23185433      core   (null) inene424  R    2:47:42      1 r479\n  23185391      core   (null) inene424  R    2:50:04      1 r479\n  23185540      core   (null)  shengyz  R    2:46:20      1 r480\n  23185119      core   (null)    wingf  R    2:53:40      1 r479\n\n\n\nSorted list of jobs per user\nsqueue -A edu25.uppmax | tr -s ' ' | tail -n +2 | cut -d' ' -f5 | sort | uniq -c | sort -k1\nSample output\n      1 tami\n      1 tommal\n      1 valeriia\n      1 vioww\n      1 vishnupk\n      1 ylvafr\n      2 mehran96\n      2 miika\n      3 mariasve\n\n\n\n\nTotal number of cores used on a project\nsqueue -A edu25.uppmax -o %C | awk '{total += $0} END{print total}'\n\n\n\nAmount of storage space used per project\nprojinfo\nSample output\nInformation for compute project: edu24.uppmax (PI: mdahlo)\nUPPMAX intro course\nActive from 2024-09-29 00:00:00 to 2025-02-03 00:00:00\nMembers: asmae,ccollins,dianaek,emmers,eswku,heddaja,larsger,laurea,leonco,malina,mariaio,marziar,mdahlo,muyi,peihung,royfranc,rubencw,shaja23,svelo,szczot,theoseri,tudoran,vibe1827,xuji\ndardel: 2000 corehours/month, used 39.51% (790 corehours) during the past 30 days\nUsage by user royfranc: 36.85% (736 corehours)\n\n\n\nprojinfo\nSample output\nInformation for compute project: edu24.uppmax (PI: mdahlo)\nUPPMAX intro course\nActive from 2024-09-29 00:00:00 to 2025-02-03 00:00:00\nMembers: asmae,ccollins,dianaek,emmers,eswku,heddaja,larsger,laurea,leonco,malina,mariaio,marziar,mdahlo,muyi,peihung,royfranc,rubencw,shaja23,svelo,szczot,theoseri,tudoran,vibe1827,xuji\ndardel: 2000 corehours/month, used 39.51% (790 corehours) during the past 30 days\nUsage by user royfranc: 36.85% (736 corehours)\n\n\n\nList last activity in a directory for all users in a project (not possible on Dardel, as courses don’t have project directories)\nbash /sw/courses/utils/list_modification_times.sh /proj/edu25.uppmax/nobackup/\nSample output\nhkyle           2021-11-22 13:59:59     (/proj/snic2021-22-644/nobackup/hkyle/uppmax_tutorial/job_template)\nmalinh          2021-11-22 14:12:10     (/proj/snic2021-22-644/nobackup/malinh/slurm-23182638.out)\naliraz          2021-11-22 14:23:16     (/proj/snic2021-22-644/nobackup/aliraz/slurm-23184022.out)\nkristaps        2021-11-22 14:25:08     (/proj/snic2021-22-644/nobackup/kristaps/uppmax_tutorial/jobData.sam)\nanapin          2021-11-22 14:56:14     (/proj/snic2021-22-644/nobackup/anapin/uppmax_tutorial/uppmax_tutorial/jobData.sam)\nanalopez        2021-11-22 14:56:16     (/proj/snic2021-22-644/nobackup/analopez/uppmax_tutorial/jobData.sam)\nprivate         Not available     ()\n\n\n\nscontrol show res | grep edu25.uppmax\nSample output\nReservationName=snic2021-22-644_wed StartTime=2021-11-24T12:00:00 EndTime=2021-11-24T18:00:00 Duration=06:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\nReservationName=snic2021-22-644_thu StartTime=2021-11-25T08:30:00 EndTime=2021-11-25T17:30:00 Duration=09:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\nReservationName=snic2021-22-644_fri StartTime=2021-11-26T08:30:00 EndTime=2021-11-26T13:30:00 Duration=05:00:00\n   Users=(null) Accounts=snic2021-22-644 Licenses=(null) State=INACTIVE BurstBuffer=(null) Watts=n/a\n\n\n\nFetch user information from username\nfinger username"
  },
  {
    "objectID": "topics/other/lab_troubleshooting.html#troubleshooting",
    "href": "topics/other/lab_troubleshooting.html#troubleshooting",
    "title": "Status & Troubleshooting",
    "section": "2 Troubleshooting",
    "text": "2 Troubleshooting\n\n2.1 X-forwarding\n\n2.1.1 Setup\nMac users\n\nInstall Xquartz and restart computer\nOpen the Xquartz terminal and connect to Uppmax\nIn my recent macos (10.xxx) , x-forwarding only works if I explicitly run the Xquartz terminal\n\nssh -XY username@dardel.pdc.kth.se\nssh -XY username@rackham.uppmax.uu.se\nAlso use this when logging in to the compute node!\nssh -XY username@node\nWindows users\nIn MobaXTerm, go to settings and make sure that X-forwarding is checked.\n\n\n2.1.2 Testing X-forwarding on Rackham\nType xeyes in the terminal.\n\n\n\n2.2 Open .html documents on Rackham\nFirst ensure X-forwarding works, then run firefox --no-remote filename.html\n\n\n2.3 SCP fails with *\nSometimes students have problems to download files with SCP when there is a * in the end of the line. For example; scp user@dardel.pdc.kth.se:~/ngsintro/HG00097.bam* .. It needs to be changed to scp user@dardel.pdc.kth.se:~/ngsintro/HG00097.bam\\* ..\n\n\n2.4 Thinlinc\n\n2.4.1 Login\nWhen logging in through an installed client, username/password seems to work. When logging in through the browser, username/password+2FA may be required.\n\n\n2.4.2 Minimize the ThinLinc window\nOn a Mac, press Fn + F8, then select: Minimize window.\n\n\n2.4.3 Gedit opening issues\nWhen opening it from the terminal (gedit &), it was not able to connect to a display to show the graphics. The DISPLAY variable was empty. This was when using ThinLinc, so it should have worked. The issue was solved by opening gedit from the menu. Gedit could be labelled Text Editor.\n\n\n\n2.5 Set persistent home directory in MobaXTerm\nThis is to specify where the home directory in MobaXTerm is located in the computer’s file system. In Settings &gt; Configuration, set the persistent home directory to a suitable folder. Restart MobaXTerm.\n\n\n2.6 Typing $ on a Swedish keyboard\nPress AltGr + 4.\n\n\n2.7 Black background for XQuartz windows on M1 Mac\nAs documented here, run:\ndefaults write org.xquartz.X11 enable_render_extension 0\n\n\n2.8 Thinlinc client installs in Swedish\nThere seems to be no option to change language in the app. So one will have to reinstall with some changes. In region settings, if English is set as primary and Swedish as secondary, it still installs in Swedish. The solution is to remove Swedish from the list completely and then reinstall."
  },
  {
    "objectID": "home_info.html",
    "href": "home_info.html",
    "title": "Practical Info",
    "section": "",
    "text": "Online\n\n\nOnline meeting links are sent to participants by email."
  },
  {
    "objectID": "home_info.html#location",
    "href": "home_info.html#location",
    "title": "Practical Info",
    "section": "",
    "text": "Online\n\n\nOnline meeting links are sent to participants by email."
  },
  {
    "objectID": "home_info.html#contact",
    "href": "home_info.html#contact",
    "title": "Practical Info",
    "section": "Contact",
    "text": "Contact\nThis workshop is run by the National Bioinformatics Infrastructure Sweden (NBIS) in collaboration with National Genomics Infrastructure (NGI). Both, NGI and NBIS are platforms at SciLifeLab.\nIf you would like to get in touch with us regarding this workshop, please contact us at edu.intro-ngs [at] nbis.se."
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html",
    "href": "topics/linux/lab_linux_intro.html",
    "title": "Introduction To Linux",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#zoom-link-for-help-over-zoom",
    "href": "topics/linux/lab_linux_intro.html#zoom-link-for-help-over-zoom",
    "title": "Introduction To Linux",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#connect-to-pdc",
    "href": "topics/linux/lab_linux_intro.html#connect-to-pdc",
    "title": "Introduction To Linux",
    "section": "2 Connect to PDC",
    "text": "2 Connect to PDC\nThe first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#logon-to-a-node",
    "href": "topics/linux/lab_linux_intro.html#logon-to-a-node",
    "title": "Introduction To Linux",
    "section": "3 Logon to a node",
    "text": "3 Logon to a node\nUsually you would do most of the work in this lab directly on one of the login nodes, but we have arranged for you to have one core each for better performance. This was covered briefly in the lecture notes.\nsalloc -A edu25.uppmax --reservation=edu25-03-24 -t 07:00:00 -p shared -n 1\ncheck which node you got (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u username\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node I got (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested (7 hours in this case) the session will shut down, and you will lose all unsaved data. Connect to this node from the login node.\nssh -Y nid001009"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#navigation",
    "href": "topics/linux/lab_linux_intro.html#navigation",
    "title": "Introduction To Linux",
    "section": "4 Navigation",
    "text": "4 Navigation\nIt is good to know how to move around in the file system. I’m sure you all have experienced this using a graphical user interface (GUI) before, Windows Explorer in Windows and Finder in OSX. Using the command line can be confusing at first, but the more your do it, the easier it gets.\nWhen you connect to PDC, you will start out in your home folder. The absolute path to your home folder is\n# structure\n/cfs/klemming/home/&lt;first letter of username&gt;/&lt;username&gt;\n\n# e.g. \n/cfs/klemming/home/u/username\n# or \n/cfs/klemming/home/a/anderson\nStart with looking at what you have in your home folder. The command for this ls, and it stand for LiSt (list).\nls -l\nThis is how my home folder looks like, and yours should look somewhat similar:\n[12:56:32] user@login1 ~ $ ls -l\ntotal 4384\ndrwxr-xr-x  3 user g20XXXXX   2048 Apr 27  2017 archive\n-rw-rw-r--  1 user user      49042 Sep 23  2016 bad.png\ndrwxr-xr-x  2 user user       2048 Mar 18  2016 data\n-rw-rw-r--  1 user user      60944 Sep 23  2016 good.png\ndrwxr-xr-x  4 user g20XXXXX   2048 Oct 30  2014 igv\ndrwxrwxr-x  5 user user       2048 Sep 20  2016 ngsintro\ndrwx--S---  2 user user       2048 May  4  2010 private\ndrwxr-xr-x 26 user user       4096 May 18 10:43 scripts\ndrwxrwxr-x  5 user user    2201600 May 14 14:02 work\n[12:57:36] user@login1 ~ $\nUsually when you are running analysis for real, you would not keep the data or analysis result files in your own home folder. You would store all files belonging to a project in a project folder that is shared among all the project members. Since this is a course project, we have not gotten a project folder so we will be storing all the files in a subfolder in our own home folder. We start off with creating this folder, our workspace, so that we have a place to organize our course files.\n# create the course folder\nmkdir ~/ngsintro\nAs seen in the lecture, the command for moving around is cd. The command stands for Change Directory and does exactly that. It is the equivalent of double clicking a folder in a GUI.\nTo enter the course folder, simply type\ncd ~/ngsintro\nWe can easily see that this is a relative path, since it does not start with a /. That means that this command will only work when you are standing in your home folder. If you are standing somewhere else and say that you want to enter a folder named ngsintro, the computer will tell you that there is no folder named ngsintro where you are located at the moment.\nThe absolute path to the workspace folder would be\n~/ngsintro or\n/cfs/klemming/home/u/username/ngsintro\nif you write out the whole path to your home folder.\nIt is the exact same thing as if you are using a GUI. If you are standing on your desktop, you can double click a folder which is located on your desktop. But if you are standing in another folder, you can’t double click on that same folder, because it is just not located there. You have to move to your desktop first, and then double click it.\nTyping ls -l all the time is.. more annoying than one would think, so someone came up with the good idea to add a shortcut here. If you type ll, it is the same as typing ls -l. Use it from now on.\n\n\n\n\n\n\nNote\n\n\n\nIf ll does not work (you get bash: ll: command not found), it means that you do not have that shortcut (alias) defined. You can do it yourself by defining the alias:\nalias ll=\"ls -lah --color --group-directories-first\"\nThis alias will only work in the terminal you ran it in, and as soon as you close that terminal it will be forgotten forever. If you want to have this alias available in all your future terminals, you can append the alias command to your autostart file (~/.bash_profile) by running this command:\necho -e '\\nalias ll=\"ls -lah --color --group-directories-first\"' &gt;&gt; ~/.bash_profile\nThe alias will now be available in all new terminals you open.\n\n\nNow we have practised moving around and looking at what we have in folders. The next step will show you how to do the same thing, but without the moving around part.\nIf we want to look at what we have in our home folder, while standing in the course’s workspace folder, we type\nll /cfs/klemming/home/&lt;first letter&gt;/&lt;username&gt;/\nand remember to substitute &lt;first letter&gt; with the first letter of your username, and &lt;username&gt; with your own user name.\nll /cfs/klemming/home/u/username/\nSince most programmers are lazy (efficient), there is a shortcut to your home folder so that you don’t have to write it all the time. If you write ~/ it means the same as if you would write\n/cfs/klemming/home/u/username/\nTry using it with ls:\nll ~/"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#copy-lab-files",
    "href": "topics/linux/lab_linux_intro.html#copy-lab-files",
    "title": "Introduction To Linux",
    "section": "5 Copy lab files",
    "text": "5 Copy lab files\nNow you will need some files. The files are located in the folder\n/sw/courses/ngsintro/linux/linux_tutorial\nor they can be downloaded if you are not logged on the cluster at the moment, files.tar.gz (instruction on how to download further down)\nFor structures sake, first create a folder called linux_tutorial inside the workspace folder, where you can put all your files belonging to this lab.\nmkdir ~/ngsintro/linux_tutorial\nNext, copy the lab files to this folder.\n# syntax\ncp -r &lt;source-folder&gt; &lt;destination-folder&gt;\n\ncp -r /sw/courses/ngsintro/linux/linux_tutorial/* ~/ngsintro/linux_tutorial\n-r denotes recursively, which means all the files including sub-folders of the source folder. Without it, only files directly in the source folder would be copied, not sub-folders and files in sub-folders.\n Remember to tab-complete to avoid typos and too much writing.\nIf you are unable to copy the files on PDC, you can download the files from this link instead of copying them. This is done with the command wget (web get). It works kind of the same way as the cp command, but you give it a source URL instead of a source file, and you can specify the destination by giving it a prefix, a path that will be appended in front on the file name when it’s downloaded.\ni.e; if you want to download the file http://somewhere.com/my.file and you give it the prefix ~/analysis/, the downloaded file will be saved as ~/analysis/my.file.\n# Ex:\nwget -P ~/analysis/ http://somewhere.com/my.file\n\n# or download the file to where you are standing by not specifying a prefix\nwget https://nbisweden.github.io/workshop-ngsintro/2503/topics/linux/assets/files.tar.gz"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#unpack-files",
    "href": "topics/linux/lab_linux_intro.html#unpack-files",
    "title": "Introduction To Linux",
    "section": "6 Unpack files",
    "text": "6 Unpack files\nGo to the folder you just copied and see what is in it.\n Remember to tab-complete to avoid typos and too much writing.\ncd ~/ngsintro/linux_tutorial\nll\ntar.gz is a file ending given to compressed files, something you will encounter quite often. Compression decreases the size of the files which is good when downloading, and it can take thousands of files and compress them all into a single compressed file. This is both convenient for the person downloading and speeds up the transfer more than you would think.\nTo unpack the files.tar.gz file use the following line while standing in the newly copied linux_tutorial folder.\ntar -xzvf files.tar.gz\nThe command will always be the same for all tar.gz files you want to unpack. -xzvf means eXtract from a Zipped file, Verbose (prints the name of the file being unpacked), from the specified File (f must always be the last of the letters).\nLook in the folder again and see what we just unpacked:\n[user@login1 linux_tutorial]$ ll\ntotal 512\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 a_strange_name\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 backed_up_proj_folder\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 external_hdd\n-rwxrwxr-x  1 user g20XXXXX  17198 Sep 24 13:19 files.tar.gz\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 important_results\ndrwxrwsr-x  2 user g20XXXXX 129024 Sep 19  2012 many_files\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 old_project\n-rwxrwxr-x  1 user g20XXXXX      0 Sep 24 13:19 other_file.old\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 part_1\ndrwxrwsr-x  2 user g20XXXXX   2048 Sep 19  2012 part_2\ndrwxrwsr-x  2 user g20XXXXX   2048 Jan 28  2012 this_has_a_file\ndrwxrwsr-x  2 user g20XXXXX   2048 Jan 28  2012 this_is_empty\n-rwxrwxr-x  1 user g20XXXXX      0 Sep 19  2012 useless_file"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#copying-and-moving-files",
    "href": "topics/linux/lab_linux_intro.html#copying-and-moving-files",
    "title": "Introduction To Linux",
    "section": "7 Copying and moving files",
    "text": "7 Copying and moving files\nLet’s move some files. Moving files might be one of the more common things you do, after cd and ls. You might want to organize your files in a better way, or move important result files to the project folder, who knows?\nWe will start with moving our important result to a backed-up folder. When months of analysis is done, the last thing you want is to lose your files. Typically this would mean that you move the final results to your project folder.\nIn this example, we want to move the result files only, located in the folder important_results, to our fake project folder, called backed_up_proj_folder.\nThe syntax for the move command is:\nmv &lt;source&gt; &lt;destination&gt;\nFirst, take a look inside the important_results folder:\n[user@login1 linux_tutorial]$ ll important_results/\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 dna_data_analysis_result_file_that_is_important-you_should_really_use_tab_completion_for_file_names.bam\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 temp_file-1\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 temp_file-2\nYou see that there are some unimportant temporary files that you have no interest in. Just to demonstrate the move command, I will show you how to move one of these temporary files to your backed-up project folder:\nmv important_results/temp_file-1 backed_up_proj_folder/\n Now do the same, but move the important DNA data file!\nLook in the backed-up project folder to make sure you moved the file correctly.\n[user@login1 linux_tutorial]$ ll backed_up_proj_folder/\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 dna_data_analysis_result_file_that_is_important-you_should_really_use_tab_completion_for_file_names.bam\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 last_years_data\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 temp_file-1\nAnother use for the move command is to rename things. When you think of it, renaming is just a special case of moving. You move the file to a location and give the file a new name in the process. The location you move the file to can very well be the same folder the file already is in. To give this a try, we will rename the folder a_strange_name to a better name.\nmv a_strange_name a_better_name\nLook around to see that the name change worked.\n[user@login1 linux_tutorial]$ mv a_strange_name a_better_name\n[user@login1 linux_tutorial]$ ll\ntotal 448\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 a_better_name\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 24 13:40 backed_up_proj_folder\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 external_hdd\n-rwxrwxr-x 1 user g20XXXXX  17198 Sep 24 13:36 files.tar.gz\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 24 13:40 important_results\ndrwxrwsr-x 2 user g20XXXXX 129024 Sep 19  2012 many_files\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 old_project\n-rwxrwxr-x 1 user g20XXXXX      0 Sep 24 13:36 other_file.old\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 part_1\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 part_2\ndrwxrwsr-x 2 user g20XXXXX   2048 Jan 28  2012 this_has_a_file\ndrwxrwsr-x 2 user g20XXXXX   2048 Jan 28  2012 this_is_empty\n-rwxrwxr-x 1 user g20XXXXX      0 Sep 19  2012 useless_file\nSometimes you don’t want to move things, you want to copy them. Moving a file will remove the original file, whereas copying the file will leave the original untouched. An example when you want to do this could be that you want to give a copy of a file to a friend. Imagine that you have a external hard drive that you want to place the file on. The file you want to give to your friend is data from last years project, which is located in your backed up project folder, backed_up_proj_folder/last_years_data\nAs with the move command, the syntax is\ncp &lt;source&gt; &lt;destination&gt;\ncp backed_up_proj_folder/last_years_data external_hdd/\nTake a look in the external_hdd to make sure the file got copied.\n[user@login1 linux_tutorial]$ cp backed_up_proj_folder/last_years_data external_hdd/\n[user@login1 linux_tutorial]$ ll external_hdd/\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 24 13:46 last_years_data"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#deleting-files",
    "href": "topics/linux/lab_linux_intro.html#deleting-files",
    "title": "Introduction To Linux",
    "section": "8 Deleting files",
    "text": "8 Deleting files\nSometimes you will delete files. Usually this is when you know that the file or files are useless to you, and they only take up space on your hard drive or your project’s folder.\nTo delete a file, we use the ReMove command, rm. Syntax:\n# syntax\nrm &lt;file to remove&gt;\nIf you want, you can also specify multiple files at once, as many as you want!\nrm &lt;file to remove&gt; &lt;file to remove&gt; &lt;file to remove&gt; &lt;file to remove&gt; &lt;file to remove&gt;\n\n Danger\nThere is no trash bin in Linux. If you delete a file, it is gone. So be careful when deleting stuff.\n\nTry it out by deleting the useless file in the folder you are standing in. First, look around in the folder to see the file.\n[user@login1 linux_tutorial]$ ll\ntotal 448\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 a_better_name\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 24 13:40 backed_up_proj_folder\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 24 13:46 external_hdd\n-rwxrwxr-x 1 user g20XXXXX  17198 Sep 24 13:36 files.tar.gz\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 24 13:40 important_results\ndrwxrwsr-x 2 user g20XXXXX 129024 Sep 19  2012 many_files\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 old_project\n-rwxrwxr-x 1 user g20XXXXX      0 Sep 24 13:36 other_file.old\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 part_1\ndrwxrwsr-x 2 user g20XXXXX   2048 Sep 19  2012 part_2\ndrwxrwsr-x 2 user g20XXXXX   2048 Jan 28  2012 this_has_a_file\ndrwxrwsr-x 2 user g20XXXXX   2048 Jan 28  2012 this_is_empty\n-rwxrwxr-x 1 user g20XXXXX      0 Sep 19  2012 useless_file\nNow remove it.\nrm useless_file\nSimilarly, folders can be removed too. There is even a special command for removing folders, rmdir. They work similar to rm, except that they can’t remove files. There are two folders, this_is_empty and this_has_a_file, that we now will delete.\nrmdir this_is_empty\nrmdir this_has_a_file\nIf you look inside this_has_a_file,\n[user@login1 linux_tutorial]$ ll this_has_a_file\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Jan 28  2012 file\nyou see that there is a file in there! Only directories that are completely empty can be deleted using rmdir. To be able to delete this_has_a_file, either delete the file manually and then remove the folder\nrm this_has_a_file/file\nrmdir this_has_a_file\nor delete the directory recursively, which will remove this_has_a_file and everything inside:\nrm -r this_has_a_file"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#open-files",
    "href": "topics/linux/lab_linux_intro.html#open-files",
    "title": "Introduction To Linux",
    "section": "9 Open files",
    "text": "9 Open files\nSo what happens if you give your files bad names like file1 or results? You take a break in a project and return to it 4 months later, and all those short names you gave your files doesn’t tell you at all what the files actually contain.\nOf course, this never happens because you ALWAYS name your files so that you definitely know what they contain (right?). But let’s say it did happen. Then the only way out is to look at the contents of the files and try to figure out if it is the file you are looking for.\n Now, we are looking for that really good script we wrote a couple of months ago in that other project. Look in the project’s folder, old_project and find the script.\n[user@login1 linux_tutorial]$ ll old_project/\ntotal 96\n-rwxrwxr-x 1 user g20XXXXX 39904 Sep 19  2012 a\n-rwxrwxr-x 1 user g20XXXXX     0 Sep 19  2012 stuff_1\n-rwxrwxr-x 1 user g20XXXXX  1008 Sep 19  2012 the_best\nNot so easy with those names.. We will have to use less to look at the files and figure out which is which.\n# syntax\nless &lt;filename&gt;\n Press q to close it down, use arrows keys to scroll up/down.\nHave a look at the_best, that must be our script, right?\nless old_project/the_best\nI guess not. Carrot cakes might be the bomb, but they won’t solve bioinformatic problems. Have a look at the file a instead.\nThat’s more like it!\nNow imagine that you had hundreds of files with weird names, and you really needed to find it. Lesson learned: name your files so that you know what they are! And don’t be afraid to create folders to organise files.\nAnother thing to think about when opening files in Linux is which program should you open the file in? The programs we covered during the lectures are nano and less. The main difference between these programs in that less can’t edit files, only view them. Another difference is that less doesn’t load the whole file into the RAM memory when opening it.\nSo, why care about how the program works? I’ll show you why. This time we will be opening a larger file, located in the course’s project folder. It’s 65 megabytes, so it is a tiny file compared with bio-data. Normal sequencing files can easily be 100-1000 times larger than this.\nFirst, open the file with nano.\n# load the nano module if you have not done so already\nmodule load nano\n\n# syntax\nnano &lt;filename&gt;\n\nnano /sw/courses/ngsintro/linux/linux_additional-files/large_file\n Press Ctrl+X to close it down, use arrows to scroll up/down).\nIs the file loaded yet? Now take that waiting time and multiply it with 100-1000. Now open the file with less. Notice the difference?\nhead and tail works the same was as less in this regard. They don’t load the whole file into RAM, they just take what they need.\nTo view the first rows of the large file, use head.\n# syntax\nhead &lt;filename&gt;\n\nhead /sw/courses/ngsintro/linux/linux_additional-files/large_file\nRemember how to view an arbitrary number of first rows in a file?\n# syntax\nhead -n &lt;number of rows to view&gt; &lt;filename&gt;\n\nhead -n 23 /sw/courses/ngsintro/linux/linux_additional-files/large_file\nThe same syntax for viewing the last rows of a file with tail:\n# syntax\ntail &lt;filename&gt;\n\ntail /sw/courses/ngsintro/linux/linux_additional-files/large_file\n\n\n# syntax\ntail -n &lt;number of rows to view&gt; &lt;filename&gt;\n\ntail -n 23 /sw/courses/ngsintro/linux/linux_additional-files/large_file"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#wildcards",
    "href": "topics/linux/lab_linux_intro.html#wildcards",
    "title": "Introduction To Linux",
    "section": "10 Wildcards",
    "text": "10 Wildcards\nSometimes (most of the time really) you have many files. So many that it would take you a day just to type all their names. This is where wildcards saves the day. The wildcard symbol in Linux is the star sign, * , and it means literally anything. Say that you want to move all the files which has names starting with sample_1_ and the rest of the name doesn’t matter. You want all the files belonging to sample_1. Then you could use the wildcard to represent the rest of the name.\n DO NOT run this command, it’s just an example.\nmv  sample_1_*  my_other_folder\nWe can try it out on the example files I have prepared. There are two folders called part_1 and part_2. We want to collect all the .txt files from both these folders in one of the folders. Look around in both the folders to see what they contain.\n[user@login1 linux_tutorial]$ ll part_1/\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 file_1.txt\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 file_2.txt\n[user@login1 linux_tutorial]$ ll part_2\ntotal 0\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 file_3.txt\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 file_4.txt\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 garbage.tmp\n-rwxrwxr-x 1 user g20XXXXX 0 Sep 19  2012 incomplete_datasets.dat\nWe see that part_1 only contains .txt files, and that part_2 contains some other files as well. The best option seems to be to move all .txt files from part_2 info part_1.\nmv part_2/*.txt part_1/\nThe wildcard works with most, if not all, Linux commands. We can try using wildcards with ls. Look in the folder many_files. Yes, there are hundreds of .docx files in there. But, there are a couple of .txt files in there as well. Find out how many .docx and .txt files exist.\n Try to figure out the solution on your own. Then check the answer below.\nll many_files/*.docx\nll many_files/*.txt"
  },
  {
    "objectID": "topics/linux/lab_linux_intro.html#utility-commands",
    "href": "topics/linux/lab_linux_intro.html#utility-commands",
    "title": "Introduction To Linux",
    "section": "11 Utility commands",
    "text": "11 Utility commands\nOk, the last 2 commands for now are top and man.\ntop can be useful when you want to look at which programs are being run on the computer, and how hard the computer is working. Type top and have a look.\ntop\n Press q to exit.\nTasks: 376 total,   2 running, 290 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  2.7 us,  1.3 sy,  0.0 ni, 95.3 id,  0.1 wa,  0.0 hi,  0.6 si,  0.0 st\nKiB Mem : 32590776 total, 16233548 free,  8394804 used,  7962424 buff/cache\nKiB Swap: 99999744 total, 99999744 free,        0 used. 22658832 avail Mem\n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND                       \n 3286 roy       20   0 4557248 522400 170808 R  12.3  1.6  62:49.20 gnome-shell                   \n 3113 roy       20   0 1282356 385012 290540 S   8.0  1.2  42:00.49 Xorg                          \n22213 roy       20   0 5474576 544848 101592 S   5.6  1.7 102:55.33 zoom                          \n 6186 roy       20   0  710140  60504  35836 S   3.0  0.2   0:00.62 terminator                    \n 4248 roy       20   0 2737604 556212 140580 S   2.7  1.7  54:51.48 QtWebEngineProc               \n 4632 roy       20   0 4866068 0.993g 281532 S   2.7  3.2  69:18.68 firefox                       \n 6548 roy       20   0 3703060 509340 189452 S   2.7  1.6  15:26.80 Web Content                   \n 9338 roy       20   0 4407324 846700 213324 S   2.7  2.6  15:53.71 Web Content                   \n 4776 roy       20   0 3310524 318364 102700 S   2.0  1.0   8:53.07 WebExtensions                 \n 6595 roy       20   0 4133152 992224 187540 S   1.3  3.0  18:51.05 Web Content                   \n  952 root     -51   0       0      0      0 S   1.0  0.0   2:40.89 irq/51-SYNA2393               \n 7800 roy       20   0 1213744 238536 129392 S   1.0  0.7  11:15.74 atom                          \n    1 root      20   0  226080   9836   6692 S   0.7  0.0   2:07.87 systemd                       \n 6690 roy       20   0 3492596 560304 166588 S   0.7  1.7   8:08.77 Web Content                   \n12895 roy       20   0 3320332 294820 172212 S   0.7  0.9   7:05.93 Web Content                   \n   10 root      20   0       0      0      0 I   0.3  0.0   2:43.21 rcu_sched                     \n 1052 root      20   0 2505296  36228  22444 S   0.3  0.1   3:45.16 containerd                    \n 2631 gdm       20   0 4044492 198480 142328 S   0.3  0.6   1:55.32 gnome-shell\nEach row in top corresponds to one program running on the computer, and the column describe various information about the program. The right-most column shows you which program the row is about.\nThere are mainly 2 things that are interesting when looking in top. The column %CPU describes how much cpu is used by each program. If you are doing calculations, which is what bioinformatics is mostly about, the cpu usage should be high. The numbers in the column is how many percent of a core the program is running. If you have a computer with 8 cores, you can have 8 programs using 100% of a core each running at the same time without anything slowing down. As soon as you start a 9th program, it will have to share a core with another program and those 2 programs will run at half-speed since a core can only work that fast. In the example above, program gnome-shell is using 12.3% of a core.\nThe column %MEM describes how much memory each program uses. The numbers mean how many percent of the total memory a program uses. In the example above, the program firefox is using 3.2% of the total memory.\nThe area in the top describes the overall memory usage. Total tells you how much memory the computer has, used tells you how much of the memory is being used at the moment, and free tells you how much memory is free at the moment.\nTotal = Used + Free\nA warning sign you can look for in top is when you are running an analysis which seems to take forever to complete, and you see that there is almost no cpu usage on the computer. That means that the computer is not doing any calculation, which could be bad. If you look at the memory usage at the same time, and see that it’s maxed out (used 100% of total), you can just abort the analysis.\nWhen the memory runs out, the computer more or less stops. Since it can’t fit everything into the RAM memory, it will start using the hard drive to store the things it can’t fit in the RAM. Since the hard drive is ~1000 times slower than the RAM, things will be going in slow-motion. The solution could be to either change the settings of the program you are running to decrease the memory usage (if the program has that functionality), or just get a computer with more memory.\nYou might wonder how the heck am I supposed to be able to remember all these commands, options and flags? The simple answer is that you won’t. Not all of them at least. You might remember ls, but was it -l or -a you should use to see hidden files? You might wish that there was a manual for these things.\nGood news everyone, there is a manual! To get all the nitty-gritty details about ls, you use the man command.\n# syntax\nman &lt;command you want to look at&gt;\n\nman ls\n\nLS(1)                            User Commands                            LS(1)\n\nNAME\n       ls - list directory contents\n\nSYNOPSIS\n       ls [OPTION]... [FILE]...\n\nDESCRIPTION\n       List  information  about  the  FILEs (the current directory by default).\n       Sort entries alphabetically if none of -cftuvSUX nor  --sort  is  speci‐\n       fied.\n\n       Mandatory arguments to long options are mandatory for short options too.\n\n       -a, --all\n              do not ignore entries starting with .\n\n       -A, --almost-all\n              do not list implied . and ..\n\n       --author\n              with -l, print the author of each file\n\n       -b, --escape\n              print C-style escapes for nongraphic characters\n\n       --block-size=SIZE\n              scale  sizes by SIZE before printing them; e.g., '--block-size=M'\n Manual page ls(1) line 1 (press h for help or q to quit)\nThis will open a less window (remember, q to close it down, arrows to scroll) with the manual page about ls. Here you will be able to read everything about ls. You’ll see which flag does what (-a is to show the hidden files, which in linux are files with a name starting with a dot .), which syntax the program has, etc. If you are unsure about how to use a command, look it up using man.\nThe man pages can be a bit tricky to understand at first, but you get used to it with time. If it is still unclear, try searching for it on the internet. You are bound to find someone with the exact same question as you, that has already asked on a forum, and gotten a good answer. 5 years ago.\n\n\n\n\n\n\nOptional\n\n\n\nIf you still have time left on the lab and you finished early, check out the Linux file permissions lab."
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html",
    "href": "topics/linux/lab_linux_advanced.html",
    "title": "Advanced Linux",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#zoom-link-for-help-over-zoom",
    "href": "topics/linux/lab_linux_advanced.html#zoom-link-for-help-over-zoom",
    "title": "Advanced Linux",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#connect-to-pdc",
    "href": "topics/linux/lab_linux_advanced.html#connect-to-pdc",
    "title": "Advanced Linux",
    "section": "2 Connect to PDC",
    "text": "2 Connect to PDC\nThe first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#logon-to-a-node",
    "href": "topics/linux/lab_linux_advanced.html#logon-to-a-node",
    "title": "Advanced Linux",
    "section": "3 Logon to a node",
    "text": "3 Logon to a node\nCheck which node you got when you booked resources this morning (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node I got (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested the session will shut down, and you will lose all unsaved data. Connect to this node from the login node.\nssh -Y nid001009\nIf the list is empty you can run the allocation command again and it should be in the list:\nsalloc -A edu25.uppmax --reservation=edu25-03-25 -t 04:00:00 -p shared -c 1"
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#first-things-first",
    "href": "topics/linux/lab_linux_advanced.html#first-things-first",
    "title": "Advanced Linux",
    "section": "4 First things first",
    "text": "4 First things first\nWe will have to load the module for nano before we can start editing files.\nmodule load nano\nLet’s make sure nano has syntax highlighting enabled. What that will do is to paint the boring code in pretty colors, making it much easier to read it. See the difference for yourself by first looking at this file before you enable it:\nnano /sw/courses/ngsintro/linux/qol/generate_random_protein.py\nClose down nano when you have seen how boing it looks without colors by pressing ctrl+x. Now, let’s enable syntax highlighting. To do this, we will simply tell nano to include the syntax highlighting instructions from a bunch of files that are already installed on the computer. Run this command to do just that:\nfind /pdc/software/eb/software/nano/7.2/share/nano/ -iname \"*.nanorc\" -exec echo include {} \\; &gt;&gt; ~/.nanorc\nThis command will put one line per language instructions (~30 of them, located in /pdc/software/eb/software/nano/7.2/share/nano/), into the nano autostart file (~/.nanorc) and put the word ‘include’ infront of each file name. That will make nano include the instructions from each of those files whenever it starts. Now have a look at the same file as before and enjoy the colors:\nnano /sw/courses/ngsintro/linux/qol/generate_random_protein.py\nThen close nano and continue with the lab."
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#copy-files-for-lab",
    "href": "topics/linux/lab_linux_advanced.html#copy-files-for-lab",
    "title": "Advanced Linux",
    "section": "5 Copy files for lab",
    "text": "5 Copy files for lab\nNow, you will need some files. To avoid all the course participants editing the same file all at once, undoing each other’s edits, each participant will get their own copy of the needed files. The files are located in the folder /sw/courses/ngsintro/linux/linux_advanced/\nNext, copy the lab files from this folder. -r means recursively, which means all the files including sub-folders of the source folder. Without it, only files directly in the source folder would be copied, NOT sub-folders and files in sub-folders.\n Remember to use tab-complete to avoid typos and too much writing.\ncp -r &lt;source&gt; &lt;destination&gt;\ncp -r /sw/courses/ngsintro/linux/linux_advanced ~/ngsintro/linux_advanced\nHave a look in ~/ngsintro/linux_advanced\ncd ~/ngsintro/linux_advanced\nll\nIf you see files, the copying was successful.\n\n\n\n\n\n\nTip\n\n\n\nIf you for some reason have problems copying the files, or if you are not on the cluster when running this lab, you can download these files here. You can download and unpack the file using the commands below:\nwget https://nbisweden.github.io/workshop-ngsintro/2503/topics/linux/assets/linux_advanced.tar.gz\nmkdir -p ~/ngsintro\ntar -C ~/ngsintro -xzvf linux_advanced.tar.gz\nThe -C ~/ngsintro in that command will make the files be unpacked in the folder ~/ngsintro instead of whichever folder you are in when you run the command. This makes sure the instructions in the lab will work as intended.\nAfter unpacking, continue the lab from the “Using variables” section."
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#using-variables",
    "href": "topics/linux/lab_linux_advanced.html#using-variables",
    "title": "Advanced Linux",
    "section": "6 Using variables",
    "text": "6 Using variables\nVariables are like small notes you write stuff on. If you want to save the value of something to be able to use it later, variables is the way to go. Let’s try assigning values to some variables and see how we use them.\na=5\nNow the values 5 is stored in the variable named a. To use the variable we have to put a $ sign in front of it so that bash knows we are referring to the variable a and not just typing the letter a. To see the result of using the variable, we can use the program echo, which prints whatever you give it to the terminal.\necho print this text to the terminal\necho \"you can use quotes if you want to\"\nAs you see, all the words you give it are printed just the way they are. Try putting the variable somewhere in the text.\necho Most international flights leave from terminal $a at Arlanda airport\nBash will see that you have a variable there and will replace the variable name with the value the variable have before sending the text to echo. If you change the value of a and run the exact command again you will see that it changes.\na=\"five\"\necho Most international flights leave from terminal $a at Arlanda airport\nSo without changing anything in the echo statement, we can make it output different things, all depending on the value of the variable a. This is one of the main points of using variables, that you don’t have to change the code or script but you can still make it behave differently depending on the values of the variables.\nYou can also do mathematics with variables, but we have to tell bash that we want to do calculations first. We do this by wrapping the calculations inside a dollar sign (telling bash it’s a variable) and double parentheses, i.e. $((5+5)).\na=4\necho $a squared is $(($a*$a))\n Write a echo command that will print out the volume of a rectangular cuboid, with the side lengths specified by variables named x, y, and z. To see that it works correctly, the volume of a rectangular cuboid with sides 5,5,5 is 125, and for 4,5,10 is 200. Give it a couple of tries on your own first. If you get completely stuck you can see a suggested solution below.\n\n\nSolution\n\nx=4\ny=5\nz=10\necho The volume of the rectangular cuboid with the sides $x,$y,$z is $(($x*$y*$z))."
  },
  {
    "objectID": "topics/linux/lab_linux_advanced.html#exercises",
    "href": "topics/linux/lab_linux_advanced.html#exercises",
    "title": "Advanced Linux",
    "section": "7 Exercises",
    "text": "7 Exercises\nFirst off, let’s open another terminal to the cluster so that you have 2 of them open. Scripting is a lot easier if you have one terminal on the command line ready to run commands and test things, and another one with a text editor where you write the actual code. That way you will never have to close down the text editor when you want to run the script you are writing on, and then open it up again when you want to continue editing the code.\nSo open a new terminal window, connect it to the cluster and then connect it to the node you have booked. Make sure both terminals are in the ~/ngsintro/linux_advanced directory, and start editing a new file with gedit or nano where you write your script. Name the file whatever you want, but in the examples I will refer to it as loop_01.sh. Write your loops to this file (or create a new file for each new example) and test run it in the other terminal.\n\n\n\n\n\n\nTip\n\n\n\nIf you get error messages like this when you run gedit, (gedit:27463): dconf-WARNING **: 10:59:00.575: failed to commit changes to dconf: Failed to execute child process “dbus-launch” (No such file or directory), and if you can’t change any preferences, you can try starting gedit through the graphical menu in ThinLinc instead. If you are using the Xfce desktop environment you should have a start-menu-like button at the top-left of the screen named Applications, or if you right-click somewhere on the desktop you should find it in the context menu that pops up. In the Applications menu, look in the category Accessories and you should find a program called Text editor which will start gedit.\n\n\nThe most simple loops are the ones that loop over a predefined list. You saw examples of this in the lecture slides, for example:\nfor i in \"Print these words\" one by one;\ndo\n    echo $i\ndone\nwhich will print the value of $i in each iteration of the loop. Write this loop in the file you are editing with gedit/nano, save the file, and then run it in the other terminal you have open.\nbash loop_01.sh\nAs you see, the words inside the quotation marks are treated as a single unit, unlike the words after. You can also iterate over numbers, so erase the previous loop you wrote and try this instead:\nfor number in 1 2 3;\ndo\n    echo $number\ndone\nIf everything worked correctly you should have gotten the numbers 1 2 3 printed to the screen. As you might guess, this way of writing the list of numbers to iterate over will not be usable once you have more than 10 or so numbers you want to loop over. Fortunately, the creators of bash (and most other computer languages) saw this problem coming a mile away and did something about it. To quickly create a list of numbers in bash, you can use something called a sequence expression to create the list for you.\nfor whatevernameyouwant in {12..72};  \ndo  \n    echo $whatevernameyouwant  \ndone  \n\n7.1 Exercise 1\n Let’s say it’s New Year’s Eve and you want to impress your friends with a computerized countdown of the last 10 seconds of the year (don’t we all?).\nStart off with getting a loop to count down from 10 to 0 first. Notice how fast the computer counts? That won’t do if it’s seconds we want to be counting down. Try looking the man page for the sleep command (man sleep) and figure out how to use it. The point of using sleep is to tell the computer to wait for 1 second after printing the number, instead of rushing to the next iteration in the loop directly. Try to implement this on your own.\n\n\nSolution\n\n# declare the values the loop will loop over\nfor secondsToGo in {10..0};\ndo\n    # print out the current number\n    echo $secondsToGo\n\n    # sleep for 1 second\n    sleep 1\n\ndone\n\n# Declare the start of a new year in a festive manner\necho Happy New Year everyone!!\n\n\n\n7.2 Exercise 2\nLet’s try to do something similar to the example in the lecture slides, to run the same commands on multiple files. In the Intro to high-performance computing lab, we learned how to use samtools to convert BAM files to SAM files so that humans can read them. In real life you will never do this, instead you will most likely always do it the other way around. SAM files take up ~4x more space on the hard drive compared to the same file in BAM format, so as soon as you see a SAM file you should instinctively convert it to a BAM file instead to conserve hard drive space. If you have many SAM files that needs converting you don’t want to sit there and type all the commands by hand like some kind of animal.\n Write a script that converts all the SAM files in a specified directory to BAM files. Incidentally, you can find 50 SAM files in need of conversion in the folder called sam in the folder you copied to your folder earlier in this lab (~/ngsintro/linux_advanced/sam). Bonus points if you make the program take the specified directory as an argument, and another bonus point if you get the program to name the resulting BAM file to the same name as the SAM file but with a .bam ending instead of .sam.\n\n\n\n\n\n\nTip\n\n\n\nRemember that you have to load the samtools module to be able to run it. The way you get samtools to convert a SAM file to a BAM file is by typing the following command:\nsamtools view -bS sample_1.sam &gt; sample_1.bam\nThe -b option tells samtools to output BAM format, and the -S option tells samtools that the input is in SAM format.\nRemember, Google is a good place to get help. If you get stuck, google bash remove file ending or bash argument to script and look for hits from StackOverflow/StackExchange or similar pages. There are always many different way to solve a problem. Try finding one you understand what they do and test if you can get them to work the way you want. If not, look for another solution and try that one instead.\n\n\nBasic, without bonus points:\n\n\nSolution\n\n# load the modules needed for samtools\nmodule load bioinfo-tools samtools\n\n# move to the SAM files directory to start with\ncd sam\n\n# use ls to get the list to iterate over\nfor file in *.sam;\ndo\n    # do the actual converting, just slapping on .bam at the end of the name\n    samtools view -bS $file &gt; $file.bam\ndone\n\nAdvanced, with bonus points:\n\n\nSolution\n\n# load the modules needed for samtools\nmodule load bioinfo-tools samtools\n\n# move to the SAM files directory to start with.\n# $1 contains the first argument given to the program\ncd $1\n\n# use ls to get the list to iterate over.\nfor file in *.sam;\ndo\n\n    # print a message to the screen so that the user knows what is happening.\n    # $(basename $file .sam) means that it will take the file name and remove .sam\n    # at the end of the name.\n    echo \"Converting $file to $(basename $file .sam).bam\"\n\n    # do the actual converting\n    samtools view -bS $file &gt; $(basename $file .sam).bam\ndone\n\n\n\n7.3 Exercise 3\nLet’s add a small thing to the exercise we just did. If there already exists a BAM file with the same name as the SAM file it’s not necessary to convert it again. Let’s use an if statement to check if the file already exists before we do the conversion.\nThe following if statement will check if a given filename exists, and prints a message depending on if it exists or not.\nFILE=$1\n\nif [ -f $FILE ];\nthen\n   echo \"File $FILE exists.\"\nelse\n   echo \"File $FILE does not exist.\"\nfi\nWhat we want to do is to check if the file doesn’t exists. The way to do that is to invert the answer of the check if the file does exist. To do that in bash, and many other languages, is to use the exclamation mark, !, which in these kinds of logical situations means NOT or the opposite of.\nFILE=$1\n\nif [ ! -f $FILE ];\nthen\n    echo \"File $FILE does not exist.\"\nfi\n Now, modify the previous exercise to only do the conversion if a file with the intended name of the BAM file doesn’t already exists. i.e; if you have a.sam and want to create a BAM file named a.bam, first check if a.bam already exists and only do the conversion if it does not exist.\nBasic:\n\n\nSolution\n\n# load the modules needed for samtools\nmodule load bioinfo-tools samtools\n\n# move to the SAM files directory to start with.\ncd sam\n\n# use ls to get the list to iterate over.\nfor file in *.sam;\ndo\n    # check if the intended output file does not already exists\n    if [ ! -f $file.bam ];\n    then\n        # do the actual converting, just slapping on .bam at the end of the name\n        samtools view -bS $file &gt; $file.bam\n    fi\ndone\n\nAdvanced:\n\n\nSolution\n\n# load the modules needed for samtools\nmodule load bioinfo-tools samtools\n\ncd $1\n\n# use ls to get the list to iterate over.\n# $1 contains the first argument given to the program\nfor file in *.sam;\ndo\n\n    # basename will remove the path information to the file, and will also remove the .sam ending\n    filename_bam=$(basename $file .sam)\n\n    # add the .bam file ending to the filename\n    filename_bam=$filename_bam.bam\n\n    # check if the intended output file does not already exists.\n    if [ ! -f $filename_bam ];\n    then\n\n        # print a message to the screen so that the user knows what is happening.\n        echo \"Converting $file to $filename_bam\"\n\n        # do the actual converting\n        samtools view -bS $file &gt; $filename_bam\n\n    else\n        # inform the user that the conversion is skipped\n        echo \"Skipping conversion of $file as $filename_bam already exist\"\n    fi\ndone\n\n\n\n7.4 Bonus exercise 1\nMaths and programming are usually a very good combination, so many of the examples of programming you’ll see involve some kind of maths. Now we will write a loop that will calculate the factorial of a number. As wikipedia will tell you, “the factorial of a non-negative integer n, denoted by n!, is the product of all positive integers less than or equal to n”, i.e. multiply all the integers, starting from 1, leading up to and including a number with each other.\nThe factorial of 5, written 5!, would be 1*2*3*4*5=120. Doing this by hand would start taking its time even after a couple of steps, but since we know how to loop that should not be a problem anymore.\n Write a loop that will calculate the factorial of a given number stored in the variable $n.\n\n\n\n\n\n\nTip\n\n\n\nA problem that you will encounter is that the sequence expression, {1..10}, from the previous exercise doesn’t handle variables. This is because of the way bash is built. The sequence expressions are handled before handling the variables so when bash tries to generate the sequence, the variable names have not yet been replaced with the values they contain. This leads to bash trying to create a sequence from 1 to $n, which of course doesn’t mean anything.\nTo get around this we can use a different way of generating sequences (there are always alternatives). There is a program called seq that does pretty much the same thing as the sequence expression, and since it is a program it will be executed after the variables have been handled. It’s as easy to use as the sequence expressions; instead of writing {1..10} just write $( seq 1 10 ).\nThe $() tells bash to run something in a subshell, which pretty much means it will run the command within the paratheses and then take whatever that command printed to the screen and replace the parantheses expression:\necho $(seq 1 5)\nbecomes\necho 1 2 3 4 5\n\n\n\n\nSolution\n\n# set the number you want to calculate the factorial of\nn=10\n\n# you have to initialize a variable before you can start using it.\n# Leaving this empty would lead to the first iteration of the loop trying\n# to use a variable that has no value, which would cause it to crash\nfactorial=1\n\n# declare the values the loop will loop over (1 to whatever $n is)\nfor i in $( seq 1 $n );\ndo\n\n    # set factorial to whatever factorial is at the moment, multiplied with the variable $i\n    factorial=$(( $factorial * $i ))\n\n    # an alternative solution which gives exactly the same result, but makes it a bit more readable maybe\n    # temporary_sum=$(( $factorial * $i ))\n    # factorial=$temporary_sum\n\ndone\n\n# print the result\necho The factorial of $n is $factorial\n\n\n\n7.5 Bonus exercise 2\nNow, let’s combine everything you’ve learned so far in this course.\n Write a script that runs the pipeline from the filetypes lab for each fastq file in a specified directory, using the same reference genome as in the filetype lab.\nIf that sounds too easy, make the script submits a slurm job for each sample that will run the pipeline for that sample on a calculation node (1 core, 5 minutes each). When the analysis is done, only fastq files and sorted and indexed BAM files should be in your folder.\nThere is a bunch of fastq files in the directory ~/ngsintro/linux_advanced/fastq that is to be used for this exercise.\nBasic solution:\n\n\nSolution\n\n# make the dummy pipeline available\nexport PATH=$PATH:/sw/courses/ngsintro/hpc/pipeline/dummy_scripts\n\n# index the reference genome\nreference_indexer -r ~/ngsintro/filetypes/0_ref/ad2.fa\n\n# go to the input files\ncd $1\n\n# loop over all the fastq files\nfor file in *.fastq;\ndo\n\n    # align the reads\n    align_reads -r ~/ngsintro/filetypes/0_ref/ad2.fa -i $file -o $file.sam\n\n    # convert the sam file to a bam file\n    sambam_tool -f bam -i $file.sam -o $file.bam\n\n    # sort the bam file\n    sambam_tool -f sort -i $file.bam -o $file.sorted.bam\n\n    # index the bam file\n    sambam_tool -f index -i $file.sorted.bam\n\ndone\n\nAdvanced solution:\n\n\nSolution\n\n# make the dummy pipeline available in this script\nexport PATH=$PATH:/sw/courses/ngsintro/hpc/pipeline/dummy_scripts\n\n# index the reference genome once, only if needed\nif [ ! -f ~/ngsintro/filetypes/0_ref/ad2.fa.idx ];\nthen\n    reference_indexer -r ~/ngsintro/filetypes/0_ref/ad2.fa\nfi\n\n\n# find out the absolute path to the input files\ncd $1\ninput_absolute_path=$(pwd)\n\n# go back to the previous directory now that the absolute path has been saved\ncd -\n\n# alternative way to get the absolute path to the input files\n# input_absolute_path=$(realpath $1)\n\n\n\n# loop over all the fastq files\nfor file in $input_absolute_path/*.fastq;\ndo\n\n    # print status report\n    echo Processing $file\n\n    # save the file name without the path information for convenience\n    file_basename=$(basename $file)\n\n    # save the file name without the file ending for convenience\n    file_prefix=$(basename $file .fastq)\n\n    # print a temporary script file that will be submitted to slurm\n    echo \"\"\"#!/bin/bash -l\n\n#SBATCH -A edu25.uppmax\n#SBATCH -p shared\n#SBATCH -c 1\n#SBATCH -t 00:05:00\n#SBATCH -J $file_basename\n\n# make the dummy pipeline available on the calculation node\necho \"Loading modules\"\nexport PATH=\\$PATH:/sw/courses/ngsintro/hpc/pipeline/dummy_scripts\n\n# go to the input files\ncd $input_absolute_path\n\n# align the reads\necho \"Aligning the reads\"\nalign_reads -r ~/ngsintro/filetypes/0_ref/ad2.fa -i $file_basename -o $file_prefix.sam                                                                                    \n\n# convert the SAM file to a BAM file\necho \"Converting sam to bam\"\nsambam_tool -f bam -i $file_prefix.sam -o $file_prefix.bam\n\n# sort the BAM file\necho \"Sorting the bam file\"\nsambam_tool -f sort -i $file_prefix.bam -o $file_prefix.sorted.bam\n\n# index the BAM file\necho \"Indexing the sorted bam file\"\nsambam_tool -f index -i $file_prefix.sorted.bam\n\n# rename the index file to the same as the .bam file but with different file ending\nmv $file_prefix.sorted.bam.bai $file_prefix.sorted.bai\n\n# delete the files you don't want to keep\nrm $file_prefix.sam $file_prefix.bam\n\n\necho \"Finished\"\n\"\"\" &gt; tmp.sbatch\n\n    # submit the temporary script file\n    sbatch tmp.sbatch\n\ndone\n\n# remove the temporary job file now that everything has been submitted\nrm tmp.sbatch"
  },
  {
    "objectID": "topics/other/lab_mac_keyboard.html",
    "href": "topics/other/lab_mac_keyboard.html",
    "title": "Keyboard Guide",
    "section": "",
    "text": "Normal\n\nHolding shift\n\nHolding alt\n\nHolding alt+shift"
  },
  {
    "objectID": "topics/other/lab_connect_uppmax.html",
    "href": "topics/other/lab_connect_uppmax.html",
    "title": "Connecting to UPPMAX",
    "section": "",
    "text": "We will teach you two different ways to connect to UPPMAX. From UPPMAX point of view it doesn’t matter which one you use, and you can change whenever you want to or even use both ways simultaiously. The first one is a text-based SSH connection, and the other one is a graphical remote desktop. The latter one is useful if you need to view images or documents in GUI programs without having to first download the image/document to your own computer first. Since it is using graphics, it will require you to have an internet connection that is good and stable.\nThe reason we will teach you two ways is that some parts of this course will require you to view plots and images, and adding an additional download step would just unneccesarily complicate things."
  },
  {
    "objectID": "topics/other/lab_connect_uppmax.html#ssh-connection",
    "href": "topics/other/lab_connect_uppmax.html#ssh-connection",
    "title": "Connecting to UPPMAX",
    "section": "1 SSH connection",
    "text": "1 SSH connection\nLet’s look at the text-based SSH approach first. This type of connection work just fine even on slow internet connections since it only transmitts small amounts of text when you work with it. You will need an SSH program to do this, which fortunately is included in most major operating systems:\n\n Linux: Use Terminal (Included by default)\n OSX: Use Terminal (Included by default)\n Windows: Use Powershell or Command prompt, both should be installed by default\n\n\n\n\n\n\n\nNote\n\n\n\nWhere username is mentioned, change to your user name.\n\n\nFire up the available SSH program and enter the following:\nssh username@rackham.uppmax.uu.se\nEnter your password when prompted. As you type, nothing will show on the screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter, it will be fine.\nNow your screen should look something like this:\ndahlo@dahlo-xps ~ $ ssh dahlo@rackham.uppmax.uu.se\nLast login: Fri May 18 15:03:59 2018 from mi04.icm.uu.se\n _   _ ____  ____  __  __    _    __  __\n| | | |  _ \\|  _ \\|  \\/  |  / \\   \\ \\/ /   | System:    rackham4\n| | | | |_) | |_) | |\\/| | / _ \\   \\  /    | User:      dahlo\n| |_| |  __/|  __/| |  | |/ ___ \\  /  \\    |\n \\___/|_|   |_|   |_|  |_/_/   \\_\\/_/\\_\\   |\n\n###############################################################################\n\n       User Guides: http://www.uppmax.uu.se/support/user-guides\n       FAQ: http://www.uppmax.uu.se/support/faq\n\n       Write to support@uppmax.uu.se, if you have questions or comments.\n\n\ndahlo@rackham4 ~ $\nNow you are connected to UPPMAX and can start working."
  },
  {
    "objectID": "topics/other/lab_connect_uppmax.html#remote-desktop",
    "href": "topics/other/lab_connect_uppmax.html#remote-desktop",
    "title": "Connecting to UPPMAX",
    "section": "2 Remote desktop",
    "text": "2 Remote desktop\nYou can work on UPPMAX interactively through a graphical-user-interface (GUI) desktop environment using ThinLinc.\nWe have a ThinLinc server running at one of the login nodes which allows users to run a remote desktop. It can be reached from a web browser (Chrome and Firefox are the recommended web browsers) or from the ThinLink App. For more details please look here: https://docs.uppmax.uu.se/getting_started/login_rackham_remote_desktop_local_thinlinc_client/\n\n2.1 Web browser\nTo be able to login via a web browser you will have to set up two-factor authentication first. Follow the instructions at the UPPMAX homepage, and once you are done you can continue below.\n\nGo to the login page, https://rackham-gui.uppmax.uu.se/\nEnter your UPPMAX username.\nEnter your UPPMAX password, followed by your current two-factor authentication code. Eg. if your password is hunter2 and your current two-factor authentication code is 123456 you will enter hunter2123456 as your password.\n\nIt will ask you which profile you want to use, so first press the Forward button. Then you can choose which desktop environment you want to use. Xfce is pretty straight-forward and easy to use, but feel free to try either of them. You get to choose every time you login so it’s not a permanent choice.\n\nOnce your desktop has been loaded, start a terminal either by clicking the black terminal icon at the bottom of the screen, or by pressing the Applications button in the top left corner and select Terminal Emulator.\n\n\n\n\n2.2 ThinLink App\n\nIf you haven’t already done so, download the ThinLinc client matching your local computer (i.e Windows, Linux, MacOS X or Solaris) from https://www.cendio.com/thinlinc/download and install it.\nLaunch the ThinLinc client. You should see a form where you can enter your username and password, and possibly a server name. If you only see this simple form as shown below, you can click Advanced to be able to set the server name.\n\nChange the Server setting to rackham-gui.uppmax.uu.se.\nChange the Name setting to your UPPMAX username.\nSet the Password setting to your UPPMAX password.\nYou do not need to change any other settings.\nYou will first come to the ThinLinc profile chooser. Press the Forward button to continue. Then you can choose which desktop environment you want to use. Xfce is pretty straight-forward and easy to use, but feel free to try either of them. You get to choose every time you login so it’s not a permanent choice.\nPress the Connect button.\nIf you connect for the first time you will see the “The server’s host key is not cached …” dialog.\n\nOnce your desktop has been loaded, start a terminal either by clicking the black terminal icon at the bottom of the screen, or by pressing the Applications button in the top left corner and select Terminal Emulator.\n\n\n\nTwo factor authentication: The ThinLinc client connects over SSH which means it may be required to present a two factor authentication code. If you need to use this when logging in with SSH you also need to use it when logging in with ThinLinc (it depends on where you connect from). The ThinLinc client does not know how to ask for the two factor code, so you will need to use the grace time feature. To do this, first you have to connect with regular SSH and present the required two factor code. Once you have logged in over SSH you can safely exit again. The login server will remember that you just logged in for a few minutes and will not ask for two factor authentication again, so make sure you do not wait too long to connect with the ThinLinc client."
  },
  {
    "objectID": "topics/other/lab_connect_uppmax.html#after-connection-to-uppmax",
    "href": "topics/other/lab_connect_uppmax.html#after-connection-to-uppmax",
    "title": "Connecting to UPPMAX",
    "section": "3 After connection to UPPMAX",
    "text": "3 After connection to UPPMAX\nFrom this point forward there is no difference between the two different ways of connection to UPPMAX. Both ways result in you having a terminal running on UPPMAX and from UPPMAX point of view they are the same."
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html",
    "href": "topics/linux/lab_linux_permissions.html",
    "title": "File permissions",
    "section": "",
    "text": "As Linux can be a multi-user environment it is important that files and directories can have different owners and permissions to keep people from editing or executing your files.\n\n\nThe permissions are defined separately for users, groups and others.\nThe user is the username of the person who owns the file. By default the user who creates a file will become its owner. The group is a group of users that co-own the file. They will all have the same permissions to the file. This is useful in any project where a group of people are working together. The others is quite simply everyone else’s permissions.\n\n\n\nThere are four permissions that a file or directory can have. Note the one character designations/flags, r,w,x and -.\nIn all cases, if the file or directory has the flag it means that it is enabled.\nRead: r\nFile: Whether the file can be opened and read.\nDirectory: Whether the contents of the directory can be listed.\nWrite: w\nFile: Whether the file can be modified. (Note that for renaming or deleting a file you need additional directory permissions.)\nDirectory: Whether the files in the directory can be renamed or deleted.\nExecute: x\nFile: Whether the file can be executed as a program or shell script.\nDirectory: Whether the directory can be entered using cd.\nNo permissions: -"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#ownership-permissions",
    "href": "topics/linux/lab_linux_permissions.html#ownership-permissions",
    "title": "File permissions",
    "section": "",
    "text": "As Linux can be a multi-user environment it is important that files and directories can have different owners and permissions to keep people from editing or executing your files.\n\n\nThe permissions are defined separately for users, groups and others.\nThe user is the username of the person who owns the file. By default the user who creates a file will become its owner. The group is a group of users that co-own the file. They will all have the same permissions to the file. This is useful in any project where a group of people are working together. The others is quite simply everyone else’s permissions.\n\n\n\nThere are four permissions that a file or directory can have. Note the one character designations/flags, r,w,x and -.\nIn all cases, if the file or directory has the flag it means that it is enabled.\nRead: r\nFile: Whether the file can be opened and read.\nDirectory: Whether the contents of the directory can be listed.\nWrite: w\nFile: Whether the file can be modified. (Note that for renaming or deleting a file you need additional directory permissions.)\nDirectory: Whether the files in the directory can be renamed or deleted.\nExecute: x\nFile: Whether the file can be executed as a program or shell script.\nDirectory: Whether the directory can be entered using cd.\nNo permissions: -"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#interpreting-permissions",
    "href": "topics/linux/lab_linux_permissions.html#interpreting-permissions",
    "title": "File permissions",
    "section": "2 Interpreting permissions",
    "text": "2 Interpreting permissions\nMake an empty directory we can work in and make a file.\nuser@login1 ~ $ cd ~/ngsintro\nuser@login1 ngsintro $ mkdir advlinux\nuser@login1 ngsintro $ cd advlinux\nuser@login1 advlinux $ touch  filename\nuser@login1 advlinux $ ls -lh\ntotal 0\n -rw-r--r-- 1 user g20XXXXX 0 Sep 21 13:54 filename\"))\n(-lh means long and human readable, displaying more information about the files or directories in a human understandable format)\nThis shows us a cryptic line for each file/folder, where the columns are as following:\n-rw-r--r--   : permissions\n1            : number of linked hard-links\nuser         : owner of the file\ng20XXXXX     : to which group this file belongs to\n0            : file size\nSep 21 13:54 : modification/creation date and time\nfilename     : file/directory name\nThe first segment, -rw-r--r--, describes the ownerships and permissions of our newly created file. The very first character, in this case -, shows the file’s type. It can be any of these:\nCommon ones\nd = directory\n- = regular file\nl = symbolic link\nLess common ones\ns = Unix domain socket\np = named pipe\nc = character device file\nb = block device file\nAs expected the file we have just created is a regular file. Ignore the types other than directory, regular and symbolic link as they are outside the scope of this course.\nThe next nine characters, in our case rw-r--r--, can be divided into three groups consisting of three characters in order from left to right. In our case rw-, r-- and r--. The first group designates the users permissions, the second the groups permissions and the third the others permissions. As you may have guessed the within group permissions are ordered, the first always designates read permissions, the second write and the third executability.\nThis translates our files permissions to say this -rw-r--r--:\n- It is a regular file.\n- The user has read & write permission, but not execute.\n- The group has read permission but not write and execute.\n- Everyone else (other), have read permission but not write and execute.\nAs another example, lets create a directory.\nuser@login1 advlinux $ mkdir directoryname\nuser@login1 advlinux $ ls -lh\n\ntotal 0\ndrwxr-xr-x  2 user  g20XXXXX    68B Sep 21 14:41 directoryname\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 13:54 filename\nAs you can see the first character correctly identifies it as d, a directory, and all user groups have x, execute permissions, to enter the directory by default."
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#editing-ownership-permissions",
    "href": "topics/linux/lab_linux_permissions.html#editing-ownership-permissions",
    "title": "File permissions",
    "section": "3 Editing Ownership & Permissions",
    "text": "3 Editing Ownership & Permissions\nThe command to set file permission is chmod which means CHange MODe. Only the owner can set file permissions.\n\nFirst you decide which group you want to set permissions for. User, u, group, g, other, o, or all three, a.\nNext you either add, +, remove, -, or wipe out previous and add new, =, permissions.\nThen you specify the kind of permission: r,w,x, or -.\n\nLets revisit our example file and directory to test this.\nuser@login1 advlinux $ ls -lh\n\ntotal 0\ndrwxr-xr-x  2 user  g20XXXXX    68B Sep 21 14:41 directoryname\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 13:54 filename\n\nuser@login1 advlinux $ chmod a=x filename\nuser@login1 advlinux $ ls -lh\n\ntotal 0\ndrwxr-xr-x  2 user  g20XXXXX    68B Sep 21 14:41 directoryname\n---x--x--x  1 user  g20XXXXX     0B Sep 21 13:54 filename\nAs you can see this affected all three, a, it wiped the previous permissions, =, and added an executable permission, x, to all three groups.\nTry some others both on the file and directory to get the hang of it.\nchmod g+r   filename\nchmod u-x   filename\nchmod ug=rx filename\nchmod a=-   filename\nchmod a+w   directoryname\n In no more than two commands, change the file permissions from\n----------\nto\n-rw-rw--wx\nNotice also that we here gave everyone writing permission to the file, that means that ANYONE can write to the file. Not very safe.\nchmod ug+rw filename\nchmod o=wx filename"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#symbolic-links",
    "href": "topics/linux/lab_linux_permissions.html#symbolic-links",
    "title": "File permissions",
    "section": "4 Symbolic links",
    "text": "4 Symbolic links\n\n4.1 Files\nMuch like a windows user has a shortcut on his desktop to WorldOfWarcraft.exe, being able to create links to files or directories is good to know in Linux. An important thing to remember about symbolic links is that they are not updated, so if you or someone else moves or removes the original file/directory the link will stop working.\nMake sure you are standing in the directory ~/ngsintro/advlinux. Then remove our old file and directory.\n\n Danger\nrm -r * permanently removes all folder and subfolders from where you are standing. Be extremely careful when using this. Always double check current working directory or path.\n\nrm -r *\nNow that the directory is empty, let’s make a new folder and a new file in that folder.\nmkdir stuff\ntouch stuff/linkfile\nLets put some information into the file, just put some text, anything, like “slartibartfast” or something.\nnano stuff/linkfile\nNow let’s create a link to this file in our original folder. ln stands for link and -s makes it symbolic. The other options are not in the scope of this course, but feel free to read about them on your own, https://stackoverflow.com/a/29786294.\nuser@login1 advlinux $ ln -s stuff/linkfile\nuser@login1 advlinux $ ls -l\n\ntotal 8\nlrwxr-xr-x  1 user  g20XXXXX    14B Sep 21 15:38 linkfile -&gt; stuff/linkfile\ndrwxr-xr-x  3 user  g20XXXXX   102B Sep 21 15:36 stuff\nNotice that we see the type of the file is l, for symbolic link, and that we have a pointer after the links name for where the link goes, -&gt; stuff/linkfile.\nIf want you to change the information in the file using the link file, then you should change the information in the original file.\n Change the information using the original file, then check the link. Has the information changed in the original file and the linked file?\n Now move or delete the original file. What happens to the link? What information is there now if you open the link?\n Now create a new file in stuff/ with exactly the same name that your link file is pointing too with new information in it. What happens now? Is the link still broken? What is the content of the linked file now?\n\n\n4.2 Directories\nNow let’s create a link to a directory. Lets clean our workspace.\nrm -r *\nAnd create a directory three, arbitrarily, two directories away. The -p option to mkdir will make it create all 3 directories as needed. Without it, it would crash saying it can’t create three because the directory two does not exist.\nmkdir -p one/two/three\nNow let’s enter the directory and create some files there.\nuser@login1 advlinux $ cd one/two/three\nuser@login1 advlinux $ touch a b c d e\nuser@login1 advlinux $ ls -lh\n\ntotal 0\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 16:11 a\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 16:11 b\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 16:11 c\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 16:11 d\n-rw-r--r--  1 user  g20XXXXX     0B Sep 21 16:11 e\nReturn to our starting folder and create a symbolic link to folder three.\nuser@login1 advlinux $ cd ../../..\nuser@login1 advlinux $ ln -s one/two/three\nuser@login1 advlinux $ ls -lh\n\ntotal 8\ndrwxr-xr-x  3 user  g20XXXXX   102B Sep 21 16:11 one\nlrwxr-xr-x  1 user  g20XXXXX    13B Sep 21 16:13 three -&gt; one/two/three\nOnce again, we see that it is correctly identified as a symbolic link, l, that it’s default name is the same as the directory it is pointing to, same as the files link had the same name as the file by default previously, and that we have the additional pointer after the links name showing us where it’s going.\n Run ls and cd on the link. Does it act just as if you were standing in directory two/ performing the very same actions on three/?\n After entering the link directory using cd, go back one step using cd .., where do you end up?\nMoving, deleting or renaming the directory would, just like in the case with the file, break the link."
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#grep",
    "href": "topics/linux/lab_linux_permissions.html#grep",
    "title": "File permissions",
    "section": "5 Grep",
    "text": "5 Grep\nSome files can be so large that opening it in a program would be very hard on your computer. It could be a file containing biological data, it could be a log file of a transfer where we want check for any errors. No matter the reason, a handy tool to know is the grep command.\ngrep searches for a specific string in one or more files. Case sensitive/insensitive or regular expressions work as well.\nLet’s start, as always, by cleaning our directory.\nrm -r *\nThen let’s create a file with some text in it that we can work with. I have supplied some great text below.\nuser@login1 advlinux $ nano textfile\n\nCats sleep anywhere, any table, any chair.\nTop of piano, window-ledge, in the middle, on the edge.\nOpen draw, empty shoe, anybody's lap will do.\nFitted in a cardboard box, in the cupboard with your frocks.\nAnywhere! They don't care! Cats sleep anywhere.\nNow let’s see how the grep command works. The syntax is:\ngrep \"string\" filename/filepattern\nSome examples for you to try and think about:\ngrep \"Cat\" textfile\ngrep \"cat\" textfile\nAs you can see the last one did not return any results. Add a -i for case insensitive search.\ngrep -i \"cat\" textfile\nNow let’s copy the file and check both of them together by matching a pattern for the filenames.\ncp textfile textcopy\ngrep \"Cat\" text*\nThe * will ensure that any file starting with text and then anything following will be searched. This example would perhaps be more real if we had several text files with different texts and we were looking for a specific string in any of them.\nCopy the file sample_1.sam to your folder using the command below\ncp /sw/courses/ngsintro/linux/linux_additional-files/sample_1.sam .\n Use grep to search in the file for a specific string of nucleotides, for example:\ngrep \"TACCACCGAAATCTGTGCAGAGGAGAACGCAGCTCCGCCCTCGCGGTGCTCTCCGGGTCTGTGCTGAGGAG\" sample_1.sam\n Try with shorter sequences. When do you start getting lots of hits? This file is only a fraction of a genome, you would have gotten many times more hits doing this to a complete many GB large sam file.\n Use grep to find all lines with chr1 in them. This output is too much to be meaningful. Send it to a file (&gt;) where you have now effectively stored all the chromosome 1 information."
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#piping",
    "href": "topics/linux/lab_linux_permissions.html#piping",
    "title": "File permissions",
    "section": "6 Piping",
    "text": "6 Piping\nA useful tool in linux environment is the use of pipes. What they essentially do is connect the first command to the second command and the second command to the third command etc for as many commands as you want or need.\nThis is often used in jobs and other analysis as there are three major benefits. The first is that you do not have to stand in line to get a core or a node twice. The second is that you do not generate intermediary data which will clog your storage, you go from start file to result. The third is that it may actually be faster.\nThe pipe command has this syntax\ncommand 1 | command 2\nThe | is the pipe symbol (on mac keyboard alt+7), signifying that whatever output usually comes out of command 1 should instead be directly sent to command 2 and output in the manner that command 2 inputs.\nIn a hypothetical situation you have a folder with hundreds of files and you know the file you are looking for is very large but you can’t remember its name.\nLet’s do a ls -l in the /etc directory and pipe the output to be sorted by file size.\nls -l /etc | sort -n -k 5\n-n means we are sorting numerically and not alphabetically, -k 5 says look at the fifth column of output, which happens to be the file size of ls command.\nAn example use would be to align a file and directly send the now aligned file to be converted into a different format that may be required for another part of the analysis.\nThe next step requires us to use a bioinformatics software called samtools. To be able to use this program we first have to load the module for it. We will cover this in the HPC lectures, so if you are a bit too fast for you own good, you will just have to type this command:\nmodule load bioinfo-tools samtools\nHere is an example where we convert the samfile to a bamfile:\nsamtools view -bS sample_1.sam | samtools sort -o outbam -\n-Sb literally means the input is Sam and to output in bam, and pipe it to immediately get sorted, not even creating the unsorted bamfile intermediary. Notice that samtools is made to take the single - at the end of the samtools sort command to make it read the piped data from samtools view instead of reading it from a file.\nThis should have generated a file called outbam.bam in your current folder. We will have some more examples of pipes in the next section."
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#word-count",
    "href": "topics/linux/lab_linux_permissions.html#word-count",
    "title": "File permissions",
    "section": "7 Word Count",
    "text": "7 Word Count\nwc for Word Count is a useful command for counting the number of occurrences of a word in a file. This is easiest explained with an example.\nLet’s return to our sample_1.sam.\nuser@login1 advlinux $ wc sample_1.sam\n233288 3666760 58105794 sample_1.sam\nThis can be interpreted like this:\nNumber of lines      = 233288 \nNumber of words      = 3666760\nNumber of characters = 58105794\nTo make this more meaningful, let’s use the pipes and grep command seen previously to see how many lines and how many times the string of nucleotides CATCATCAT exist in this file.\nuser@login1 advlinux $ grep \"CATCATCAT\" sample_1.sam | wc\n60  957 15074\nTo see only the line count you can add -l after wc. To count only characters you add -m, and to only count words you add -w.\n Output only the amount of lines that have chr1 in them from sample_1.sam.\n\n\nSolution\n\ngrep \"chr1\" sample_1.sam | wc -l\n\n Count the lines containing CATCATCAT in the outbam.bam file.\n\n\nSolution\n\nsamtools view outbam.bam | grep \"CATCATCAT\" | wc -l"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#bonus-exercise-1",
    "href": "topics/linux/lab_linux_permissions.html#bonus-exercise-1",
    "title": "File permissions",
    "section": "8 Bonus exercise 1",
    "text": "8 Bonus exercise 1\nThese are some harder assignments, so don’t worry about it if you didn’t have time to do it.\nLets look at grep and use some regular expressions http://www.cheatography.com/davechild/cheat-sheets/regular-expressions/\n From file sample_1.sam find all lines that start with @ and put them in a file called at.txt.\n\n\nSolution\n\ngrep \"^@\" sample_1.sam &gt; at.txt\n\n Find all the lines that end with at least 3 numbers from at.txt.  Sometimes, you have to escape {} with \\{\\})\n\n\nSolution\n\ngrep \"[0-9]\\\\{3\\\\}$\" sample_1.sam"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#bonus-exercise-2",
    "href": "topics/linux/lab_linux_permissions.html#bonus-exercise-2",
    "title": "File permissions",
    "section": "9 Bonus exercise 2",
    "text": "9 Bonus exercise 2\nsed is a handy tool to replace strings in files.\n You have realized that all the chromosomes have been misnamed as chr3 when they should be chr4. Use sed to replace chr3 with chr4 in sample_1.sam and output it to sample_2.sam.\n The solution to this replaces the first instance on each line of chr3. What if we have multiple instances? What if we had wanted to replace chr1? This would effect chr10-19 as well! There are many things to consider :).\n\n\nSolution\n\nsed \"s/chr1/chr2/\" sample_1.sam &gt; sample_2.sam"
  },
  {
    "objectID": "topics/linux/lab_linux_permissions.html#bonus-exercise-3",
    "href": "topics/linux/lab_linux_permissions.html#bonus-exercise-3",
    "title": "File permissions",
    "section": "10 Bonus exercise 3",
    "text": "10 Bonus exercise 3\nBash loops are great for moving or renaming multiple files as well as many many other uses.\nCreate a couple of files as seen below\ntouch one.bam two.sam three.bam four.sam five.bam six.sam\n All the files are actually in bam format. What a crazy mistake! Create a bash loop that changes all files ending in .sam to end with .bam instead.\n The bash loop syntax is this:\nfor _variable_ in _pattern_\ndo \n    _command with $variable_\ndone\n To rename file1 to file2 you write this:\nmv file1 file2\nwhich effectively is the same thing as\ncp file1 file2\nrm file1\n Ponder how this can be used to your advantage:\ni=filename\necho ${i/name}stuff\nfilestuff\n\n\nSolution\n\nfor f in *.sam\ndo\n    mv $f ${f/.sam}.bam\ndone"
  },
  {
    "objectID": "topics/hpc/intro/lab_intro.html",
    "href": "topics/hpc/intro/lab_intro.html",
    "title": "Intro to high-performance computing",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/hpc/intro/lab_intro.html#zoom-link-for-help-over-zoom",
    "href": "topics/hpc/intro/lab_intro.html#zoom-link-for-help-over-zoom",
    "title": "Intro to high-performance computing",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/hpc/intro/lab_intro.html#connect-to-pdc",
    "href": "topics/hpc/intro/lab_intro.html#connect-to-pdc",
    "title": "Intro to high-performance computing",
    "section": "0.2 Connect to PDC",
    "text": "0.2 Connect to PDC\nThe first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/hpc/intro/lab_intro.html#logon-to-a-node",
    "href": "topics/hpc/intro/lab_intro.html#logon-to-a-node",
    "title": "Intro to high-performance computing",
    "section": "0.3 Logon to a node",
    "text": "0.3 Logon to a node\nCheck which node you got when you booked resources this morning (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node I got (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested the session will shut down, and you will lose all unsaved data. Connect to this node from the login node.\nssh -Y nid001009\nIf the list is empty you can run the allocation command again and it should be in the list:\nsalloc -A edu25.uppmax --reservation=edu25-03-24 -t 04:00:00 -p shared -c 1"
  },
  {
    "objectID": "topics/hpc/intro/lab_intro.html#time-and-space",
    "href": "topics/hpc/intro/lab_intro.html#time-and-space",
    "title": "Intro to high-performance computing",
    "section": "7.1 Time and space",
    "text": "7.1 Time and space\nRemember the command projinfo (shows you how much of your allocated resources you have used) from the lecture? Try running it and see how you are doing. Run projinfo -h to see options you can give it.\n\n\n\n\n\n\nOptional\n\n\n\nThis optional material on HPC pipelines will teach you the basics in creating pipelines. Continue with this if you finish the current lab ahead of time. Navigate to the exercise HPC Pipelines lab."
  },
  {
    "objectID": "home_syllabus.html",
    "href": "home_syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "SyllabusLearning outcomesEntry requirements\n\n\nThe syllabus for this workshop are as follows.\n\nWorking on the unix/linux command line\n\nCommand line navigation and related commands: cd, mkdir, rm, rmdir\nCommonly used linux tools: cp, mv, tar, less, more, head, tail, nano, grep, top, man\nWildcards\nOwnership and permissions\nSymbolic links\nPiping commands\n\nWorking on remote computing cluster\n\nLogging on to HPC\nBooking resources\nJob templates, submission and queues\nModules\n\nCommonly used bioinformatic tools and pipelines\nWorking with integrated genome viewer\nVariant-calling workflow\n\nMapping reads to the reference genome\nVariant detection\nVCF file format\n\nRNA-Seq workflow\n\nRNA-Seq experimental design and considerations\nQC, mapping and gene expression counts\nDifferential gene expression analyses\n\nCurrent advances in NGS technologies\n\n\n\nAfter this workshop you should be able to:\n\nDescribe the basic principles of next generation sequencing.\nUse the Linux command line interface to manage simple file processing operations, and organise directory structures.\nConnect to and work on a remote compute cluster.\nApply programs in Linux for analysis of NGS data.\nSummarise the applications of current NGS technologies, including the weakness and strengths of the approaches and when it is appropriate to use which one of them.\nExplain common NGS file formats.\nInterpret quality control of NGS reads.\nExplain the steps involved in variant calling using whole genome sequencing data.\nIndependently perform a basic variant calling workflow on example data.\nExplain the steps involved in a differential gene expression workflow using RNA seq data.\nHands-on experience with handling of raw RNA sequencing data, QC and quantification of gene expression.\nConceptual understanding of differential gene expression analysis.\n\n\n\n\nThis is a national course open to PhD students, postdocs, group leaders and core facility staff.\nA background in genetics, cell biology, biomedicine, biochemistry, bioinformatics or comparable is desirable. To get the maximum benefit from the workshop we would like you to\nHave a research project where you are currently using next generation sequencing or are planning to use next generation sequencing. It is beneficial if you are directly performing analyses or if you have a support role and will be able to participate in a wide range of projects and transfer your knowledge to others.\nSelection criteria include correct entry requirements, motivation to attend the workshop as well as gender and geographical balance. Applicants affiliated to a Swedish institution are prioritized. International applicants are considered only if/when seats are available. Further prioritization: PhD scholars &gt; Post-Docs, PIs, Healthcare staff &gt; Master’s students).\nPlease note that NBIS training events do not provide any formal university credits. The training content is estimated to correspond to a certain number of credits, however the estimated credits are just guidelines. If formal credits are crucial, the student needs to confer with the home department before submitting a course application in order to establish whether the course is valid for formal credits or not."
  },
  {
    "objectID": "home_contents.html",
    "href": "home_contents.html",
    "title": "Contents",
    "section": "",
    "text": "Introduction to Linux\n\nIntroduction to Linux   \nLinux file permissions \n\n\n\nHigh-Performance Computing cluster\n\nIntroduction to HPC   \n\nHPC Pipelines \n\n\n\nFile types in Linux\n\nFile types in Bioinformatics   \n\n\n\nAdvanced Linux\n\nBetter terminal experience  \nAdvanced Linux   \n\n\n\nVariant-calling workflow\n\nVariant-calling    \n\n\n\nRNA-Seq workflow\n\nRNA-Seq workflow   \nSimon’s report \n\n\n\nNGS technologies\n\nNGS technologies and challenges  \nNGS Pipelines  \n\n\n\nOther\n\nQC of FastQ reads  \nData management  \n\n\n\nUseful resources\n\nConnecting to HPC PDC \nConnecting to HPC UPPMAX \nUploading & downloading files from HPC \nWorking on Tetralith: The backup cluster \nLinux cheatsheet \nBash cheat sheet 1 \nBash cheat sheet 2 \nBash cheat sheet 3 \nUppmax cheatsheet \nMac keyboard"
  },
  {
    "objectID": "home_precourse.html",
    "href": "home_precourse.html",
    "title": "Precourse",
    "section": "",
    "text": "You will have to have 3 different accounts to complete the labs in this course; one in SUPR to handle the two other accounts, and one each at the compute centers PDC in Stockholm and UPPMAX in Uppsala."
  },
  {
    "objectID": "home_precourse.html#supr",
    "href": "home_precourse.html#supr",
    "title": "Precourse",
    "section": "1 SUPR",
    "text": "1 SUPR\nA SUPR/NAISS account is needed to create the accounts for the computers we will be using during the course. SUPR is also the system that you will handle your own projects in if you want to use this kind of resources in your research after the course.\nIf you do not already have one, create an account at SUPR/NAISS. Then, Log in to SUPR/NAISS, preferably using the SWAMID.\n\nBefore proceeding with applying for project membership and user accounts, we have to accept the NAISS User Agreement. Do this by clicking the Personal Information link in the left sidebar menu. The scroll down a bit until you reach the section User Agreements. If you already have accepted it the State will be a green box with the text Accepted in it. If it is anything else, click it to start the accepting process.\n\n\n\n\n\n\nTip\n\n\n\nThis is where you might run into trouble if you don’t have a SWAMID connected account. You will not be able to accept the user agreement online without it, so you will have to send in your acceptance in paper form together with a copy of your passport. This process can take a week or more, so please make sure you can accept the user agreement in good time."
  },
  {
    "objectID": "home_precourse.html#uppmax",
    "href": "home_precourse.html#uppmax",
    "title": "Precourse",
    "section": "2 UPPMAX",
    "text": "2 UPPMAX\nRemote computing cluster UPPMAX will be use as a fallback cluster, if there should be any problems at PDC. After making sure you have an accepted user agreement, go to the SUPR/NAISS Projects page and request membership to the project ID: naissXXXX-XX-XXXX\n\nOnce you are accepted to a project, you should see that project listed under your active projects.\n\nFinally you need to request a login account to UPPMAX. This will be the account you use to log in to the actual computers, so it is not the same as your SUPR account. Login to SUPR and go to the Accounts page. Under the Possible Resource Account Requests heading click on Request Account on Rackham @ UPPMAX button and confirm it on the next page. If it is missing from this page, it could be because you already have a login account created (only 1 account per person allowed), or that you have not yet gotten your project memberships approved.\nChecking your request and approving your account requires some manual work, so you might have to wait for some time (up to a working day) before the next step. When the account is ready to be created, you will receive an email to your registered email address (shown in your SUPR contact information) with information on how to proceed. You will get a one-time URL that you use to get the password (within seven days) to login to the cluster with. The link is only valid for 1 visit, so write down the password you get. When that has been done, the account ready for use within 15 minutes and you can then login using your password. Once you have logged into the cluster you can change your password by typing passwd in the terminal and follow the instuctions.\n\n\n\n\n\n\nNote\n\n\n\nYou will get one username & password for the account on UPPMAX, and one username and password for the account on PDC. Please keep track of both, we will tell you when to use which account during the workshop."
  },
  {
    "objectID": "home_precourse.html#pdc",
    "href": "home_precourse.html#pdc",
    "title": "Precourse",
    "section": "3 PDC",
    "text": "3 PDC\nRemote computing cluster Dardel at PDC in Stockholm will be use for data analyses. A PDC account is needed to use these resources. Normally you would do this the same way as when you applied for the UPPMAX account above, but PDC handles uses for their courses through another system. If you do not already have a PDC account, please fill in the form at https://blackfish.pdc.kth.se/cgi-bin/accounts/request.py at least 2 weeks before the course start and use the same email that you applied to the course with. Fill in your personal information, as well as this information about which course it is:\n\n\n\n\n\n\n\nLecturer/ Project Leader:\nMartin Dahlö\n\n\nCourse / Project Title:\nedu25.uppmax\n\n\n\nWe have notified PDC who the students for our course is and they will create the account needed to login to their computers. Follow the instructions you get from them to complete your account creation. The morning session on the Monday of the course will cover how to connect to PDC, a bunch of steps that might not be trivial for novice users."
  },
  {
    "objectID": "home_precourse.html#install-tools",
    "href": "home_precourse.html#install-tools",
    "title": "Precourse",
    "section": "4 Install tools",
    "text": "4 Install tools\n\n4.1 Eduroam\n    Please make sure you have a working Eduroam wifi connection setup before the course, since we will only have wifi internet access in the computer room where we have the course. The instructions on how to get it working could differ university to university, so please refere to you home university’s instructions on how to do that. Try searching for “eduroam” and your home university, e.g. eduroam uppsala university, to find it.\n\n\n4.2 ThinLinc\n    ThinLinc allows graphical connection to UPPMAX. Download and install from https://www.cendio.com/thinlinc/download.\n\n\n4.3 XQuartz\n  Mac users will need to download and install XQuartz for X11 forwarding. ie; to forward remotely opened windows to local machine.\n\n\n4.4 MobaXterm*\n  If you are on a Windows system, and you want to open graphical applications from the terminal, we recommend MobaXterm. It is recommended that you INSTALL the program and not use the portable version. MobaXterm also has an integrated SFTP file browser.\n\n\n4.5 Filezilla*\n    When you need to transfer data between the remote cluster and your computer, you can use the tools SCP or SFTP through the terminal. Windows users can use the SFTP browser available with MobaXterm. If you prefer a GUI to upload and download files from the remote cluster, we recommend installing FileZilla.\n* Optional"
  },
  {
    "objectID": "home_precourse.html#connect-to-uppmax",
    "href": "home_precourse.html#connect-to-uppmax",
    "title": "Precourse",
    "section": "5 Connect to UPPMAX",
    "text": "5 Connect to UPPMAX\nSee Connecting to UPPMAX instructions listed on the Contents page.\n\n\n\n\n\n\nTip\n\n\n\nIf you want to get a primer on using the terminal, you can get started with the following Tutorial One at this link Unix tutorial for beginners. You can use https://scilifelab.github.io/courses/ngsintro/common/emu/ (or this mirror) to try the commands in the tutorial, so that you don’t mess up any real world system. If you have any questions regarding this tutorial contact: martin.dahlo [at] nbis.se.\n\n\n\n5.1 Create a user folder\n\n\n\n\n\n\nNote\n\n\n\nThis part is only to be done on UPPMAX, and NOT at PDC.\n\n\nOnce you have logged in to UPPMAX, run the following command.\nmkdir /proj/naissXXXX-XX-XXXX/nobackup/$USER\nThis creates a directory with your user name. You will work inside this directory for the workshop. If you cannot write to the folder, the most likely reason is that you have not requested access to the workshop project via SUPR. This is described in step 1 above.\n\n\n\n\n\n\nNote\n\n\n\nIt may take an hour or so from request approval, before you can actually write to the folder. We will check before the workshop that all students have logged in and done this, so do not forget!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Working on the Linux command line\nSequence data formats and QC\nDNA variant calling workflow\nRNA sequence analyses workflow\n\n\nUpdated: 24-01-2025 at 07:40:23."
  },
  {
    "objectID": "index.html#introduction-to-bioinformatics-using-ngs-data",
    "href": "index.html#introduction-to-bioinformatics-using-ngs-data",
    "title": "",
    "section": "",
    "text": "Working on the Linux command line\nSequence data formats and QC\nDNA variant calling workflow\nRNA sequence analyses workflow\n\n\nUpdated: 24-01-2025 at 07:40:23."
  },
  {
    "objectID": "topics/hpc/pipeline/lab_pipeline.html",
    "href": "topics/hpc/pipeline/lab_pipeline.html",
    "title": "HPC Pipelines",
    "section": "",
    "text": "The first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/hpc/pipeline/lab_pipeline.html#connect-to-pdc",
    "href": "topics/hpc/pipeline/lab_pipeline.html#connect-to-pdc",
    "title": "HPC Pipelines",
    "section": "",
    "text": "The first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/hpc/pipeline/lab_pipeline.html#logon-to-a-node",
    "href": "topics/hpc/pipeline/lab_pipeline.html#logon-to-a-node",
    "title": "HPC Pipelines",
    "section": "0.2 Logon to a node",
    "text": "0.2 Logon to a node\nCheck which node you got when you booked resources this morning (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node I got (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested the session will shut down, and you will lose all unsaved data. Connect to this node from the login node.\nssh -Y nid001009\nIf the list is empty you can run the allocation command again and it should be in the list:\nsalloc -A edu25.uppmax --reservation=edu25-03-24 -t 04:00:00 -p shared -c 1"
  },
  {
    "objectID": "topics/hpc/pipeline/lab_pipeline.html#load-the-module",
    "href": "topics/hpc/pipeline/lab_pipeline.html#load-the-module",
    "title": "HPC Pipelines",
    "section": "2.1 Load the module",
    "text": "2.1 Load the module\nIn this exercise, we’ll pretend that we are running analyses. This will give you a peek at what running programs in linux is like, and get you ready for the real stuff during the week!\nThe first thing you usually do is to load the modules for the programs you want to run. During this exercise we’ll only run my dummy scripts that don’t actually do any analysis, so they don’t have a module of their own. What we can do instead is to manually do what module loading usually does: to modify the $PATH variable.\nThe $PATH variable specifies directories where the computer should look for programs whenever you type a command. For instance, when you type\nless\nhow does the computer know which program to start? You gave it the name less, but that could refer to any file named less in the computer, yet it starts the correct one every time. The answer is that it looks in the directories stored in the $PATH variable and start the first program it finds that is named less. You can check exactly which program it starts by typing which less. It will give you something like the following:\nwhich less\n/usr/bin/less\ntelling you that the program less is located in the directory /usr/bin.\nTo see which directories that are checked by default, type\necho $PATH\nIt should give you something like this, a long list of directories (abbreviated below), separated by colon signs:\necho $PATH\n/pdc/software/modules/systemdefault/bin:/opt/cray/pe/mpich/8.1.28/of\ni/crayclang/17.0/bin: ...\n...\nTry loading a module, and then look at the $PATH variable again. You’ll see that there are a few extra directories there now, after the module has been loaded.\nmodule load bioinfo-tools samtools\necho $PATH\n/pdc/software/eb/software/samtools/1.20/bin:/pdc/software/eb/softwar\ne/htslib/1.20/bin:/pdc/software/eb/software/libdeflate/1.19/bin:/pdc\n/software/eb/software/xz/5.4.5/bin:/pdc/software/eb/software/bzip2/1\n.0.8/bin:/pdc/software/eb/software/ncurses/6.4/bin:/pdc/software/mod\nules/systemdefault/bin:/opt/cray/pe/mpich/8.1.28/ofi/crayclang/17.0/\nbin: ...\n...\nTo pretend that we are loading a module, instead of actually loading a module, we’ll manually do what the module system would have done. We will just add a the directory containing my dummy scripts to the $PATH variable, and it will be like we loaded the module for them. Now, when we type the name of one of my scripts, the computer will look in all the directories specified in the $PATH variable, which now includes the location where i keep my scripts. The computer will now find programs named as my scripts are and it will run them.\nexport PATH=$PATH:/sw/courses/ngsintro/hpc/pipeline/dummy_scripts\nThis will set the $PATH variable to whatever it is at the moment, and add a directory at the end of it. Note the lack of a dollar sign in front of the variable name directly after export. You don’t use dollar signs when assigning values to variables, and you always use dollar signs when getting values from variables.\n\n Important\nThe export command affects only the terminal you type it in. If you have 2 terminals open, only the terminal you typed it in will have a modified path. If you close that terminal and open a new one, it will not have the modified path.\n\nEnough with variables now. Let’s try the scripts out!"
  },
  {
    "objectID": "topics/other/lab_connect_pdc.html",
    "href": "topics/other/lab_connect_pdc.html",
    "title": "Connecting to PDC",
    "section": "",
    "text": "We will teach you two different ways to connect to PDC. From PDC’s point of view, it doesn’t matter which one you use, and you can change whenever you want to or even use both ways simultaneously. The first one is a text-based SSH connection, and the other one is a graphical remote desktop. The latter one is useful if you need to view images or documents in GUI programs without having to first download the image/document to your own computer first. Since it is using graphics, it will require you to have an internet connection that is good and stable.\nThe reason we will teach you two ways is that some parts of this course will require you to view plots and images, and adding an additional download step would just unneccesarily complicate things."
  },
  {
    "objectID": "topics/other/lab_connect_pdc.html#ssh-connection",
    "href": "topics/other/lab_connect_pdc.html#ssh-connection",
    "title": "Connecting to PDC",
    "section": "1 SSH connection",
    "text": "1 SSH connection\nLet’s look at the text-based SSH approach first. This type of connection work just fine even on slow internet connections since it only transmitts small amounts of text when you work with it. You will need a terminal and an SSH program to do this, which is fortunately included in most major operating systems:\n\n Linux: Use Terminal (Included by default)\n OSX: Use Terminal (Included by default)\n Windows: Use Powershell or Command prompt, both should be installed by default\n\nThe guide below is based on PDCs official instructions.\n\n1.1 Setting up SSH keys\n(If you already have done this once, please proceed to the next section instead.)\nBefore you can connect to PDC, you will have to create SSH keys and tell PDC which key they should allow you to login with, and which computer it should be allowed to be used from. You will only have to create and register SSH keys once. Start by opening your terminal program (see list above) and follow these steps:\nGenerate a key pair.\nssh-keygen\nand accept the default names it suggests by just pressing enter on the question about file in which to save the key. When it asks for a password, provide the password you will use to unlock the key each time you use it. Repeat the password when asked to in the next question. Once that is done it will say something like\nuser@computerName ~ $ ssh-keygen \nGenerating public/private ed25519 key pair.\nEnter file in which to save the key (/home/user/.ssh/id_ed25519):\nEnter passphrase (empty for no passphrase): \nEnter same passphrase again: \nYour identification has been saved in /home/user/.ssh/id_ed25519\nYour public key has been saved in /home/user/.ssh/id_ed25519.pub\nThe key fingerprint is:\nSHA256:6T19rVfxVndEpmHHY0vZc8/Xu03iDBYjS04//rOOvks user@computerName\nThe key's randomart image is:\n+--[ED25519 256]--+\n|              oo*|\n|             . @=|\n|              +oB|\n|         .+ o  oO|\n|        S+ + o  O|\n|       . .o.= .o=|\n|        . ooE=.+=|\n|           o.o+oo|\n|           .===+ |\n+----[SHA256]-----+\nThe cool thing with SSH keys (public-key cryptography) like this is that they consist of two parts. One is the public key, which you can publish on the internet for everyone to see. The other part is the private part, which should be kept hidden from everyone, like a password. The public key is given to, in this case, PDC and they will let anyone who has the private key in. Print your public key to the terminal and copy the text so that you can use it in the next step.\ncat ~/.ssh/id_ed25519.pub\nTo register our public key, we will go to PDC login portal. Following a link from that page, you will login by first logging in to your SUPR account and then validating your identity to PDC through SUPR.\n\nPress Add new key\n\nPaste the public key content you printed to the terminal with cat before into the SSH public key field in the form, and give a Key name of your choosing, then press Save.\n\nYou have now added your public key and told PDC that it should be allowed to login using this key when you are connecting form the IP (385.142.529.933 in the screenshot above) you have at the moment. Since you might get new IPs for your computer each time you connect to the internet, we will have to add a broader list of allowed IPs for this key. Press Add address on the entry for the key you just added.\n\nAdd *.liu.se to the field define one yourself. This will allow you to use this key as long as you are connecting from anywhere at Linköping university. If you want to use this key from your home university in the future you can add a similar pattern that matches your home university’s (or internet provider) hostname (e.g. *.uu.se, *.kth.se or *.telia.com).\n\n\n\n1.2 Connecting to PDC using SSH\n\n\n\n\n\n\nImportant\n\n\n\nIf you have problems connecting to PDC even if you have set up you SSH keys in the step above, please make sure your computer has not changed it IP address since you defined allowed addresses in the PDC login portal. Make sure that the IP range or hostname pattern you have there allows your current IP/hostname\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhere username is mentioned, change to your PDC user name.\n\n\nFire up the available SSH program and enter the following:\nssh username@dardel.pdc.kth.se\nEnter your password when prompted. As you type, nothing will show on the screen. No stars, no dots. It is supposed to be that way. Just type the password and press enter, it will be fine.\nNow your screen should look something like this:\nuser@computerName ~ $ ssh username@dardel.pdc.kth.se\nEnter passphrase for key '/home/user/.ssh/id_ed25519':\nLast login: Mon Oct 23 21:11:35 2024 from 385-142-529-933.bredband.obe.net\n\n     --== Welcome to Dardel! ==--\n\nusername@login1 ~ $\nNow you are connected to PDC and can start working."
  },
  {
    "objectID": "topics/other/lab_connect_pdc.html#remote-desktop",
    "href": "topics/other/lab_connect_pdc.html#remote-desktop",
    "title": "Connecting to PDC",
    "section": "2 Remote desktop",
    "text": "2 Remote desktop\nYou can work on PDC interactively through a graphical-user-interface (GUI) desktop environment using ThinLinc.\nThere is a ThinLinc server running at one of the login nodes which allows users to run a remote desktop. It can be reached using the ThinLink client, following these steps:\n\nDownload and install ThinLinc, https://www.cendio.com/thinlinc/download/\nStart ThinLinc.\n\nPress Options, select the Security tab and select authentication method Public key.\n\nEnter server dardel-vnc.pdc.kth.se, your PDC username, and the path to your private SSH key.\n\nPress Connect, and enter the password to unlock your SSH key when asked.\nWhen you are done with working with the remote desktop, press the power icon  in the top-right corner of the screen and select Log Out.\n\nFor more detailed instructions, please look here: https://www.pdc.kth.se/support/documents/login/interactive_hpc.html"
  },
  {
    "objectID": "topics/other/lab_download_files.html",
    "href": "topics/other/lab_download_files.html",
    "title": "HPC IO",
    "section": "",
    "text": "Start by creating a folder on your laptop where the files that you will download should end up. You need to have write permission in this folder. This folder will be referred to as your local workspace throughout these instructions.\nOpen a terminal window on your laptop and move into your local workspace."
  },
  {
    "objectID": "topics/other/lab_download_files.html#local-workspace",
    "href": "topics/other/lab_download_files.html#local-workspace",
    "title": "HPC IO",
    "section": "",
    "text": "Start by creating a folder on your laptop where the files that you will download should end up. You need to have write permission in this folder. This folder will be referred to as your local workspace throughout these instructions.\nOpen a terminal window on your laptop and move into your local workspace."
  },
  {
    "objectID": "topics/other/lab_download_files.html#download-a-file-from-hpc",
    "href": "topics/other/lab_download_files.html#download-a-file-from-hpc",
    "title": "HPC IO",
    "section": "2 Download a file from HPC",
    "text": "2 Download a file from HPC\nLets assume that you have a file “results.txt” in the following folder on HPC:\n~/ngsintro/somefolder/\n\n\n\n\n\n\nNote\n\n\n\nusername and somefolder should be replaced with your real username and a real folder name. Run this in a LOCAL terminal.\n\n\nTo download the file to your local workspace type:\nscp username@dardel.pdc.kth.se:~/ngsintro/somefolder/results.txt .\nNote that the last . means that the file will keep the original name."
  },
  {
    "objectID": "topics/other/lab_download_files.html#upload-a-file-to-hpc",
    "href": "topics/other/lab_download_files.html#upload-a-file-to-hpc",
    "title": "HPC IO",
    "section": "3 Upload a file to HPC",
    "text": "3 Upload a file to HPC\nNow lets imagine that you have developed a script on your laptop and want to upload it to HPC. The script is stored in your local workspace and is called “script.sh”. Type this in your local workspace to upload the file to HPC:\n\n\n\n\n\n\nNote\n\n\n\nusername and somefolder should be replaced with your real username and a real folder name. Run this on HPC.\n\n\nscp script.sh username@dardel.pdc.kth.se:~/ngsintro/somefolder/."
  },
  {
    "objectID": "topics/vc/lab_vc_answers.html",
    "href": "topics/vc/lab_vc_answers.html",
    "title": "Variant Calling Answers",
    "section": "",
    "text": "Unable to display PDF file. Download instead."
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html",
    "href": "topics/linux/lab_linux_filetypes.html",
    "title": "Filetypes",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#zoom-link-for-help-over-zoom",
    "href": "topics/linux/lab_linux_filetypes.html#zoom-link-for-help-over-zoom",
    "title": "Filetypes",
    "section": "",
    "text": "https://liu-se.zoom.us/j/65307893994"
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#connect-to-pdc",
    "href": "topics/linux/lab_linux_filetypes.html#connect-to-pdc",
    "title": "Filetypes",
    "section": "2 Connect to PDC",
    "text": "2 Connect to PDC\nThe first step of this lab is to open a ssh connection to PDC. Please refer to Connecting to PDC for instructions. Once connected to PDC, return here and continue reading the instructions below."
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#logon-to-a-node",
    "href": "topics/linux/lab_linux_filetypes.html#logon-to-a-node",
    "title": "Filetypes",
    "section": "3 Logon to a node",
    "text": "3 Logon to a node\nUsually you would do most of the work in this lab directly on one of the login nodes, but we have arranged for you to have one core each for better performance. This was covered briefly in the lecture notes.\nsalloc -A edu25.uppmax --reservation=edu25-03-25 -t 08:00:00 -p shared -n 1\ncheck which node you got (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u username\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node I got (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested (7 hours in this case) the session will shut down, and you will lose all unsaved data. Connect to this node from the login node.\nssh -Y nid001009"
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#copy-lab-files",
    "href": "topics/linux/lab_linux_filetypes.html#copy-lab-files",
    "title": "Filetypes",
    "section": "4 Copy lab files",
    "text": "4 Copy lab files\nNow you will need some files. To avoid all the course participants editing the same file all at once, undoing each other’s edits, each participant will get their own copy of the needed files. The files are located in the folder /sw/courses/ngsintro/linux/filetypes\nNext, copy the lab files from this folder. -r means recursively, which means all the files including sub-folders of the source folder. Without it, only files directly in the source folder would be copied, NOT sub-folders and files in sub-folders.\n Remember to use tab-complete to avoid typos and too much writing.\ncp -r &lt;source&gt; &lt;destination&gt;\ncp -r /sw/courses/ngsintro/linux/filetypes ~/ngsintro\nHave a look in ~/ngsintro/filetypes using the tree command.\n# go to the folder\ncd  ~/ngsintro/filetypes\n\n# load the tree module and run it\nmodule load tree\ntree\nThis will print a file tree, which gives you a nice overview of the folders where you are standing in. As you can see, you have a couple of files and a couple of empty folders. In the 0_ref folder you have a reference genome in fasta format and annotations for the genome in GTF format. In 0_seq you have a fastq file containing the reads we will align."
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#run-pipeline",
    "href": "topics/linux/lab_linux_filetypes.html#run-pipeline",
    "title": "Filetypes",
    "section": "5 Run pipeline",
    "text": "5 Run pipeline\nThe best way to see all the different file formats is to run a small pipeline and see which files we encounter along the way. The pipeline is roughly the same steps you’ll do in the variant-calling part of the course, so for now we’ll stick with the dummy pipeline which some of you might have encoutered in the extra material for the HPC exercise.\nThe programs in the dummy pipeline does not actually do any analysis but they work the same way as the real deal, although slightly simplified, to get you familiar with how to work with analysis programs. The data is from a sequencing of the adenovirus genome, which is tiny compared to the human genome (36kb vs 3gb).\nThe starting point of the pipeline are fresh reads from the sequencing machine in fastq format, and a reference genome in fasta format. The goal of the exercise is to look at our aligned reads in a genome viewer together with the annotations of the adenovirus genome.\nFirst, let’s go through the steps of the pipeline:\n\nBuild an index for the reference genome. This will speed up the alignment process. Not possible to do the analysis without it.\nAlign the reads.\nConvert the SAM file to a BAM file. We want to use the space efficiently.\nSort the BAM file. We have to sort it to be able to index it.\nIndex the BAM file. We have to index it to make it fast to access the data in the file.\nView the aligned data together with the annotations.\n\nThe first thing you usually do is to load the modules for the programs you want to run. During this exercise we’ll only run my dummy scripts that don’t actually do any analysis, so they don’t have a module of their own. What we can do instead is to manually do what module loading usually does: to modify the $PATH variable.\nThe $PATH variable specifies directories where the computer should look for programs. For instance, when you type nano, how does the computer know which program to start? You gave it the name nano, but that could refer to any file named nano in the computer, yet it starts the correct one every time. The answer is that it looks in the directories stored in the $PATH variable.\nTo see which directories that are available by default, type\necho $PATH\nIt should give you something like this, a list of directories, separated by colon signs:\nuser@rackham2 work $ echo $PATH\n/pdc/software/eb/software/tree/2.1.1/bin:/pdc/software/modules/sy\nstemdefault/bin:/opt/cray/pe/mpich/8.1.28/ofi/crayclang/17.0/bin:\n/opt/cray/pe/mpich/8.1.28/bin:/opt/cray/pe/craype/2.7.30/bin:/opt\n...\n...\nTry loading a module, and then look at the $PATH variable again. You’ll see that there are a few extra directories there now, after the module has been loaded.\nuser@rackham2 work $ module load bioinfo-tools samtools/1.20\nuser@rackham2 work $ echo $PATH\n/pdc/software/eb/software/samtools/1.20/bin:/pdc/software/eb/soft\nware/htslib/1.20/bin:/pdc/software/eb/software/libdeflate/1.19/bi\nn:/pdc/software/eb/software/xz/5.4.5/bin:/pdc/software/eb/softwar\ne/bzip2/1.0.8/bin:/pdc/software/eb/software/ncurses/6.4/bin:/pdc/\nsoftware/eb/software/tree/2.1.1/bin:/pdc/software/modules/systemd\n...\n...\nTo pretend that we are loading a module, we will just add a the directory containing my dummy scripts to the $PATH variable, and it will be like we loaded the module for them.\nexport PATH=$PATH:/sw/courses/ngsintro/hpc/pipeline/dummy_scripts/\nThis will set the $PATH variable to whatever it is at the moment, and add a directory at the end of it. Note the lack of a dollar sign in front of the variable name directly after export. You don’t use dollar sign when assigning values to variables, and you always use dollar signs when getting values from variables.\n\n Important\nThe export command affects only the terminal you type it in. If you have 2 terminals open, only the terminal you typed it in will have a modified path. If you close that terminal and open a new one, it will not have the modified path.\n\n\n5.1 Build index\n\nBuild an index for the reference genome.\nAlign the reads.\nConvert the SAM file to a BAM file.\nSort the BAM file.\nIndex the BAM file.\nView the aligned data together with the annotations.\n\nAll aligners will have to index the reference genome you are aligning your data against. This is only done once per reference genome, and then you reuse that index whenever you need it. All aligners have their own kind of index unfortunately, so you will have to build one index for each aligner you want to use. In this lab, we will use the dummy aligher called align_reads, and we will build a index using it’s indexing progam, called reference_indexer.\nFirst, have a look in the 0_ref folder\nll 0_ref\nYou should see 2 files: the fasta file, the gtf file. Have a look at each of them with less, just to see how they look inside. To do the actual indexing of the genome:\n Run reference_indexer.\nSyntax: reference_indexer -r &lt;name of the fasta file you want to index&gt;\nreference_indexer -r 0_ref/ad2.fa\nSince the genome is so small this should only take a second or so. The human genome will probably take a couple of hours. Look in the 0_ref folder again and see if anything has changed.\nll 0_ref\nThe new file you see is the index file created by reference_indexer. This index is in the same format as you would get from the real program samtools. Try viewing the index file with less and see how it looks. The samtools type of index contains one row per fasta record in the reference file. In this case, there is only one record for the adenovirus genome, and it’s called ad2 in the fasta file. The human reference genome typically have one record per chromosome, so a index of the human genome would then have 24 rows.\nThe numbers after the record name specifies how many bases the record has, how far into the file (in bytes) the record starts, the number of bases on each line in the record, and how many bytes each line takes up in the file. Using this information the program can quickly jump to the start location of each record, without having to read the file from the first row every time.\nOther aligners might use more complex indexing of the file to speed up the alignment process even further, e.g. creating an index over where it can find all possible “words” that you can form with 5 or so bases, making it easier to find possible matching sites for reads. If the read starts with ATGTT you can quickly look in the index and see all places in the geonome that contains this word and start looking if the rest of the read matches the area around the word.\nThis greatly decreases the number of places you have to look when looking for a match. These types of indexes usually take a long time to create (5+ hours maybe), but since you only have to do it once per reference genome it’s easily worth it, seeing how the alignment process probably would take 100s of hours without the index, instead of 6-12 hours.\nWe are now ready to align our reads.\n\n\n5.2 Align reads\n\nBuild an index for the reference genome.\nAlign the reads.\nConvert the SAM file to a BAM file.\nSort the BAM file.\nIndex the BAM file.\nView the aligned data together with the annotations.\n\n Align reads using align_reads, naming the output file ad2.sam, placed in the 1_alignment folder.\nSyntax: align_reads -r &lt;reference genome&gt; -i &lt;fastq file with reads&gt; -o &lt;name of the output file&gt;\nalign_reads -r 0_ref/ad2.fa -i 0_seq/ad2.fq -o 1_alignment/ad2.sam\nThis will create a SAM file in 1_alignment called ad2.sam. Have a look at it with less. If you think the file looks messy, add a -S after less to make it stop wrapping long lines, less -S 1_alignment/ad2.sam and scroll sideways using the arrow keys. As you can see there is one row per aligned read in this file. Each row contains information about the read, like the name of the read, where in the reference genome it aligned, and also a copy of the reads sequence and quality score, among other things.\n\n\n5.3 SAM to BAM\n\nBuild an index for the reference genome.\nAlign the reads.\nConvert the SAM file to a BAM file.\nSort the BAM file.\nIndex the BAM file.\nView the aligned data together with the annotations.\n\n The next step is to convert the SAM file to a BAM file. This is more or less just compressing the file, like creating a zip file. To do that we will use the dummy program sambam_tools, telling it we want to convert a file to BAM (-f bam), which file we want to convert (-i), where it should save the resulting BAM file (-o). Save the BAM file in the 2_bam folder and name it ad2.bam.\nSyntax: sambam_tool -f bam -i &lt;sam file&gt; -o &lt;bam&gt;\nsambam_tool -f bam -i 1_alignment/ad2.sam -o 2_bam/ad2.bam\nHave a look in the 2_bam folder.\nll 2_bam\nThe created BAM file is an exact copy of the SAM file, but stored in a much more efficient format. Aligners usually have an option to output BAM format directly, saving you the trouble to convert it yourself, but not all tools can do this (they really should though). Have a look at the difference in file size, though in this example it’s quite an extreme difference (2.9 MB vs 0.3 MB). The quality score of all reads is the same (BBBBBBBBB..), and files with less differences are easier to compress. Usually the BAM file is about 25% of the size of the SAM file.\nSince the BAM format is a binary format we can’t look at it with less. We would have to use a tool, like samtools which you will probably see later in the week, to first convert the file back to a SAM file before we can read it. In that case we can just look at the SAM file before converting it since they will be the same.\n\n\n5.4 Sort & index BAM\n\nBuild an index for the reference genome.\nAlign the reads.\nConvert the SAM file to a BAM file.\nSort the BAM file.\nIndex the BAM file.\nView the aligned data together with the annotations.\n\nA BAM file is taking up much less space than the SAM file, but we can still improve performance. An indexed BAM file is infinitely faster for programs to work with, but before we can index it, we have to sort it since it’s not possible to index an unsorted file in any meaningful way.\n To sort the BAM file we’ll use the sambam_tool again, but specifying a different function, -f sort instead. Tell it to store the sorted BAM file in the 3_sorted folder and name the file ad2.sorted.bam\nSyntax: sambam_tool -f sort -i &lt;unsorted bam file&gt; -o &lt;sorted bam file&gt;\nsambam_tool -f sort -i 2_bam/ad2.bam -o 3_sorted/ad2.sorted.bam\nThis will sort the ad2.bam file and create a new BAM file which is sorted, called ad2.sorted.bam\n Now when we have a sorted BAM file, we can index it. Use the command\nSyntax: sambam_tool -f index -i &lt;sorted bam file&gt;\nsambam_tool -f index -i 3_sorted/ad2.sorted.bam\nThis will create an index named ad2.sorted.bam.bai in the same folder as the ad2.sorted.bam file is located. It’s nicer to have the .bam and .bai named to the same “prefix”, so rename the .bai file to not have the .bam in its name.\nmv 3_sorted/ad2.sorted.bam.bai 3_sorted/ad2.sorted.bai\n\n\n5.5 View in a genome viewer\n\nBuild an index for the reference genome.\nAlign the reads.\nConvert the SAM file to a BAM file.\nSort the BAM file.\nIndex the BAM file.\nView the aligned data together with the annotations.\n\nNow that we have to data aligned and prepared for easy access, we will view it in a genome viewer together with the annotations for the genome. Have a look at the annotations file with less.\nless -S 0_ref/ad2.gtf\nThe -S will tell less to not wrap the lines, and instead show one line per line. If the line is longer than the window, you can user the left and right arrow to scroll to the left and right. Many tabular files are much more readable when using the -S option. Try viewing the file without it and see the difference.\nTo view the file, we will use the program IGV (Integrated Genome Viewer). Before we can do this, we have to load the module for IGV.\nNOTE: Many Mac users are getting only black background when running IGV using XQuartz. There is a command you can run on your computer to fix this.\n# open a terminal on your mac and run this command:\ndefaults write org.xquartz.X11 enable_render_extension 0\n\nNow you have to quit and reinstall [XQuartz](https://www.xquartz.org/), then reconnect to the cluster and it should now look the way it should.\n If you are using a Mac you might have to install the program XQuartz, if you have not already installed that program. By using -Y in your ssh command you enable graphical transfer over ssh, but you will also have to have a program able to receive the graphics in order to display it.\nmodule load bioinfo-tools IGV\nStart it by typing the following command (now we’ll find out if you used -Y in all your ssh connections!):\nigv.sh\n\n\n\n\n\n\nTip\n\n\n\nIf you notice that IGV over Xforwarding is excruciatingly slow, you can try to use the ThinLinc client instead, please read the instrucutions on how to connect here. This will get you a remote desktop on one of the login nodes, and you can open a terminal and run IGV there instead. Once IGV is started, either using Xforwarding or the remote desktop, we are ready to go.\n\n\nThere are 3 files we have to load in IGV.\nThe first is the reference genome. Press the menu button located at “Genomes - Load Genome from File…” and find your reference genome in 0_ref/ad2.fa. If you are having trouble finding your files, note that IGV always starts in your home directory. Use the dropdown menu at the top to navigate to ~/ngsintro/filetypes\nThe second file you have to load is the reads. Press the menu button “File - Load from File…” (not the same menu as in the step before) and select your 3_sorted/ad2.sorted.bam.\nThe last file you have to load is the annotation data. Press “File - Load from File…” again and select you annotation file in 0_ref/ad2.gtf.\nThis will show you the reference genome, how all the reads are aligned to it, and all the annotation data. Try zooming in on an area and have a look at the reads and annotations. The figures you see in the picture are all derived from the data in the files you have given it.\nAt the top of the window you have the overview of the current chromosome you are looking at, which tells you the scale you are zoomed at for the moment. When you zoom in you will see a red rectangle apper which shows you which portion of the chromosome you are looking at. Just below the scale you’ll see the coverage graph, which tells you how many reads cover each position along the reference genome. The colored bands you see here and there are SNPs, i.e. positions where the reads of your sample does not match the reference genome.\nAll the reads, the larger area in the middle of the window, are drawn from the data in the BAM file using the chromosome name, the starting position and the ending position of each read. When you zoom in more you will be able to see individual reads and how they are aligned. The annotation in GTF format are all plotted using the data in the GTF file, visible just under all the reads, are shown as blue rectangles.\nThe reference genome, a fasta file containing the DNA sequence of the reference genome, is visible at the bottom of the window if you zoom to the smallest level so you can see the bases of the genome."
  },
  {
    "objectID": "topics/linux/lab_linux_filetypes.html#create-a-cram-file",
    "href": "topics/linux/lab_linux_filetypes.html#create-a-cram-file",
    "title": "Filetypes",
    "section": "6 Create a CRAM file",
    "text": "6 Create a CRAM file\nThe CRAM format is even more efficient than the BAM format. To create a CRAM file we’ll have to use samtools, so we will load the module for it.\nmodule load bioinfo-tools samtools\n Tell samtools that you want CRAM output (-C) and specify which reference genome it should use to do the CRAM conversion (-T)\nSyntax: samtools view -C -T &lt;reference genome&gt; -o &lt;name of cram file&gt; &lt;bam file to convert&gt;\nsamtools view -C -T 0_ref/ad2.fa -o 4_cram/ad2.cram 3_sorted/ad2.sorted.bam\nCompare the sizes of the convered BAM file and the newly created CRAM file:\nll -h 3_sorted/ad2.sorted.bam 4_cram/ad2.cram\nThis will list both the files, and print the file size in a human readable format (-h). The CRAM file is roughly 1/3 of the size of the BAM file. This is probably because all the reads in the simulated data has the same quality value (BBBBBBBBBB). Fewer types of quality values are easier to compress, hence this amazing compression ratio. Real data will have much more diverse quality scores, and the CRAM file would be pethaps 70-80% of the original BAM file.\n\n\n\n\n\n\nOptional\n\n\n\nIf you have been fast to finish this lab and you still have time left (or just can’t get enough of linux stuff), please have a look at the advanced linux tutorial where you can learn the basics in bash programming using variables, loops and control statements. This is the material we will cover in the afternoon session, but you can get a head start if you want to."
  },
  {
    "objectID": "topics/other/lab_tetralith.html",
    "href": "topics/other/lab_tetralith.html",
    "title": "Working on Tetralith cluster",
    "section": "",
    "text": "The ThinLinc client can be downloaded for free from http://www.cendio.com/downloads/clients/. It is available for Windows, Mac OS X, Linux and Solaris.\nTo use ThinLinc to connect to Tetralith:\n\nIf you haven’t already done so, download the ThinLinc client matching your local computer (i.e Windows, Linux, MacOS X or Solaris) and install it.\nStart the client.\nChange the “Server” setting to “tetralith.nsc.liu.se”.\nChange the “Name” setting to your Tetralith username (e.g x_abcde).\nYou do not need to change any other settings.\nEnter your cluster Tetralith password in the “Password” box.\nPress the “Connect” button.\nEnter the 6-digit code generated by the two-factor authentication app on your phone.\nIf you connect for the first time, you will see the “The server’s host key is not cached …” dialog. Verify that the fingerprint shown on your screen matches the one listed below! If it does not match, press Abort and then contact NSC Support!\n\nTetralith SSH server host key fingerprint: 20:19:f4:6b:38:d6:e7:ac:e6:7c:8e:38:0a:7f:34:dc\nAfter a few seconds, a window with a simple desktop session in it will appear. From the Applications menu, start a Terminal Window."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#connect-to-nsc-via-thinlinc",
    "href": "topics/other/lab_tetralith.html#connect-to-nsc-via-thinlinc",
    "title": "Working on Tetralith cluster",
    "section": "",
    "text": "The ThinLinc client can be downloaded for free from http://www.cendio.com/downloads/clients/. It is available for Windows, Mac OS X, Linux and Solaris.\nTo use ThinLinc to connect to Tetralith:\n\nIf you haven’t already done so, download the ThinLinc client matching your local computer (i.e Windows, Linux, MacOS X or Solaris) and install it.\nStart the client.\nChange the “Server” setting to “tetralith.nsc.liu.se”.\nChange the “Name” setting to your Tetralith username (e.g x_abcde).\nYou do not need to change any other settings.\nEnter your cluster Tetralith password in the “Password” box.\nPress the “Connect” button.\nEnter the 6-digit code generated by the two-factor authentication app on your phone.\nIf you connect for the first time, you will see the “The server’s host key is not cached …” dialog. Verify that the fingerprint shown on your screen matches the one listed below! If it does not match, press Abort and then contact NSC Support!\n\nTetralith SSH server host key fingerprint: 20:19:f4:6b:38:d6:e7:ac:e6:7c:8e:38:0a:7f:34:dc\nAfter a few seconds, a window with a simple desktop session in it will appear. From the Applications menu, start a Terminal Window."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#interactive-session",
    "href": "topics/other/lab_tetralith.html#interactive-session",
    "title": "Working on Tetralith cluster",
    "section": "2 Interactive session",
    "text": "2 Interactive session\nStart an interactive session with three compute cores for todays lab:\ninteractive -A edu25.uppmax -t 04:00:00 -n 3\nPlease adjust the requested time depending on what lab you will be working on. After a minute or so you should have gotten your interactive job.\n\n\n\n\n\n\nTip\n\n\n\nNote that your terminal prompt changed from &lt;nsc_username&gt;@tetralith$ to something like &lt;nsc_username&gt;@n424$ (or another node name), which means that you are now running on one of the compute nodes."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#uppmax-singularity-container",
    "href": "topics/other/lab_tetralith.html#uppmax-singularity-container",
    "title": "Working on Tetralith cluster",
    "section": "3 UPPMAX singularity container",
    "text": "3 UPPMAX singularity container\nWe will use a singularity container (a virtual computer) that mimics the UPPMAX computing environment. Once you have started the singularity container your environment will look exactly as on UPPMAX, and the software used in this workshop will be available through the module system inside the container. Use this command to start the singularity container:\nsingularity shell -B ~/ngsintro -B ~/ngsintro:~/ngsintro ~/ngsintro/ngsintro.sif\n\n\n\n\n\n\nTip\n\n\n\nYour terminal prompt changed to something like &lt;nsc_username&gt;@offline-uppmax$. This means that you have moved into a “virtual computer” that mimics the UPPMAX environment.\n\n\nIn the singularity container type this to make the module system behave properly:\nsource /uppmax_init\nEverything from this point and onwards should be identical to running the exercise on UPPMAX. To close the singularity container later on just type exitin the terminal, but don’t do that now.\n\n\n\n\n\n\nNote\n\n\n\nNote: Since this is not actually running on uppmax, none of the queue system commands (squeue, sbatch, jobinfo etc) will work."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#cluster-workspace",
    "href": "topics/other/lab_tetralith.html#cluster-workspace",
    "title": "Working on Tetralith cluster",
    "section": "4 Cluster workspace",
    "text": "4 Cluster workspace\nWhile running the UPPMAX singularity container, create a workspace for this exercise. This will be the “cluster workspace” in which you should perform the analyses, as if you would have worked on UPPMAX.\nThe name of the workspace depends on what lab you are working on. For example, if you are working on the introduction to Linux lab then call the workspace “linux_tutorial”. Once the workspace is created you should go into it.\nmkdir ~/ngsintro/linux_tutorial\ncd ~/ngsintro/linux_tutorial\nWhen you have the UPPMAX singularity container up and running you can follow regular lab instructions, except that you should not connect to UPPMAX or book a node, and you should work in the cluster workspace defined here. Also, IGV is started with a special command,see below."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#igv",
    "href": "topics/other/lab_tetralith.html#igv",
    "title": "Working on Tetralith cluster",
    "section": "5 IGV",
    "text": "5 IGV\nTo start IGV at NSC type this in the command:\n~/ngsintro/igv/igv.sh\nYou can use the same command both when running the UPPMAX singularity container and from a normal terminal window at NSC."
  },
  {
    "objectID": "topics/other/lab_tetralith.html#exit-the-uppmax-singularity-container",
    "href": "topics/other/lab_tetralith.html#exit-the-uppmax-singularity-container",
    "title": "Working on Tetralith cluster",
    "section": "6 Exit the UPPMAX singularity container",
    "text": "6 Exit the UPPMAX singularity container\nWhen you are done with the lab just type exit to close the singularity container:\noffline-uppmax$ exit\nAll files and folders that you create in ~/ngsintro while running the singularity container can be reached also from outside of the container."
  },
  {
    "objectID": "topics/other/lab_open_session.html",
    "href": "topics/other/lab_open_session.html",
    "title": "Open session",
    "section": "",
    "text": "0.1 Friday at 13:15\n\n\n\n\n\n\nRoom\nTopic\nStaff\n\n\n\n\nRoom 1\nWhole genome sequencing, variant calling\nMalin Larsson, Diana Ekman\n\n\nRoom 2\nRNA-Seq, scRNA-Seq\nJulie Lorent, Juliana Assis, Prasoon Agarwal, Roy Francis\n\n\nRoom 3\nBash, Uppmax, Computing\nMartin Dahlö\n\n\nRoom 4\nEpigenetics, Chip-Seq, RNA-Seq, scRNASeq, small RNA\nAgata Smialowska, Vincent van Hoef\n\n\nRoom 5\nNGI, Assembly, Long read sequencing\nAdam Ameur, Estelle Proux\n\n\nRoom 6\nMetagenomics, Prokaryots, RNA-Seq\nDag Ahrén, Lokesh Manoharan, John Sundh"
  },
  {
    "objectID": "topics/other/lab_qc.html",
    "href": "topics/other/lab_qc.html",
    "title": "Read quality",
    "section": "",
    "text": "FastQC performes a series of quality control analyses, called modules. The output is a HTML report with one section for each module, and a summary evaluation of the results in the top. Entirely normal results are marked with a green tick, slightly abnormal results raise warnings (orange exclamation mark), and very unusual results raise failures (red cross).\nIt is important to stress that although the analyses appear to give a pass/fail result, these evaluations must be taken in the context of what you expect from your library. A ‘normal’ sample as far as FastQC is concerned is random and diverse. Some experiments may be expected to produce libraries which are biased in particular ways. You should treat the summary evaluations therefore as pointers to where you should concentrate your attention and understand why your library may not look random and diverse."
  },
  {
    "objectID": "topics/other/lab_qc.html#fastqc",
    "href": "topics/other/lab_qc.html#fastqc",
    "title": "Read quality",
    "section": "",
    "text": "FastQC performes a series of quality control analyses, called modules. The output is a HTML report with one section for each module, and a summary evaluation of the results in the top. Entirely normal results are marked with a green tick, slightly abnormal results raise warnings (orange exclamation mark), and very unusual results raise failures (red cross).\nIt is important to stress that although the analyses appear to give a pass/fail result, these evaluations must be taken in the context of what you expect from your library. A ‘normal’ sample as far as FastQC is concerned is random and diverse. Some experiments may be expected to produce libraries which are biased in particular ways. You should treat the summary evaluations therefore as pointers to where you should concentrate your attention and understand why your library may not look random and diverse."
  },
  {
    "objectID": "topics/other/lab_qc.html#data",
    "href": "topics/other/lab_qc.html#data",
    "title": "Read quality",
    "section": "2 Data",
    "text": "2 Data\nWe will run FastQC on three low-coverage whole genome sequencing (WGS) samples from the public 1000 Genomes project. To speed up the analysis we will only use data from a small genomic region. These are the exact same samples as will be used in the variant-calling workflow lab on Wednesday.\n\n\n\nSample\nDescription\n\n\n\n\nHG00097\nLow coverage WGS\n\n\nHG00100\nLow coverage WGS\n\n\nHG00101\nLow coverage WGS"
  },
  {
    "objectID": "topics/other/lab_qc.html#run-fastqc",
    "href": "topics/other/lab_qc.html#run-fastqc",
    "title": "Read quality",
    "section": "3 Run FastQC",
    "text": "3 Run FastQC\n\n3.1 Connect to PDC\nDuring this lab it is best to connect to Dardel via a remote desktop (ThinLinc). Please refer to Connecting to PDC, section 2 Remote desktop, for instructions.\n\n\n3.2 Log on to a node\nCheck which node you got when you booked resources this morning (replace username with your username)\nsqueue -u username\nshould look something like this\nuser@login1 ~ $ squeue -u user\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           5583899    shared interact    user  R       2:22      1 nid001009\nuser@login1 ~ $\nwhere nid001009 is the name of the node (yours will probably be different). Note the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested the session will shut down, and you will lose all unsaved data.\nIf the list is empty you can run the allocation command again and it should be in the list:\nsalloc -A edu25.uppmax --reservation=edu25-03-25 -t 04:00:00 -p shared -c 1\nConnect to this node from the login node.\nssh -Y nid001009\n\n\n3.3 Create a workspace\nYou should work in a subfolder in your home folder on Dardel, just like you have done during the previous labs. Start by going to your course folder using this command:\n\n\ncd ~/ngsintro\n\n\nCreate a folder for this exercise and move into it:\nmkdir qc\ncd qc\n\n\n3.4 Symbolic links to data\nThe raw data files are located in\n\n\n/sw/courses/ngsintro/vc/data/fastq\n\n\nInstead of copying the files to your workspace you should create symbolic links (soft-links) to them. Soft-linking files and folders allows you to work with them as if they were in your current directory, but without multiplying them. Create symbolic links to the fastq files in your workspace:\n\n\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_2.fq\n\n\n\n\n3.5 Accessing FastQC\nFastQC is installed in the module system on PDC. Modules must be loaded every time you login to Dardel, or when you connect to a new compute node.\nFirst load the bioinfo-tools module:\nmodule load bioinfo-tools\nThis makes it possible to load FastQC:\nmodule load fastqc/0.12.1\n\n\n3.6 Run FastQC\nRun FastQC on all fastq files:\nfastqc -q *.fq\nThe output is .html documents that shows quality scores along the reads, and other information. Please check what new files were generated with the command:\nls -lrt"
  },
  {
    "objectID": "topics/other/lab_qc.html#check-the-results",
    "href": "topics/other/lab_qc.html#check-the-results",
    "title": "Read quality",
    "section": "4 Check the results",
    "text": "4 Check the results\nThe output from FastQC is a HTML report that should be opened in a web browser. When you have connected to PDC via ThinLinc you can open it on Dardel with this command:\nfirefox --no-remote filename.html &\nWe have made the output files that you just created available through the links below, so that you can look at them via your local web-browser:\n\n\n\nSample\nRead 1\nRead 2\n\n\n\n\nHG00097\nHG00097_1.fq\nHG00097_2.fq\n\n\nHG00100\nHG00100_1.fq\nHG00100_2.fq\n\n\nHG00101\nHG00101_1.fq\nHG00101_2.fq\n\n\n\n\n4.1 Per Base Sequence Quality\nThis module shows the distribution of the quality scores at each position in the reads. The quality scores are represented by a Box and Whisker plot with the following elements:\n\nThe central red line is the median value.\nThe yellow box represents the inter-quartile range (25-75%).\nThe upper and lower whiskers represent the 10% and 90% points\nThe blue line represents the mean quality\n\nThe background of the graph divides the y axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red).\n\n4.1.1 Questions\n\nWhich positions in the reads have a median phred-score above 28 (very good quality calls) in each sample?\n\nDo any of the samples have warnings or failures in the Per Base Sequence Quality module?\nWhy? Please look in the documentation of this module.\n\n\n\n\n4.2 Sequence Length Distribution\nThis module shows the length distribution of the reads in the file.\n\n4.2.1 Questions\n\nHow long are the reads?\nDo any of the samples have warnings or failures in the Sequence Length Distribution module?\nWhy? Please look in the documentation of this module"
  },
  {
    "objectID": "topics/other/lab_qc.html#answers",
    "href": "topics/other/lab_qc.html#answers",
    "title": "Read quality",
    "section": "5 Answers",
    "text": "5 Answers\nWhen you have finished the exercise, please have a look at this document with answers to questions, and compare them with your answers."
  },
  {
    "objectID": "topics/other/lab_qc.html#documentation",
    "href": "topics/other/lab_qc.html#documentation",
    "title": "Read quality",
    "section": "6 Documentation",
    "text": "6 Documentation\n\nFastQC\nIf you want to learn more details about FastQC please have a look at this video by the Babraham Bioinformatics Institute."
  },
  {
    "objectID": "home_schedule.html",
    "href": "home_schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "Note\n\n\n\nBreaks are planned for approximately 10:00 and 14:30 every day.\n\n\n\n\n\n\n\n\nTime\nTopic\nInstructor\n\n\n \n24-Mar-2025MonOnline\n\n09:00 - 09:30\nWelcome\nMartin Dahlö\n\n\n09:30 - 10:30\nIntro to Linux\nMartin Dahlö\n\n\n10:30 - 12:00\nIntro to Linux\nMartin Dahlö\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nIntro to HPC cluster\nMartin Dahlö\n\n\n14:00 - 17:00\nIntro to HPC cluster\nMartin Dahlö\n \n25-Mar-2025TueOnline\n\n09:00 - 09:30\nFile types in Bioinformatics\nMartin Dahlö\n\n\n09:30 - 11:00\nFile types in Bioinformatics\nMartin Dahlö\n\n\n11:00 - 12:00\nBetter terminal experience\nMartin Dahlö\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nQuality control\nMalin Larsson\n\n\n14:00 - 15:00\nAdvanced Bash\nMartin Dahlö\n\n\n15:00 - 17:00\nAdvanced Bash\nMartin Dahlö\n \n26-Mar-2025WedOnline\n\n09:00 - 12:00\nNGS tech & challenges\nAdam Ameur & Johanna Lagensjö\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nVariant-calling workflow\nMalin Larsson\n\n\n14:00 - 17:00\nVariant-calling workflow\nMalin Larsson\n \n27-Mar-2025ThuOnline\n\n09:00 - 09:30\nGATK best practices\nMalin Larsson\n\n\n09:30 - 12:00\nVariant-calling workflow\nMalin Larsson\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nRNA-Seq workflow\nRoy Francis\n\n\n14:00 - 17:00\nRNA-Seq workflow\nRoy Francis\n \n28-Mar-2025FriOnline\n\n09:00 - 12:00\nRNA-Seq workflow\nRoy Francis\n\n\n12:00 - 13:00\nLunch\n\n\n\n13:00 - 14:00\nData management practices\nElin Kronander\n\n\n14:00 - 14:15\nNBIS\nMalin Larsson\n\n\n\n\n\n\n\n   Date    Venue    Slides    Lab    Video"
  },
  {
    "objectID": "topics/vc/lab_vc.html",
    "href": "topics/vc/lab_vc.html",
    "title": "Variant Calling",
    "section": "",
    "text": "Whole genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. This workshop will take you through the process of calling germline short variants (SNVs and INDELs) in WGS data from three human samples.\n\nThe first part of the workshop will guide you through a basic variant calling workflow in one sample. The goals are that you should get familiar with the bam and vcf file formats, and be able to interpret vcf files in Integrative Genomics Viewer (IGV).\nIf you have time, the next part of the workshop will show you how to perform joint variant calling in three samples. The goals here are that you should be able to interpret multi-sample vcf files and explain the differences between the g.vcf and vcf file formats.\nIf you have time, the last part of the workshop will take you through the GATK best practices for germline short variant detection in three samples. The goal here is that you should learn how to use GATK’s documentation so that you can analyze your own samples in the future.\n\n\n\n\n\n\n\nGeneral guide\n\n\n\n\nYou will work on the computing cluster Dardel at PCD\nIf you change the node you are working on you will need to reload the tool modules.\nPlease type commands in the terminal instead of copying and pasting them which often result in formatting errors.\nUse tab completion.\nIn paths, please replace username with your actual PDC username.\nIn commands, please replace parameter with the correct parameter, for example your input file name, output file name, directory name, etc.\nA line starting with # is a comment\nRunning a command without parameters will often return a help message on how to run the command.\nAfter a command is completed, please check that the desired output file was generated and that it has a reasonable size (use ls -l).\nGoogle errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere."
  },
  {
    "objectID": "topics/vc/lab_vc.html#samples",
    "href": "topics/vc/lab_vc.html#samples",
    "title": "Variant Calling",
    "section": "Samples",
    "text": "Samples\nThe 1000 Genomes Project ran between 2008 and 2015, creating the largest public catalogue of human variation and genotype data. In this workshop we will use low coverage whole genome sequence data from three individuals, generated in the first phase of the 1000 Genomes Project.\n\n\n\nSample\nPopulation\nSequencing technology\n\n\n\n\nHG00097\nBritish in England and Scotland\nLow coverage WGS\n\n\nHG00100\nBritish in England and Scotland\nLow coverage WGS\n\n\nHG00101\nBritish in England and Scotland\nLow coverage WGS"
  },
  {
    "objectID": "topics/vc/lab_vc.html#Genomicregion",
    "href": "topics/vc/lab_vc.html#Genomicregion",
    "title": "Variant Calling",
    "section": "Genomic region",
    "text": "Genomic region\nThe LCT gene on chromosome 2 encodes the enzyme lactase, which is responsible for the metabolism of lactose in mammals. Most mammals can not digest lactose as adults, but some humans can. Genetic variants upstream of the LCT gene causes lactase persistence, which means that lactase is expressed also in adulthood and the carrier can continue to digest lactose. The variant rs4988235, located at position chr2:136608646 in the GRCh37 reference genome, has been shown to lead to lactose persistence. The alternative allele (A on the forward strand and T on the reverse strand) creates a new transcription factor binding site that enables continued expression of the gene after weaning.\nIn this workshop we will detect genetic variants in the region chr2:136545000-136617000 in the three samples listed above, and check if they carry the allele for lactase persistence.\nFor those interested in the details of the genetic bases for lactose tolerance, please read the first three pages of Lactose intolerance: diagnosis, genetic, and clinical factors by Mattar et al. The variant rs4988235 is here referred to as LCT-13910C&gt;T."
  },
  {
    "objectID": "topics/vc/lab_vc.html#Data",
    "href": "topics/vc/lab_vc.html#Data",
    "title": "Variant Calling",
    "section": "Data folder",
    "text": "Data folder\nAll input data for this exercise is located in this folder on Dardel:\n\n\n/sw/courses/ngsintro/vc/data\n\n\nThe fastq files are located in this folder:\n\n\n/sw/courses/ngsintro/vc/data/fastq\n\n\nReference files, such as the reference genome in fasta format, are located in this folder:\n\n\n/sw/courses/ngsintro/vc/data/ref"
  },
  {
    "objectID": "topics/vc/lab_vc.html#preparelaptop",
    "href": "topics/vc/lab_vc.html#preparelaptop",
    "title": "Variant Calling",
    "section": "Laptop",
    "text": "Laptop\nThis lab will be done completely on Dardel and the instructions assume that you connect via ThinLinc. However, if you prefer to connect to Dardel via ssh you can instead copy some of the resulting files to your laptop and work on them there. , install IGV, and run all the IGV steps on your laptop. If so, please create a local workspace on your laptop, for example a folder called vc on your desktop. You need to have write permission in this folder. If you connect to Dardel via ThinLinc you don’t have to create a local workspace."
  },
  {
    "objectID": "topics/vc/lab_vc.html#hpc-cluster",
    "href": "topics/vc/lab_vc.html#hpc-cluster",
    "title": "Variant Calling",
    "section": "HPC cluster",
    "text": "HPC cluster\n\nConnect to PDC\nDuring this lab it is best to connect to Dardel with ThinLinc, which gives you a graphical remote desktop. Instructions for this is available at Connecting to PDC. Please follow the instructions in section 1.2 Remote desktop connection.\n\n\nLogon to a node\nThis lab should be done on a compute node (not the login node). First check if you already have an active job allocation using this command:\nsqueue -u username\nWhere username should be replaced with your username. If no jobs are listed you should allocate a job for this lab. If you already have an active job allocation please proceed to Connect to the node below.\nUse this code to allocate a job on Wednesday:\nsalloc -A edu25.uppmax --reservation=edu25-03-26 -t 04:00:00 -p shared -c 4 --no-shell\nUse this code to allocate a job on Thursday:\nsalloc -A edu25.uppmax --reservation=edu25-03-27 -t 04:00:00 -p shared -c 4 --no-shell\nOnce your job allocation has been granted (should not take long) please check the allocation again using:\nsqueue -u username\nYou should now see that you have an active job allocation. The node name for your job is listed under the nodelist header.\nConnect to the node:\nssh -Y nodename\n\n\nWorkspace on Dardel\nYou should work in a subfolder in your home folder on Dardel, just like you have done during the previous labs. Start by going to your course folder using this command:\n\n\ncd ~/ngsintro\n\n\nCreate a folder for this exercise and move into it:\nmkdir vc\ncd vc\nMake sure you are located in\n~/ngsintro/vc\nfor the rest of this lab.\n\n\nSymbolic links to data\nThe raw data files are located in the Data folder described above.\nCreate a symbolic link to the reference genome (in this case chromosome 2 in GRCh37) in your workspace:\n\n\nln -s /sw/courses/ngsintro/vc/data/ref/human_g1k_v37_chr2.fasta\n\n\nDo the same with the fastq files:\n\n\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_2.fq\n\n\n\n\nAccessing programs\nLoad the modules that are needed during this workshop. Remember that these modules must be loaded every time you login to Dardel, or when you connect to a new compute node.\nFirst load the bioinfo-tools module:\nmodule load bioinfo-tools\nThis makes it possible to load the individual programs:\nmodule load bwa/0.7.17\nmodule load samtools/1.20\nmodule load gatk/4.5.0.0\nAlthough you don’t have to specify which versions of the tools to use, it is recommended to do so for reproducibility if you want to rerun the exact same analyses later."
  },
  {
    "objectID": "topics/vc/lab_vc.html#index-the-genome",
    "href": "topics/vc/lab_vc.html#index-the-genome",
    "title": "Variant Calling",
    "section": "Index the genome",
    "text": "Index the genome\nTools that compare short reads with a large reference genome needs indexes of the reference genome to work efficiently. You therefore need to create index files for each tool.\nFirst check if you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nGenerate BWA index files:\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\nCheck that several new files have been created using ls -l. Then Generate a samtools index:\nsamtools faidx human_g1k_v37_chr2.fasta\nCheck to see what file(s) were created using ls -lrt. Then Generate a GATK sequence dictionary:\ngatk --java-options -Xmx2g CreateSequenceDictionary -R human_g1k_v37_chr2.fasta -O human_g1k_v37_chr2.dict\nAgain, check what file(s) were created using ls -lrt."
  },
  {
    "objectID": "topics/vc/lab_vc.html#aligning-reads",
    "href": "topics/vc/lab_vc.html#aligning-reads",
    "title": "Variant Calling",
    "section": "1.1 Aligning reads",
    "text": "1.1 Aligning reads\n\n1.1.1 BWA mem\nYou should use BWA mem to align the reads to the reference genome.\nIn the call to BWA mem you need to add something called a read group, which contains information about how the reads were generated. This is required by HaplotypeCaller. Since we don’t know exactly how the reads in the 1000 Genomes Project were generated we will assume that each pair of fastq files was generated from one library preparation (libraryx), derived from one biological sample (HG00097), and run on one lane (lanex) of a flowcell (flowcellx) on the Illumina machine, and define a toy read group with this information. The code for adding this read group is\n-R \"@RG\\\\tID:HG00097\\\\tPU:lanex_flowcellx\\\\tSM:HG00097\\\\tLB:libraryx\\\\tPL:illumina\".\n\n\n\n\n\n\nNote\n\n\n\nWhen running BWA for another sample later on you have to replace HG00097 in the read group with the new sample name. To learn more about read groups please read this article at GATK forum.\n\n\nYou also need to specify how many threads the program should use, which should be the same as the number of cores you have access. As we have asked for 4 cores we can specify -t 4 below.\nYou also need to specify what reference genome file to use.\nThe output from BWA should be parsed to samtools sort, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. Finally, use a file redirect &gt; so that the output ends up in a file and not on your screen.\nFirst make sure that you are standing in the workspace you created on Dardel for this lab:\npwd\nShould return\n~/ngsintro/vc\nThen use this command to align the reads, add the read group, sort the reads and write them to a bam file:\nbwa mem \\\n-R \"@RG\\tID:readgroup_HG00097\\tPU:lanex_flowcellx\\tSM:HG00097\\tLB:libraryx\\tPL:illumina\" \\\n-t 4 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort &gt; HG00097.bam\nPlease check that the expected output file was generated and that it has content using ls -lrt.\nNext you need to index the generated bam file so that programs can access the sorted data without reading the whole file. This command creates an index file with the same name as the input bam file, except with a .bai extension:\nsamtools index HG00097.bam\nPlease check what output file was generated this time.\n\n\n1.1.2 Check bam with samtools\nThe bam file is binary so we cannot read it directly, but we can view it with the program samtools view. The header section of the bam file can be viewed separately with the -H flag:\nsamtools view -H HG00097.bam\n\nQuestion\n\nThe @SQ tag of the bam header contains information about reference sequence. What do you think SN:2 and LN:243199373 in this tag means?\n\nThe aligned reads can be viewed with samtools view without the -H. This will display the entire bam file which is quite large, so if you just want to look at the first 5 lines (for example) you can combine samtools view with head:\nsamtools view HG00097.bam | head -n 5\n\n\nQuestion\n\nWhat is the leftmost mapping position of the first read in the bamfile?\n\nPlease have a look at the Sequence Alignment/Map Format Specification for more information about bam files.\n\n\n\n1.1.3 Check bam in IGV\nTo use IGV on Dardel we recommend that you are connected via ThinLinc. Alternatively you can install IGV on your local computer, download the files using scp, and look at them locally. The instructions below assume that you have logged in to Dardel via ThinLinc.\nIGV uses java, and unfortunately it uses another version of Java than the installed version of GATK. Therefore you will not be able to use IGV and GATK in the same terminal window (this might be different if you work on another compute system later on!). Because of this you need to open a new terminal window on Dardel to run IGV. Open a new terminal window and navigate to your workspace for this lab. Use pwd to confirm that you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nTo start IGV please type this in the terminal:\nmodule load bioinfo-tools\nmodule load IGV\nigv.sh &\nIn IGV:\nIn the upper left dropdown menu choose Human hg19 (which is the same as GRCh37).\nIn the File menu, select Load from File, navigate to your workspace for the lab, and select HG00097.bam. This data should now appear as a track in the tracks window.\nYou need to Zoom in to see the reads. You can either select a region by click and drag, or by typing a region or a gene name in the text box at the top. Remember that we have data for the region chr2:136545000-136617000 which covers the LTC gene, so you can type this region or the gene name in the search box .\nIGV can be closed by selecting exit in the File menu or by clicking x in the upper right corner of the IGV window, but you can keep it open for the rest of the lab.\n\nQuestions\n\nWhat is the read length?\nApproximately how many reads cover an arbitrary position in the genomic region we are looking at?\nWhich RefSeq Genes are located within the region chr2:136545000-136617000?"
  },
  {
    "objectID": "topics/vc/lab_vc.html#variant-calling",
    "href": "topics/vc/lab_vc.html#variant-calling",
    "title": "Variant Calling",
    "section": "1.2 Variant Calling",
    "text": "1.2 Variant Calling\n\n1.2.1 HaplotypeCaller\nNow we will detect short variants in the bam file using GATK’s HaplotypeCaller. First use pwd to check if you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nThen run:\ngatk --java-options -Xmx2g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-I HG00097.bam \\\n-O HG00097.vcf\nCheck what new files were generated with ls -lrt.\n\n\n1.2.2 Explore the vcf file\nNow you have your first vcf file containing the raw variants in the region chr2:136545000-136617000 in sample HG00097. Please look at the vcf file with less and try to understand its structure.\nVcf files contains meta-information lines starting with ##, a header line starting with #CHROM, and then data lines each containing information about one variant position in the genome. The header line defines the columns of the data lines, and to view the header line you can type this command:\ngrep '#CHROM' HG00097.vcf\n\nQuestion\n\nWhat column of the VCF file contains genotype information for the sample HG00097?\n\nThe meta-information lines starting with ##INFO defines how the data in the INFO column is encoded, and the meta-information lines starting with ##FORMAT defines how the data in the FORMAT column is encoded.\nTo view the meta-information lines describing the INFO column use:\ngrep '##INFO' HG00097.vcf\nTo view the meta-information lines describing the FORMAT column use:\ngrep '##FORMAT' HG00097.vcf\n\n\nQuestion\n\nWhat does GT in the FORMAT column of the data lines mean?\nWhat does AD in the FORMAT column of the data lines mean?\n\nTo look at the details of one specific genetic variant at position 2:136545844 use:\ngrep '136545844' HG00097.vcf\n\n\nQuestions\n\nWhat genotype does the sample HG00097 have at position 2:136545844?\nWhat are the allelic depths for the reference and alternative alles in sample HG00097 at position 2:136545844?\n\nThe following command can be used to count the data lines (i.e. number of lines that don’t start with “#”) in the vcf file:\ngrep -v \"#\" HG00097.vcf | wc -l\n\n\nQuestion\n\nHow many genetic variants was detected in HG00097?\n\nFor more detailed information about vcf files please have a look at The Variant Call Format specification.\n\n\n\n1.2.3 Check vcf in IGV\nWe assume that you have logged in to Dardel via ThinLinc.\nIf you have closed IGV please open it again as described above.\nLoad the file HG00097.vcf into tracks window of IGV as you did with the HG00097.bam file earlier (load the bam file as well if it is not already loaded). You will now see all the variants called in HG00097.\nYou can view variants in the LCT gene by typing the gene name in the search box, and you can look specifically at the variant at position chr2:136545844 by typing that position in the search box.\nPlease use IGV to answer the questions below.\n\nQuestions\n\nHover the mouse over the upper row of the vcf track. What is the reference and alternative alleles of the variant at position chr2:136545844?\nHover the mouse over the lower row of the vcf track and look under “Genotype Information”. What genotype does HG00097 have at position chr2:136545844? Is this the same as you found by looking directly in the vcf file in question 10?\nLook in the bam track and count the number of reads that have “G” and “C”, respectively, at position chr2:136545844. How is this information captured under “Genotype Attributes”? (Again, hoover the mouse over the lower row of the vcf track.)"
  },
  {
    "objectID": "topics/vc/lab_vc.html#bwa_joint",
    "href": "topics/vc/lab_vc.html#bwa_joint",
    "title": "Variant Calling",
    "section": "2.1 BWA mem",
    "text": "2.1 BWA mem\nRun BWA mem for all three samples in the data set. BWA mem should be run exactly as above, but with the new sample names. You also need to adjust the read group information so that it matches each new sample name. \nFirst use pwd to check if you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nThen use this command for every sample to align the reads, add the read group, sort the reads and write them to a bam file:\nbwa mem -R \"@RG\\tID:readgroup_&lt;sample&gt;\\tPU:lanex_flowcellx\\tSM:&lt;sample&gt;\\tLB:libraryx\\tPL:illumina\" \\\n-t 4 human_g1k_v37_chr2.fasta sample_1.fq sample_2.fq | samtools sort &gt; sample.bam\nWhere sample should be replaced with the real samples name, i.e. HG00097, HG00100 and HG00101. Please check that the expected output files were generated and have content using ls -lrt. You also need to index each output bam file:\nsamtools index &lt;sample&gt;.bam\nPlease check what output file was generated this time. If you run out of time you can click below to get paths to precomputed bam files.\n\n\n/sw/courses/ngsintro/vc/data/bam/HG00097.bam\n/sw/courses/ngsintro/vc/data/bam/HG00100.bam\n/sw/courses/ngsintro/vc/data/bam/HG00101.bam"
  },
  {
    "objectID": "topics/vc/lab_vc.html#generategvcf",
    "href": "topics/vc/lab_vc.html#generategvcf",
    "title": "Variant Calling",
    "section": "2.2 Generate g.vcf files",
    "text": "2.2 Generate g.vcf files\nHaplotypeCaller should also be run for all three samples, but this time the output for each sample needs to be in g.vcf format. This is accomplished with a small change in the HaploteypCaller command. First use pwd to check if you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nThen:\ngatk --java-options -Xmx2g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-ERC GVCF \\\n-I sample.bam \\\n-O sample.g.vcf\nPlease replace sample with the real sample names.\nIf you run out of time you can click below to get paths to the precomputed g.vcf files.\n\n\n/sw/courses/ngsintro/vc/data/vcf/HG00097.g.vcf\n/sw/courses/ngsintro/vc/data/vcf/HG00100.g.vcf\n/sw/courses/ngsintro/vc/data/vcf/HG00101.g.vcf"
  },
  {
    "objectID": "topics/vc/lab_vc.html#jointgenotyping",
    "href": "topics/vc/lab_vc.html#jointgenotyping",
    "title": "Variant Calling",
    "section": "2.3 Joint genotyping",
    "text": "2.3 Joint genotyping\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. To do this you first need to combine all individual .g.vcf files to one file using CombineGVCFs.\nFirst use pwd to check if you are standing in the correct directory:\npwd\nShould return\n~/ngsintro/vc\nThen:\ngatk --java-options -Xmx2g CombineGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V sample1.g.vcf \\\n-V sample2.g.vcf \\\n-V sample3.g.vcf \\\n-O cohort.g.vcf\nPlease replace sample1, sample2, sample3 with the real sample names. Then run GATK’s GenoteypeGVC to generate a vcf file:\ngatk --java-options -Xmx2g GenotypeGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.g.vcf \\\n-O cohort.vcf\nIf you run out of time you can click below to get paths to the precomputed cohort.g.vcf and cohort.vcf files.\n\n\n/sw/courses/ngsintro/vc/data/vcf/cohort.g.vcf\n/sw/courses/ngsintro/vc/data/vcf/cohort.vcf\n\n\n\nQuestions\n\nHow many data lines do the cohort.g.vcf file have? You can use the Linux command grep -v \"#\" cohort.g.vcf to extract all lines in “cohort.g.vcf” that don’t start with “#”, then |, and then wc -l to count those lines.\nHow many data lines do the cohort.vcf file have?\nExplain the difference in number of data lines.\nLook at the header line of the cohort.vcf file. What columns does it have?\nWhat is encoded in the last three columns of the data lines?"
  },
  {
    "objectID": "topics/vc/lab_vc.html#check-combined-vcf-file-in-igv",
    "href": "topics/vc/lab_vc.html#check-combined-vcf-file-in-igv",
    "title": "Variant Calling",
    "section": "2.4 Check combined vcf file in IGV",
    "text": "2.4 Check combined vcf file in IGV\nAgain we assume that you have logged in to Dardel via ThinLinc.\nIf you have closed IGV please open it again as described above.\nLoad the files cohort.vcf, HG00097.bam, HG00100.bam and HG00101.bam into IGV as described earlier.\nThis time lets look closer at the variant rs4988235, located at position chr2:136608646 in the HG19 reference genome. This is the variant that has been shown to lead to lactase persistence.\nPlease use IGV to answer the questions below.\n\nQuestions\n\nWhat is the reference and alternative alleles at chr2:136608646?\nWhat genotype do the three samples have at chr2:136608646? Note how genotypes are color coded in IGV.\nShould any of the individuals avoid drinking milk?\nNow compare the data shown in IGV with the data in the VCF file. Extract the row for the chr2:136608646 variant in the cohort.vcf file, for example using grep '136608646' cohort.vcf. What columns of the vcf file contain the information shown in the upper part of the vcf track in IGV?\nWhat columns of the vcf file contain the information shown in the lower part of the vcf track?\nZoom out so that you can see the MCM6 and LCT genes. Is the variant at chr2:136608646 locate within the LCT gene?\n\nIf you are interested in how this variant affects lactose tolerance please read the article by Mattar et al presented above, or in OMIM."
  },
  {
    "objectID": "topics/vc/lab_vc.html#bwa-mem",
    "href": "topics/vc/lab_vc.html#bwa-mem",
    "title": "Variant Calling",
    "section": "3.1 BWA mem",
    "text": "3.1 BWA mem\nThe first step in GATK’s best pracice variant calling workflow is to run BWA mem for each sample exactly as you did in Variant calling in cohort. You have already done this step, so please use the bam files that you generated in step 2.1 for the steps below."
  },
  {
    "objectID": "topics/vc/lab_vc.html#mark-duplicates",
    "href": "topics/vc/lab_vc.html#mark-duplicates",
    "title": "Variant Calling",
    "section": "3.2 Mark Duplicates",
    "text": "3.2 Mark Duplicates\nSometimes the same DNA fragment is sequenced multiple times, which leads to multiple reads from the same fragment in the fastq file. This can occur due to PCR amplification in the library preparation, or if one read cluster is incorrectly detected as multiple clusters by the sequencing instrument. If a duplicated read contains a genetic variant, the ratio of the two alleles might be obscured, which can lead to incorrect genotyping. It is therefore recommended (in most cases) to mark duplicate reads so that they are counted as one during genotyping.\nPlease read about Picard’s MarkDuplicates here. Picard’s MarkDuplicates has recently been incorporated into the GATK suite, but the usage example in GATKs documentation describes how to call it via the stand alone Picard program. To learn how to use it as part of the GATK module, please call MarkDuplicates without input parameters like this:\ngatk --java-options -Xmx2g MarkDuplicates\nPlease run MarkDuplicates on all three bam files generated in step 2.1. Here is the code for running MarkDuplicates on the sample HG00097:\ngatk --java-options -Xmx2g MarkDuplicates \\\n    -I HG00097.bam \\\n    -O HG00097.md.bam \\\n    -M HG00097_mdmetrics.txt"
  },
  {
    "objectID": "topics/vc/lab_vc.html#recalibrate-base-quality-scores",
    "href": "topics/vc/lab_vc.html#recalibrate-base-quality-scores",
    "title": "Variant Calling",
    "section": "3.3 Recalibrate Base Quality Scores",
    "text": "3.3 Recalibrate Base Quality Scores\nAnother source of error is systematic biases in the assignment of base quality scores by the sequencing instrument. This can be corrected by GATK’s Base Quality Score Recalibration. In short, you first use BaseRecalibrator to build a recalibration model, and then ApplyBQSR to recalibrate the base qualities in your bam file.\nBaseRecalibrator requires a file with known SNPs as input. This file is available in the data folder on Dardel:\n\n\n/sw/courses/ngsintro/vc/data/ref/1000G_phase1.snps.high_confidence.b37.chr2.vcf\n\n\nPlease recalibrate the base quality scores in all the bam files generated in the previous step. Below is our example solution for sample HG00097. First run BaseRecalibrator:\n\n\ngatk --java-options -Xmx2g BaseRecalibrator \\\n    -R human_g1k_v37_chr2.fasta \\\n    -I HG00097.md.bam \\\n    --known-sites /sw/courses/ngsintro/vc/data/ref/1000G_phase1.snps.high_confidence.b37.chr2.vcf \\\n    -O HG00097.recal.table\n\n\nThen run ApplyBQSR:\ngatk --java-options -Xmx2g ApplyBQSR \\\n    -R human_g1k_v37_chr2.fasta \\\n    -I HG00097.md.bam \\\n    --bqsr-recal-file HG00097.recal.table \\\n    -O   HG00097.recal.bam"
  },
  {
    "objectID": "topics/vc/lab_vc.html#generate-g.vcf-files",
    "href": "topics/vc/lab_vc.html#generate-g.vcf-files",
    "title": "Variant Calling",
    "section": "3.4 Generate g.vcf files",
    "text": "3.4 Generate g.vcf files\nHaplotypeCaller should also be run for all three samples, and the output should be in g.vcf exactly as described above. This time use recalibrated bam files as input."
  },
  {
    "objectID": "topics/vc/lab_vc.html#joint-genotyping",
    "href": "topics/vc/lab_vc.html#joint-genotyping",
    "title": "Variant Calling",
    "section": "3.5 Joint genotyping",
    "text": "3.5 Joint genotyping\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. This should be done with the commands CombineGVCFs and GenotypeGVCFs exactly as described above, but you should use the g.vcf files generated from the recalibrated bam files as input."
  },
  {
    "objectID": "topics/vc/lab_vc.html#variant-filtering",
    "href": "topics/vc/lab_vc.html#variant-filtering",
    "title": "Variant Calling",
    "section": "3.6 Variant Filtering",
    "text": "3.6 Variant Filtering\nHaplotypeCaller is designed to be very sensitive, which is good because it minimizes the chance of missing real variants. However, it means that the number of false positives can be quite large, so we need to filter the raw callset. GATK offers two ways to filter variants:\n\nThe variant quality score recalibration (VQSR) method uses machine learning to identify variants that are likely to be real. This is the best method if you have a lot of data, for example one whole genome sequence sample or several whole exome samples.\n\nIf you have less data you can use hard filters as described here.\n\nSince we have very little data we will use hard filters. The parameters are slightly different for SNVs and INDELs, so you need to first select all SNVs using SelectVariants and filter them using VariantFiltration with the parameters suggested for SNVs. Then select all INDELs and filter them with the parameters suggested for INDELs. Finally merge the SNVs and INDELs to get all variants in one file using MergeVCFs.\nAn explanation of what the hard filters do can be found here.\nExample solution for filtering SNVs:\ngatk --java-options -Xmx2g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include SNP \\\n  -O cohort.snvs.vcf\n\ngatk --java-options -Xmx2g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.snvs.vcf \\\n  -O cohort.snvs.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD &lt; 2.0\"  \\\n  --filter-name MQfilter --filter-expression \"MQ &lt; 40.0\"  \\\n  --filter-name FSfilter --filter-expression \"FS &gt; 60.0\"\nExample solution for filtering INDELs:\ngatk --java-options -Xmx2g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include INDEL \\\n  -O cohort.indels.vcf\n\ngatk --java-options -Xmx2g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.indels.vcf \\\n  -O cohort.indels.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD &lt; 2.0\" \\\n  --filter-name FSfilter --filter-expression \"FS &gt; 200.0\"\nExample solution for merging filtered SNVs and INDELs:\ngatk --java-options -Xmx2g MergeVcfs \\\n    -I cohort.snvs.filtered.vcf \\\n    -I cohort.indels.filtered.vcf \\\n    -O cohort.filtered.vcf\nOpen your filtered vcf with less and page through it. It still has all the variant lines, but the FILTER column that was blank before is now filled in, with PASS or a list of the filters it failed. Note also that the filters that were run are described in the header section.\n\nPrecomputed files\nIf you run out of time, please click below to get the path to precomputed bam and vcf files for the GATK’s best practices section.\n\n\nPath to intermediary and final bam files: /sw/courses/ngsintro/vc/data/best_practise_bam\nPath to intermediary and final vcf files: /sw/courses/ngsintro/vc/data/best_practise_vcf\n\n\n\n\nQuestions\n\nCheck how many variants in total that are present in the cohort.filtered.vcf file and how many that have passed the filters. Is the difference big?\nLook at the variants that did not pass the filters using grep -v 'PASS' cohort.filtered.vcf. Try to understand why these variants didn’t pass the filter."
  },
  {
    "objectID": "topics/vc/lab_vc.html#variant-calling-in-cohort",
    "href": "topics/vc/lab_vc.html#variant-calling-in-cohort",
    "title": "Variant Calling",
    "section": "Variant calling in cohort",
    "text": "Variant calling in cohort\nBelow is a skeleton script that can be used as a template for running variant calling in a cohort. Please modify it to run all the steps in part two of this workshop.\n\n\n#!/bin/bash\n#SBATCH -A edu25.uppmax \n#SBATCH -p shared\n#SBATCH -c 4\n#SBATCH -t 1:00:00\n#SBATCH -J JointVariantCalling\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.20\nmodule load gatk/4.5.0.0\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  #Fill in the code for running bwa-mem for each sample here\n  #Fill in the code for samtools index for each sample here\n  #Fill in the code for HaplotypeCaller for each sample here\ndone\n#Fill in the code for CombineGVCFs for all samples here\n#Fill in the code for GenotypeGVCFs here\n\n\nPlease save the template in your workspace and call it “joint_genotyping.sbatch” or similar. Make the script executable by this command:\nchmod u+x joint_genotyping.sbatch\nTo run the sbatch script in the SLURM queue, use this command:\nsbatch joint_genotyping.sbatch\nIf you have an active node reservation you can run the script as a normal bash script:\n./joint_genotyping.sbatch\nIf you would like more help with creating the sbatch script, please look at our example solution below (click on the link):\n\n\nExample solution\n\n\n\n#!/bin/bash\n#SBATCH -A edu25.uppmax\n#SBATCH -p shared\n#SBATCH -c 4\n#SBATCH -t 2:00:00\n#SBATCH -J JointVariantCalling\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.20\nmodule load gatk/4.5.0.0\n\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  bwa mem -R \\\n  \"@RG\\tID:${sample}\\tPU:flowcellx_lanex\\tSM:${sample}\\tLB:libraryx\\tPL:illumina\" \\\n  -t 4 human_g1k_v37_chr2.fasta \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort &gt; \"${sample}.bam\"\n\n  samtools index \"${sample}.bam\"\n\n  gatk --java-options -Xmx2g HaplotypeCaller \\\n  -R human_g1k_v37_chr2.fasta \\\n  -ERC GVCF -I \"${sample}.bam\" \\\n  -O \"${sample}.g.vcf\"\n\ndone\n\ngatk --java-options -Xmx2g CombineGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V HG00097.g.vcf \\\n-V HG00100.g.vcf \\\n-V HG00101.g.vcf \\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx2g GenotypeGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.g.vcf \\\n-O cohort.vcf"
  },
  {
    "objectID": "topics/vc/lab_vc.html#gatk-best-practices",
    "href": "topics/vc/lab_vc.html#gatk-best-practices",
    "title": "Variant Calling",
    "section": "GATK best practices",
    "text": "GATK best practices\nNow please try to incorporate the additional steps from GATK’s best practices into the workflow. If you run out of time you can sneak peek at our example solution below.\n\n\nExample solution\n\n\n\n#!/bin/bash\n#SBATCH -A edu25.uppmax\n#SBATCH -p shared\n#SBATCH -c 4\n#SBATCH -t 2:00:00\n#SBATCH -J BestPractise\n\n## load modules\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.20\nmodule load gatk/4.5.0.0\n\n# define path to reference genome\nref=\"/sw/courses/ngsintro/vc/data/ref\"\n\n# make symbolic links\nln -s /sw/courses/ngsintro/vc/data/ref/human_g1k_v37_chr2.fasta\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/vc/data/fastq/HG00101_2.fq\n\n# index reference genome\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\nsamtools faidx human_g1k_v37_chr2.fasta\ngatk --java-options -Xmx2g CreateSequenceDictionary \\\n-R human_g1k_v37_chr2.fasta \\\n-O human_g1k_v37_chr2.dict\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: ${sample}\"\n  # map the reads\n  bwa mem \\\n  -R \"@RG\\tID:${sample}\\tPU:flowcellx_lanex\\tSM:${sample}\\tLB:libraryx\\tPL:illumina\" \\\n  -t 4 human_g1k_v37_chr2.fasta \\\n  \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort &gt; \"${sample}.bam\"\n\n  samtools index $sample\".bam\"\n  # mark duplicates\n  gatk --java-options -Xmx2g MarkDuplicates \\\n  -I \"${sample}.bam\" \\\n  -O \"${sample}.md.bam\" \\\n  -M \"${sample}_mdmetrics.txt\"\n\n  # base quality score recalibration\n  gatk --java-options -Xmx2g BaseRecalibrator \\\n  -R human_g1k_v37_chr2.fasta \\\n  -I \"${sample}.md.bam\" \\\n  --known-sites \"${ref}/1000G_phase1.snps.high_confidence.b37.chr2.vcf\" \\\n  -O \"${sample}.recal.table\"\n\n  gatk --java-options -Xmx2g ApplyBQSR \\\n  -R human_g1k_v37_chr2.fasta \\\n  -I \"${sample}.md.bam\" \\\n  --bqsr-recal-file \"${sample}.recal.table\" \\\n  -O \"${sample}.recal.bam\"\n\n  # haplotypeCaller in -ERC mode\n  gatk --java-options -Xmx2g HaplotypeCaller \\\n  -R human_g1k_v37_chr2.fasta \\\n  -ERC GVCF \\\n  -I \"${sample}.bam\" \\\n  -O \"${sample}.g.vcf\"\ndone\n\n# joint genotyping\ngatk --java-options -Xmx2g CombineGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V HG00097.g.vcf \\\n-V HG00100.g.vcf \\\n-V HG00101.g.vcf \\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx2g GenotypeGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.g.vcf \\\n-O cohort.vcf\n\n# variant filtration SNPs\ngatk --java-options -Xmx2g SelectVariants \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.vcf \\\n--select-type-to-include SNP \\\n-O cohort.snvs.vcf\n\ngatk --java-options -Xmx2g VariantFiltration \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.snvs.vcf \\\n--filter-name QDfilter --filter-expression \"QD &lt; 2.0\" \\\n--filter-name MQfilter --filter-expression \"MQ &lt; 40.0\" \\\n--filter-name FSfilter --filter-expression \"FS &gt; 60.0\" \\\n-O cohort.snvs.filtered.vcf\n\n# variant filtration indels\ngatk --java-options -Xmx2g SelectVariants \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.vcf \\\n--select-type-to-include INDEL \\\n-O cohort.indels.vcf\n\ngatk --java-options -Xmx2g VariantFiltration \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.indels.vcf \\\n--filter-name QDfilter --filter-expression \"QD &lt; 2.0\" \\\n--filter-name FSfilter --filter-expression \"FS &gt; 200.0\" \\\n-O cohort.indels.filtered.vcf\n\n# merge filtered SNPs and indels\ngatk --java-options -Xmx2g MergeVcfs \\\n-I cohort.snvs.filtered.vcf \\\n-I cohort.indels.filtered.vcf \\\n-O cohort.filtered.vcf"
  }
]