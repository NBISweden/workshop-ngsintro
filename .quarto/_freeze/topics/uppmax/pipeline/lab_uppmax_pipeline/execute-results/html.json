{
  "hash": "3dbf9913a0bc188a252ffd57495b4bc6",
  "result": {
    "markdown": "---\ntitle: 'Uppmax Pipeline'\nsubtitle: \"Building Bioinformatic pipelines\"\nauthor: 'Martin Dahl√∂'\nformat: html\n---\n\n\n\n\n::: {.callout-note}\nIn code blocks, the dollar sign (`$`) is not to be printed. The dollar sign is usually an indicator that the text following it should be typed in a terminal window.\n:::\n\n# Connect to UPPMAX\n\nThe first step of this lab is to open a ssh connection to UPPMAX. Please refer to [**Connecting to UPPMAX**](topics/other/lab_connect.html) for instructions. Once connected to UPPMAX, return here and continue reading the instructions below.\n\n# Logon to a node\n\nUsually you would do most of the work in this lab directly on one of the login nodes at UPPMAX, but we have arranged for you to have one core each for better performance. This was covered briefly in the lecture notes.\n\nCheck which node you got when you booked resources this morning (replace **username** with your UPPMAX username)\n\n```bash\n$ squeue -u username\n```\n\nshould look something like this\n\n```bash\ndahlo@rackham2 work $ squeue -u dahlo\n             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n           3132376      core       sh    dahlo  R       0:04      1 r292\ndahlo@rackham2 work $\n```\n\nwhere **r292** is the name of the node I got (yours will probably be different).\nNote the numbers in the Time column. They show for how long the job has been running. When it reaches the time limit you requested (7 hours in this case) the session will shut down, and you will lose all unsaved data. Connect to this node from within UPPMAX.\n\n```bash\n$ ssh -Y r292\n```\n\nIf the list is empty you can run the allocation command again and it should be in the list:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n$ salloc -A snic2023-xx-xxxx -t 03:30:00 -p core -n 1 --no-shell\n```\n:::\n:::\n\n\n{{< fa lightbulb >}} There is a UPPMAX specific tool called `jobinfo` that supplies the same kind of information as `squeue` that you can use as well (`$ jobinfo -u username`).\n\n# Copy files for lab\n\nNow, you will need some files. To avoid all the course participants editing the same file all at once, undoing each other's edits, each participant will get their own copy of the needed files. The files are located in the folder **`/sw/courses/ngsintro/linux/uppmax_pipeline_exercise/data`**.\n\nNext, copy the lab files from this folder. `-r` means recursively, which means all the files including sub-folders of the source folder. Without it, only files directly in the source folder would be copied, NOT sub-folders and files in sub-folders.\n\n{{< fa lightbulb >}} Remember to use tab-complete to avoid typos and too much writing.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n$ cp -r <source> <destination>\n$ cp -r /sw/courses/ngsintro/linux/uppmax_pipeline_exercise/data/* /proj/snic2023-xx-xxxx/nobackup/username/uppmax_pipeline_exercise\n```\n:::\n:::\n\n\nHave a look in **/proj/snic2023-xx-xxxx/nobackup/&lt;username&gt;/uppmax_pipeline_exercise**.\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n$ cd /proj/snic2023-xx-xxxx/nobackup/username/uppmax_pipeline_exercise\nll\n```\n:::\n:::\n\n\nIf you see files, the copying was successful.\n\n# Running dummy pipelines\n\nMost of the work you will do in the future will be about running multiple programs after each other. This can be done manually, with you sitting by the computer and typing commands, waiting for them to finish, then start the next program. But what happens if the first program finished sometime during the night? You will not be there to start the next program, and the computer will stand idle until you have time to start the program.\n\nTo avoid this, scripts can be used. First, we'll do an analysis manually without scripts, just to get the hang of it. Then we'll start writing scripts for it!\n\n## Load the module\n\nIn this exercise, we'll pretend that we are running analyses. This will give you a peek at what running programs in linux is like, and get you ready for the real stuff during the week!\n\nThe first thing you usually do is to load the modules for the programs you want to run. During this exercise we'll only run my dummy scripts that don't actually do any analysis, so they don't have a module of their own. What we can do instead is to manually do what module loading usually does: to modify the **`$PATH variable`**.\n\nThe `$PATH` variable specifies directories where the computer should look for programs whenever you type a command.\nFor instance, when you type\n\n```bash\n$ nano\n```\n\nhow does the computer know which program to start? You gave it the name `nano`, but that could refer to any file named nano in the computer, yet it starts the correct one every time. The answer is that it looks in the directories stored in the `$PATH` variable and start the first program it finds that is named `nano`.\n\nTo see which directories that are available by default, type\n\n```bash\n$ echo $PATH\n```\n\nIt should give you something like this, a list of directories, separated by colon signs:\n\n```bash\n$ echo $PATH\n/home/dahlo/perl//bin/:/home/dahlo/.pyenv/shims:/home/dahlo/.pyenv/bin:\n/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:/usr/sbin:\n/sbin:/opt/thinlinc/bin:/sw/uppmax/bin:/home/dahlo/usr/bin\n```\n\nTry loading a module, and then look at the `$PATH` variable again. You'll see that there are a few extra directories there now, after the module has been loaded.\n\n```bash\n$ module load bioinfo-tools samtools/1.6\n$ echo $PATH\n/sw/apps/bioinfo/samtools/1.6/rackham/bin:/home/dahlo/perl/bin:/home/dahlo/.pyenv/shims:\n/home/dahlo/.pyenv/bin:/usr/lib64/qt-3.3/bin:/usr/local/bin:/bin:/usr/bin:/usr/local/sbin:\n/usr/sbin:/sbin:/opt/thinlinc/bin:/sw/uppmax/bin:/home/dahlo/usr/bin\n```\n\nTo pretend that we are loading a module, instead of actually loading a module for them, we'll manually do what the module system would have done. We will just add a the directory containing my dummy scripts to the `$PATH` variable, and it will be like we loaded the module for them. Now, when we type the name of one of my scripts, the computer will look in all the directories specified in the `$PATH` variable, which now includes the location where i keep my scripts. The computer will now find programs named as my scripts are and it will run them.\n\n```bash\n$ export PATH=$PATH:/sw/courses/ngsintro/linux/uppmax_pipeline_exercise/dummy_scripts\n```\n\nThis will set the `$PATH` variable to whatever it is at the moment, and add a directory at the end of it. Note the lack of a dollar sign infront of the variable name directly after **export**. You don't use dollar signs when **assigning** values to variables, and you always use dollar signs when **getting** values from variables.\n\n::: {.alert .alert-warning}\n\n{{< fa exclamation-circle >}} **Important**\n\nThe export command affects only the terminal you type it in. If you have 2 terminals open, only the terminal you typed it in will have a modified path. If you close that terminal and open a new one, it will not have the modified path.\n\n:::\n\nEnough with variables now. Let's try the scripts out!\n\n# Running the programs\n\nLet's pretend that we want to run an exome analysis. You will learn how to do this for real later this week. This kind of analysis usually has the following steps:\n\n1. Filter out low quality reads.\n2. Align the reads to a reference genome.\n3. Find all the SNPs in the data.\n\nTo simulate this, I have written 3 programs:\n\n* filter_reads\n* align_reads\n* find_snps\n\nTo find out how to run the programs type\n\n```bash\n$ <program name> -h\n```\n\n```bash\nor\n$ <program name> --help\n```\n\nThis is useful to remember, since most programs has this function. If you do this for the filter program, you get\n\n```bash\n$ filter_reads -h\nUsage: filter_reads -i <input file> -o <output file> [-c <cutoff>]\n\nExample runs:\n\n# Filter the reads in <input> using the default cutoff value. Save filtered reads to <output>\nfilter_reads -i <input> -o <output>\nEx.\nfilter_reads -i my_reads.rawdata.fastq -o my_reads.filtered.fastq\n\n# Filter the reads in <input> using a more relaxed cutoff value. Save filtered reads to <output>\nfilter_reads --input <input> --output <output> --cutoff 30\nEx.\nfilter_reads --input ../../my_reads.rawdata.fastq --output /home/dahlo/results/my_reads.filtered.fastq --cutoff 30\n\n\nOptions:\n  -h, --help            show this help message and exit\n  -i INPUT, --input=INPUT\n                        The path to your unfiltered reads file.\n  -o OUTPUT, --output=OUTPUT\n                        The path where to put the results of the filtering.\n  -c CUTOFF, --cutoff=CUTOFF\n                        The cutoff value for quality. Reads below this value\n                        will be filtered (default: 35).\n\n```\n\nThis help text tells you that the program has to be run a certain way. The options `-i` and `-o` are mandatory, since they are explicitly written. The hard brackets `[ ]` around `-c <cutoff>` means that the cutoff value is NOT mandatory. They can be specified if the user wishes to change the cutoff from the default values.\n\nFurther down, in the `Options:` section, each of the options are explained more in detail. You can also see that each option can be specified in two way, a short and a long format. The cutoff value can be specified either by the short `-c`, or the long `--cutoff`. It doesn't matter which format you choose, it's completely up to you, which ever you feel more comfortable with.\n\nRight, so now you know how to figure out how to run programs (just type the program name, followed by a `-h` or `--help`). Try doing a complete exome sequencing analysis, following the steps below.\n\nFirst, go to the exome directory in the lab directory that you copied to your folder in step 2 in this lab:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n$ cd /proj/snic2023-xx-xxxx/nobackup/username/uppmax_pipeline_exercise/exomeSeq\n```\n:::\n:::\n\n\nIn there, you will find a folder called `raw_data`, containing a fastq file: `my_reads.rawdata.fastq`. This file contains the raw data that you will analyse.\n\n* Filter the raw data using the program `filter_reads`, to get rid of low quality reads.\n* Align the filtered reads with the program `align_reads`, to the human reference genome located here:\n\n```bash\n/sw/data/uppnex/reference/Homo_sapiens/hg19/concat_rm/Homo_sapiens.GRCh37.57.dna_rm.concat.fa\n```\n\n* Find SNPs in your aligned data with the program `find_snps`. To find SNPs we have to have a reference to compare our data with. The same reference genome as you aligned to is the one to use.\n\nDo one step at a time, and check the `--help` of the programs to find out how they are to be run. Remember to name your files logically so that you don't confuse them with each other.\n\nMost pipelines work in a way where the output of the current program is the input of the next program in the pipeline.\nIn this pipeline, raw data gets filtered, the filtered data gets aligned, and the aligned data gets searched for SNPs. The intermediate steps are usually not interesting after you have reached the end of the pipeline. Then, only the raw data and the final result is important.\n\n# Scripting a dummy pipeline\n\nTo run the pipeline in a script, just do exactly as you just did, but write the exact same commands to a file instead of directly to the terminal. When you run the script, the computer will run the script one line at a time, until it reaches the end of the file. Just like you did manually in the previous step.\n\nThe simplest way to work with scripts is to have 2 terminals open. One will have `nano` started where you write your script file, and the other will be on the command line where you can test your commands to make sure they work before you put them in the script. When you are sure a command works, you copy/paste it to the terminal with the script file in it.\n\nStart writing you script with `nano`:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n$ cd /proj/snic2023-xx-xxxx/nobackup/username/uppmax_pipeline_exercise/exomeSeq\nnano exome_analysis_script.sh\n```\n:::\n:::\n\n\nThe `.sh` ending is commonly used for **sh**ell scripts which is what we are creating. The default shell at UPPMAX is as we know called bash, so whenever we write `sh` the computer will use bash. If the default shell at UPPMAX would change for some reason, maybe to **zsh** or any other type of shell, `sh` would point the the new shell instead.\n\n![](assets/dualTerminals.png)\n\nSince our memory is far from perfect, try to **always comment your scripts**. The comments are rows that start with a hash sign `#`. These lines will not be interpreted as a command to be run, they will just be skipped. They are only meant for humans to read, and they are real lifesavers when you are reading old scripts you have forgotten what they do. Commands are hard for humans to read, so try to write a couple of words explaining what the command below does. You'll be thankful later!\n\nWhen you are finished with your script, you can test run it. To do that, use the program `sh`:\n\n```bash\n$ sh exome_analysis_script.sh\n```\n\nIf you got everything right, you should see the whole pipeline being executed without you having to start each program manually. If something goes wrong, look at the output and try to figure out which step in the pipeline that get the error, and solve it.\n\nA tip is to read the error list from the top-down. An error early in the pipeline will most likely cause a multitude of error further down the pipeline, so your best bet is to start from the top. Solve the problem, try the script again, until it works. The real beauty of scripts is that they can be re-run really easily. Maybe you have to change a variable or option in one of the early steps of the pipeline, just do it and re-run the whole thing.\n\n# Submitting a dummy pipeline\n\nThe whole point with computer centres like UPPMAX is that you can run multiple programs at the same time to speed things up. To do this efficiently you will have to submit jobs to the queue system. As you saw in yesterday's exercise, it is ordinary shell scripts that you submit to the queue system, with a couple of extra options in the beginning. So to be able to submit our script to the queue system, the only thing we have to do is to add the queue system options in the beginning of the script.\n\nThe options needed by the queue are, as we learned yesterday:\n\n* Who is paying for the job?\n* How long time will the job need?\n* How many cores does the job need?\n\n**SLURM** is also a bit strict when it comes formalities. It requires that you specify which program should be used to run the submitted script file. The standard program for this is bash, but we have to specify it on the first line of the script non the less. This is done by having the first line in the script looking link this:\n\n```bash\n#!/bin/bash -l\n```\n\nThis is how Linux knows which program should open a file, since it does not care about the file ending like Windows commonly does (.ppt, .pdf, .html might be familiar file endings). The `#!` indicates that the next words will be the path to the program that should open the file. It could be `#!/bin/bash`, or `#!/bin/python`, or any other path to a executable program.\n\nThe `-l` after bash is a flag that tells bash that the script should be treated as a login shell, so everything behaves as when you are logged in. Without it commands like `module` and some other would not work. If it's not a login shell, it's running as a script, and then it does not need to bother making the interface human friendly so it will skip things to make it start faster and consume less resources.\n\nThe next couple of rows will contain all the options you want to give SLURM:\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```{.bash}\n#!/bin/bash -l\n#SBATCH -A snic2023-xx-xxxx\n#SBATCH -t 00:05:00\n#SBATCH -p core\n```\n:::\n:::\n\n\nSLURM options always start with **`#SBATCH`** followed by a flag (`-A` for account, `-t` for time, `-p` for partition) and the value for that flag. Your script should now look something like this (ignore the old project id and path to the scripts):\n\n![](assets/slurmScript.png)\n\nTo submit this script to the queue:\n\n```bash\n$ sbatch exome_analysis_script.sh\n```\n\n# RNAseq Analysis\n\nThe next step is to do a complete RNAseq analysis. The steps involved start off just like the exome analysis, but has a few extra steps. The goal of this part is to successfully run the pipeline using the queue system. To do this, you must construct the commands for each step, combine them in a script, include the `SLURM` options, and submit it. Much like what we did in the previous step, but with some complications.\n\nTypical RNAseq analysis consists of multiple samples / time points:\n\n* Filter the reads for each sample.\n* Align the filtered reads for each sample to the same reference genome as before.\n* Do a differential expression analysis by comparing multiple samples.\n\nThe difficulty here is that you have not just 1 sample, you have 3 of them. And they all need to be filtered and aligned separately, and then compared to each other. The program that does the differential expression analysis in this exercise is called `diff_exp` and is located in the same directory as the previous scripts. The samples are filtered and aligned individually.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}