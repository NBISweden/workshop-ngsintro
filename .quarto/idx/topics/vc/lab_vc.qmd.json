{"title":"Variant Calling","markdown":{"yaml":{"title":"Variant Calling","subtitle":"From reads to short variants","author":"Malin Larsson","format":"html"},"headingText":"VARIABLES","containsRefs":false,"markdown":"\n\n```{r,eval=TRUE,include=FALSE}\nlibrary(yaml)\nlibrary(here)\n\nupid <- yaml::read_yaml(here(\"_quarto.yml\"))$uppmax_project\n```\n\n```{css,include=FALSE}\n.workLocally{\nbackground-color: red;\n}\n```\n\n```{r,include=FALSE}\ndatadir <- \"/sw/courses/ngsintro/reseq/data\"\nfastqdir <- \"/sw/courses/ngsintro/reseq/data/fastq\"\nrefdir <- \"/sw/courses/ngsintro/reseq/data/ref\"\nbamdir <- \"/sw/courses/ngsintro/reseq/data/bam\"\nvcfdir <- \"/sw/courses/ngsintro/reseq/data/vcf\"\nbpbamdir <- \"/sw/courses/ngsintro/reseq/data/best_practise_bam\"\nbpvcfdir <- \"/sw/courses/ngsintro/reseq/data/best_practise_vcf\"\ncol_uppmax <- \"#f4f8e8\"\ncol_local <- \"#e5f4f8\"\n```\n\n# Introduction {-}\n\nWhole genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. This workshop will take you through the process of calling germline short variants (SNVs and INDELs) in WGS data from three human samples.\n\n1. The first part of the workshop will guide you through a basic variant calling workflow in one sample. The goals are that you should get familiar with the bam and vcf file formats, and be able to interpret vcf files in Integrative Genomics Viewer (IGV).\n2. If you have time, the next part of the workshop will show you how to perform joint variant calling in three samples. The goals here are that you should be able to interpret multi-sample vcf files and explain the differences between the g.vcf and vcf file formats.\n3. If you have time, the last part of the workshop will take you through the GATK best practices for germline short variant detection in three samples. The goal here is that you should learn how to use GATK's documentation so that you can analyze your own samples in the future.\n\n::: {.callout-note}\n\n## General guide\n\n* You will work on the computing cluster Rackham at UPPMAX\n* If you change the node you are working on you will need to reload the tool modules.\n* Please type commands in the terminal instead of copying and pasting them which often result in formatting errors.\n* Use tab completion.\n* In paths, please replace `username` with your actual UPPMAX username.\n* In commands, please replace `parameter` with the correct parameter, for example your input file name, output file name, directory name, etc.\n* A line starting with `#` is a comment\n* Running a command without parameters will often return a help message on how to run the command.\n* After a command is completed, please check that the desired output file was generated and that it has a reasonable size (use `ls -l`).\n* Google errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere.\n:::\n\n# Data description {-}\n## Samples  {-}\n\nThe 1000 Genomes Project ran between 2008 and 2015, creating the largest public catalogue of human variation and genotype data. In this workshop we will use low coverage whole genome sequence data from three individuals, generated in the first phase of the 1000 Genomes Project.\n\nSample        | Population | Sequencing technology\n------------- | ---------- | --------\nHG00097       | British in England and Scotland    | Low coverage WGS\nHG00100       | British in England and Scotland    | Low coverage WGS\nHG00101       | British in England and Scotland    | Low coverage WGS\n\n## Genomic region {#Genomicregion -}  \n\nThe *LCT* gene on chromosome 2 encodes the enzyme lactase, which is responsible for the metabolism of lactose in mammals.\nMost mammals can not digest lactose as adults, but some humans can.\nGenetic variants upstream of the *LCT* gene causes lactase persistence, which means that lactase is expressed also in adulthood and the carrier can continue to digest lactose.\nThe variant rs4988235, located at position chr2:136608646 in the GRCh37 reference genome, has been shown to lead to lactose persistence.\nThe alternative allele (A on the forward strand and T on the reverse strand) creates a new transcription factor binding site that enables continued expression of the gene after weaning.\n\nIn this workshop we will detect genetic variants in the region chr2:136545000-136617000 in the three samples listed above, and check if they carry the allele for lactase persistence.\n\nFor those interested in the details of the genetic bases for lactose tolerance, please read the first three pages of [Lactose intolerance: diagnosis, genetic, and clinical factors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3401057/pdf/ceg-5-113.pdf) by Mattar et al. The variant rs4988235 is here referred to as LCT-13910C>T.  \n\n## Data folder on UPPMAX {#Data -}  \n\nAll input data for this exercise is located in this folder on Rackham:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(datadir))\n```\n\nThe fastq files are located in this folder:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(fastqdir))\n```\n\nReference files, such as the reference genome in fasta format, are located in this folder:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(refdir))\n```\n\n# Preparations {-}\n\n## Laptop {#preparelaptop -}\nThis lab will be done completely on UPPMAX and the instructions assume that you connect via ThinLinc.\nHowever, if you prefer to connect to UPPMAX via ssh you can instead copy some of the resulting files to your laptop and work on them there. , install IGV, and run all the IGV steps on your laptop.\nIf so, please create a local workspace on your laptop, for example a folder called *ngsworkflow* on your desktop. You need to have write permission in this folder.\nIf you connect to UPPMAX via ThinLinc you don't have to crete a local workspace.\n\n## UPPMAX {-}\n### Connect to UPPMAX {-}\n\nDuring this lab it is best to connect to UPPMAX with ThinLinc, which gives you a graphical remote desktop. Instructions for this is available at [**Connecting to UPPMAX**](topics/other/lab_connect.html). Please follow the instructions in section 1.2 Remote desktop connection.\n\n### Logon to a node {-}\n\nThis lab should be done on a compute node (not the login node). First check if you already have an active job allocation using this command:\n\n```bash\nsqueue -u username\n```\n\nWhere `username` should be replaced with your username.<br/>\nIf no jobs are listed you should allocate a job for this lab. If you already have an active job allocation please proceed to **Connect to the node** below.<br/>\n\nUse this code to allocate a job on day 1 of variant-calling:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('salloc -A ',upid,' -t 04:00:00 -p core -n 1 --no-shell'))\n```\n\nUse this code to allocate a job on day 2 of variant-calling:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('salloc -A ',upid,' -t 04:00:00 -p core -n 1 --no-shell'))\n```\n\nOnce your job allocation has been granted (should not take long) please check the allocation again using:\n\n```bash\nsqueue -u username\n```\n\nYou should now see that you have an active job allocation. The node name for your job is listed under the nodelist header.<br/>\n\n**Connect to the node:**\n```bash\nssh -Y nodename\n```\n\n### Workspace on UPPMAX {-}\n\nYou should work in your folder under the courseâ€™s nobackup folder, just like you have done during the previous labs.\n\nStart by going there using this command:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('cd /proj/',upid,'/nobackup/username'))\n```\n\nWhere `username` should be replaced with your real username.\nCreate a folder for this exercise and move into it:\n\n```bash\nmkdir ngsworkflow\ncd ngsworkflow\n```\n\nMake sure you are located in\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nfor the rest of this lab.\n\n### Symbolic links to data {-}\n\nThe raw data files are located in the [Data](#Data) folder described above.   \nCreate a symbolic link to the reference genome (in this case chromosome 2 in GRCh37) in your workspace:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('ln -s ',refdir,'/human_g1k_v37_chr2.fasta'))  \n```\n\nDo the same with the fastq files:  \n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('ln -s ',fastqdir,'/HG00097_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00097_2.fq\\n'))  \ncat(paste0('ln -s ',fastqdir,'/HG00100_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00100_2.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00101_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00101_2.fq\\n'))\n```\n\n### Accessing programs {#Accessingprograms -}\n\nLoad the modules that are needed during this workshop. Remember that these modules must be loaded every time you login to Rackham, or when you connect to a new compute node.  \nFirst load the bioinfo-tools module:\n\n```bash\nmodule load bioinfo-tools\n```\n\nThis makes it possible to load the individual programs:\n\n```bash\nmodule load FastQC/0.11.8\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n```\n\nAlthough you don't have to specify which versions of the tools to use, it is recommended to do so for reproducibility if you want to rerun the  exact same analyses later.\nWhen loading the module GATK/4.1.4.1 you will get a warning message about the fact that GATK commands have been updated since the previous version of GATK. This is fine and you don't have to do anything about it.\n\n## Index the genome {-}\n\nTools that compare short reads with a large reference genome needs indexes of the reference genome to work efficiently. You therefore need to create index files for each tool.<br/>\n\nFirst check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nGenerate BWA index files:\n\n```bash\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\n```\n\nCheck that several new files have been created using `ls -l`. Then Generate a samtools index:\n\n```bash\nsamtools faidx human_g1k_v37_chr2.fasta\n```\n\nCheck to see what file(s) were created using `ls -lrt`. Then Generate a GATK sequence dictionary:\n\n```bash\ngatk --java-options -Xmx7g CreateSequenceDictionary -R human_g1k_v37_chr2.fasta -O human_g1k_v37_chr2.dict\n```\n\nAgain, check what file(s) were created using `ls -lrt`.\n\n# Variant calling in one sample\n\nNow let's start the main part of the workshop, which is variant calling in one sample. The workflow consists of aligning the reads with [BWA](http://bio-bwa.sourceforge.net) and detecting variants with [HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller) as illustrated below.\n\n![](assets/1_onesamplevc.png)\n\n## Aligning reads\n\n### BWA mem {#bwamem}\n\nYou should use `BWA mem` to align the reads to the reference genome.  \n\nIn the call to `BWA mem` you need to add something called a *read group*, which contains information about how the reads were generated.\nThis is required by HaplotypeCaller. Since we don't know exactly how the reads in the 1000 Genomes Project were generated we will\nassume that each pair of fastq files was generated from one library preparation (*libraryx*), derived from one biological sample (*HG00097*), and run on one lane (*lanex*) of a flowcell (*flowcellx*) on the Illumina machine, and define a toy read group with this information.\nThe code for adding this read group is<br/>\n\n**-R \"@RG\\\\tID:HG00097\\\\tPU:lanex_flowcellx\\\\tSM:HG00097\\\\tLB:libraryx\\\\tPL:illumina\"**.\n\n::: {.callout-note}\nWhen running BWA for another sample later on you have to replace *HG00097* in the read group with the new sample name. To learn more about read groups  please read [this article](https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups) at GATK forum.\n:::\n\nYou also need to specify how many threads the program should use (should be the same as the number of cores you have access to and is defined by the code `-t 1` below) and what reference genome file to use. The output from `BWA` should be parsed to `samtools sort`, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. Finally, use a file redirect `>` so that the output ends up in a file and not on your screen.\n\nFirst make sure that you are standing in the workspace you created on UPPMAX for this lab:\n\n```bash\npwd\n```\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen use this command to align the reads, add the read group, sort the reads and write them to a bam file:\n\n```bash\nbwa mem \\\n-R \"@RG\\tID:readgroup_HG00097\\tPU:lanex_flowcellx\\tSM:HG00097\\tLB:libraryx\\tPL:illumina\" \\\n-t 1 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort > HG00097.bam\n```\n\nPlease check that the expected output file was generated and that it has content using `ls -lrt`.\n\nNext you need to index the generated bam file so that programs can randomly access the sorted data without reading the whole file. This command creates an index file with the same name as the input bam file, except with a .bai extension:\n\n```bash\nsamtools index HG00097.bam\n```\nPlease check what output file was generated this time.\n\n### Check bam with samtools\n\nThe bam file is binary so we cannot read it directly, but we can view it with the program `samtools view`. The header section of the bam file can be viewed separately with the `-H` flag:\n\n```bash\nsamtools view -H HG00097.bam\n```\n\n#### Question {-}\n\n1. The @SQ tag of the bam header contains information about reference sequence. What do you think SN:2 and LN:243199373 in this tag means?\n\nThe aligned reads can be viewed with `samtools view` without the `-H`. This will display the entire bam file which is quite large, so if you just want to look at the first 5 lines (for example) you can combine `samtools view` with `head`:\n\n```bash\nsamtools view HG00097.bam | head -n 5\n```\n\n#### Question {-}\n\n2. What is the leftmost mapping position of the first read in the bamfile?\n\nPlease have a look at the [Sequence Alignment/Map Format Specification](https://samtools.github.io/hts-specs/SAMv1.pdf) for more information about bam files.\n\n### Check bam in IGV\n\nTo use IGV on UPPMAX we recommend that you are connected via ThinLinc.\nAlternatively you can install IGV on your local computer, download the files using scp, and look at them locally.\nThe instructions below assume that you have logged in to UPPMAX via ThinLinc.\\\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nTo start IGV please type this in the terminal:\n\n```bash\nmodule load IGV/2.8.13\nigv.sh &\n```\n\nIn IGV:\\\nIn the upper left dropdown menu choose `Human hg19` (which is the same as GRCh37).\\\nIn the `File` menu, select `Load from File` and select `HG00097.bam`, which should then appear in the tracks window.\\\nZoom in to see the reads. You can either select a region by click and drag, or by typing a region or a gene name in the text box at the top. Remember that we have data for the region chr2:136545000-136617000.\\\nIGV can be closed by selecting exit in the `File` menu or by clicking `x` in the upper right corner of the IGV window, but you can keep it open for the rest of the lab.\n\n#### Questions {-}\n\n3. What is the read length?\n4. Approximately how many reads cover an arbitrary position in the genomic region we are looking at?\n5. Which RefSeq Genes are located within the region chr2:136545000-136617000?\n\n## Variant Calling\n\n### HaplotypeCaller {#haplotypecaller}\n\nNow we will detect short variants in the bam file using GATK's `HaplotypeCaller`. First use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen run:\n\n```bash\ngatk --java-options -Xmx7g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-I HG00097.bam \\\n-O HG00097.vcf\n```\n\nCheck what new files were generated with `ls -lrt`.  \n\n### Explore the vcf file\n\nNow you have your first vcf file containing the raw variants in the region chr2:136545000-136617000 in sample HG00097. Please look at the vcf file with `less` and try to understand its structure.  \n\nVcf files contains meta-information lines starting with *##*, a header line starting with *#CHROM*, and then data lines each containing information about one variant position in the genome. The header line defines the columns of the data lines, and to view the header line you can type this command:  \n\n```bash\ngrep '#CHROM' HG00097.vcf\n```\n\n#### Question {-}\n\n6.  What column of the VCF file contains genotype information for the sample HG00097?\n\nThe meta-information lines starting with *##INFO* defines how the data in the *INFO* column is encoded,\nand the meta-information lines starting with *##FORMAT* defines how the data in the *FORMAT* column is encoded.\\\nTo view the meta-information lines describing the *INFO* column use:\n\n```bash\ngrep '##INFO' HG00097.vcf\n```\n\nTo view the meta-information lines describing the *FORMAT* column use:\n\n```bash\ngrep '##FORMAT' HG00097.vcf\n```\n\n#### Question {-}\n\n7. What does *GT* in the *FORMAT* column of the data lines mean?\n8. What does *AD* in the *FORMAT* column of the data lines mean?\n\nTo look at the details of one specific genetic variant at position 2:136545844 use:\n\n```bash\ngrep '136545844' HG00097.vcf\n```\n\n#### Questions {-}\n\n9. What genotype does the sample HG00097 have at position 2:136545844?\n10. What are the allelic depths for the reference and alternative alles in sample HG00097 at position 2:136545844?\n\nThe following command can be used to count the data lines (i.e. number of lines that don't start with \"#\") in the vcf file:\n\n```bash\ngrep -v \"#\" HG00097.vcf | wc -l\n```\n\n#### Question {-}\n\n11. How many genetic variants was detected in HG00097?\n\nFor more detailed information about vcf files please have a look at [The Variant Call Format specification](https://samtools.github.io/hts-specs/VCFv4.2.pdf).\n\n### Check vcf in IGV {#vcfinigv}\n\nWe assume that you have logged in to UPPMAX via ThinLinc.\\\nIf you have closed IGV please open it again as described above.\\\nLoad the file HG00097.vcf into tracks window of IGV as you did with the HG00097.bam file earlier (load the bam file as well if it is not already loaded). You will now see all the variants called in HG00097.\\\nYou can view variants in the *LCT* gene by typing the gene name in the search box, and you can look specifically at the variant at position chr2:136545844 by typing that position in the search box.\\\nPlease use IGV to answer the questions below.\n\n#### Questions {-}\n\n12. Hover the mouse over the upper row of the vcf track. What is the reference and alternative alleles of the variant at position chr2:136545844?\n13. Hover the mouse over the lower row of the vcf track and look under \"Genotype Information\". What genotype does HG00097 have at position chr2:136545844? Is this the same as you found by looking directly in the vcf file in question 10?\n14. Look in the bam track and count the number of reads that have \"G\" and \"C\", respectively,  at position chr2:136545844. How is this information captured under \"Genotype Attributes\"? (Again, hoover the mouse over the lower row of the vcf track.)\n\n# Variant calling in cohort {#jointvc}\n\nIf you have time, you can now try joint variant calling in all three samples. Each sample has to be processed with `BWA mem` as above, and then with `HaplotypeCaller` with the flag `-ERC` to generate one g.vcf file per sample. The individual g.vcf files should subsequently be combined with GATK's `CombineGVCFs`, and translated into vcf format with GATK's `GenotypeGVCFs`. The workflow below shows how the three samples should be processed and combined.  \n\n![](assets/2_jointvc.png)\n\nIf you don't have time to complete all steps we have made precomputed intermediary file available. Please see links under each analysis step.\n\n## BWA mem {#bwa_joint}\n\nRun `BWA mem` for all three samples in the data set. `BWA mem` should be run exactly as above, but with the new sample names.\nYou also need to adjust the read group information so that it matches each new sample name. <br/>\n\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen use this command for every sample to align the reads, add the read group, sort the reads and write them to a bam file:\n\n```bash\nbwa mem -R \"@RG\\tID:readgroup_<sample>\\tPU:lanex_flowcellx\\tSM:<sample>\\tLB:libraryx\\tPL:illumina\" \\\n-t 1 human_g1k_v37_chr2.fasta sample_1.fq sample_2.fq | samtools sort > sample.bam\n```\n\nWhere `sample` should be replaced with the real samples name, i.e. HG00097, HG00100 and HG00101.\nPlease check that the expected output files were generated and have content using `ls -lrt`.\nYou also need to index each output bam file:\n\n```bash\nsamtools index <sample>.bam\n```\n\nPlease check what output file was generated this time.\nIf you run out of time you can click below to get paths to precomputed bam files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(bamdir,'/HG00097.bam\\n'))\ncat(paste0(bamdir,'/HG00100.bam\\n'))\ncat(paste0(bamdir,'/HG00101.bam\\n'))\n```\n\n## Generate g.vcf files {#generategvcf}\n\n`HaplotypeCaller` should also be run for all three samples, but this time the output for each sample needs to be in g.vcf format. This is accomplished with a small change in the `HaploteypCaller` command.<br/>\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen:\n\n```bash\ngatk --java-options -Xmx7g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-ERC GVCF \\\n-I sample.bam \\\n-O sample.g.vcf\n```\n\nPlease replace `sample` with the real sample names.\n\nIf you run out of time you can click below to get paths to the precomputed g.vcf files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(vcfdir,'/HG00097.g.vcf\\n'))\ncat(paste0(vcfdir,'/HG00100.g.vcf\\n'))\ncat(paste0(vcfdir,'/HG00101.g.vcf\\n'))\n```\n\n## Joint genotyping {#jointgenotyping}\n\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. To do this you first need to combine all individual .g.vcf files to one file using `CombineGVCFs`.<br/>\n\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen:\n\n```bash\ngatk --java-options -Xmx7g CombineGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V sample1.g.vcf \\\n-V sample2.g.vcf \\\n-V sample3.g.vcf \\\n-O cohort.g.vcf\n```\n\nPlease replace `sample1`, `sample2`, `sample3` with the real sample names.\nThen run GATK's `GenoteypeGVC` to generate a vcf file:\n\n```bash\ngatk --java-options -Xmx7g GenotypeGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.g.vcf \\\n-O cohort.vcf\n```\n\nIf you run out of time you can click below to get paths to the precomputed cohort.g.vcf and cohort.vcf files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(vcfdir,'/cohort.g.vcf\\n'))\ncat(paste0(vcfdir,'/cohort.vcf\\n'))\n```\n\n#### Questions {-}\n\n15. How many data lines do the cohort.g.vcf file have? You can use the Linux command `grep -v \"#\" cohort.g.vcf` to extract all lines in \"cohort.g.vcf\" that don't start with \"#\", then `|`, and then `wc -l` to count those lines.\n16. How many data lines do the cohort.vcf file have?\n17. Explain the difference in number of data lines.\n18. Look at the header line of the cohort.vcf file. What columns does it have?\n19. What is encoded in the last three columns of the data lines?\n\n## Check combined vcf file in IGV\n\nAgain we assume that you have logged in to UPPMAX via ThinLinc.\\\nIf you have closed IGV please open it again as described above.\\\nLoad the files cohort.vcf, HG00097.bam, HG00100.bam and HG00101.bam into IGV as described earlier.\\\nThis time lets look closer at the variant rs4988235, located at position chr2:136608646 in the HG19 reference genome. This is the variant that has been shown to lead to lactase persistence.\\\nPlease use IGV to answer the questions below.\n\n#### Questions {-}\n\n20. What is the reference and alternative alleles at chr2:136608646?\n21. What genotype do the three samples have at chr2:136608646? Note how genotypes are color coded in IGV.\n22. Should any of the individuals avoid drinking milk?\n23. Now compare the data shown in IGV with the data in the VCF file. Extract the row for the chr2:136608646 variant in the cohort.vcf file, for example using `grep '136608646' cohort.vcf`. What columns of the vcf file contain the information shown in the upper part of the vcf track in IGV?\n24. What columns of the vcf file contain the information shown in the lower part of the vcf track?\n25. Zoom out so that you can see the *MCM6* and *LCT* genes. Is the variant at chr2:136608646 locate within the LCT gene?\n\nIf you are interested in how this variant affects lactose tolerance please read the article by Mattar et al presented [above](#Genomicregion), or in [OMIM](https://www.omim.org/entry/601806#0001).\n\n# GATK's best practices {#gatk_bp}\n\nThe third part of this workshop will take you through additional refinement steps that are recommended in [GATKs best practices for germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-), illustrated in the flowchart below.\nThe additional steps in the best practice workflow was not covered in the variant-calling lecture on Wednesday afternoon. There will be a short lecture about this on Thursday morning at 9 am. However, if you reach this step earlier you can have a look at this [prerecorded video of the same lecture](https://youtu.be/b7VV6e5q6ss).  \n\n![](assets/3_best_practise.png){width=80%}\n\n## BWA mem\n\nThe first step in GATK's best pracice variant calling workflow is to run BWA mem for each sample exactly as you did in [Variant calling in cohort](#bwa_joint). You have already done this step, so please use the bam files that you generated in step 2.1 for the steps below.\n\n## Mark Duplicates\n\nSometimes the same DNA fragment is sequenced multiple times, which leads to multiple reads from the same fragment in the fastq file. This can occur due to PCR amplification in the library preparation, or if one read cluster is incorrectly detected as multiple clusters by the sequencing instrument.\nIf a duplicated read contains a genetic variant, the ratio of the two alleles might be obscured, which can lead to incorrect genotyping. It is therefore recommended (in most cases) to mark duplicate reads so that they are counted as one during genotyping.\n\nPlease read about Picard's `MarkDuplicates` [here](https://gatk.broadinstitute.org/hc/en-us/articles/360037225972-MarkDuplicates-Picard-). Picard's MarkDuplicates has recently been incorporated into the GATK suite, but the usage example in GATKs documentation describes how to call it via the stand alone Picard program. To learn how to use it as part of the GATK module, please call MarkDuplicates without input parameters like this:\n\n```bash\ngatk --java-options -Xmx7g MarkDuplicates\n```\n\nPlease run MarkDuplicates on all three bam files generated in step 2.1. Here is the code for running MarkDuplicates on the sample HG00097:\n\n```bash\ngatk --java-options -Xmx7g MarkDuplicates \\\n    -I HG00097.bam \\\n    -O HG00097.md.bam \\\n    -M HG00097_mdmetrics.txt\n```\n\n## Recalibrate Base Quality Scores\n\nAnother source of error is systematic biases in the assignment of base quality scores by the sequencing instrument. This can be corrected by GATK's [Base Quality Score Recalibration](https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR-).\nIn short, you first use [BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/articles/360037593511-BaseRecalibrator) to build a recalibration model, and then [ApplyBQSR](https://gatk.broadinstitute.org/hc/en-us/articles/360037225212-ApplyBQSR) to recalibrate the base qualities in your bam file.  \n`BaseRecalibrator` requires a file with known SNPs as input. This file is available in the data folder on UPPMAX:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(refdir,'/1000G_phase1.snps.high_confidence.b37.chr2.vcf'))\n```\n\nPlease recalibrate the base quality scores in all the bam files generated in the previous step. Below is our example solution for sample HG00097.\nFirst run BaseRecalibrator:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('gatk --java-options -Xmx7g BaseRecalibrator \\\\\n    -R human_g1k_v37_chr2.fasta \\\\\n    -I HG00097.md.bam \\\\\n    --known-sites ',refdir,'/1000G_phase1.snps.high_confidence.b37.chr2.vcf \\\\\n    -O HG00097.recal.table'))\n```\n\nThen run ApplyBQSR:\n\n```bash\ngatk --java-options -Xmx7g ApplyBQSR \\\n    -R human_g1k_v37_chr2.fasta \\\n    -I HG00097.md.bam \\\n    --bqsr-recal-file HG00097.recal.table \\\n    -O   HG00097.recal.bam\n```\n\n## Generate g.vcf files\n\n`HaplotypeCaller` should also be run for all three samples, and the output should be in g.vcf exactly as described [above](#generategvcf). This time use recalibrated bam files as input.\n\n## Joint genotyping\n\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. This should be done with the commands `CombineGVCFs` and `GenotypeGVCFs` exactly as described  [above](#jointgenotyping), but you should use the g.vcf files generated from the recalibrated bam files as input.\n\n## Variant Filtering\n\nHaplotypeCaller is designed to be very sensitive, which is good because it minimizes the chance of missing real variants. However, it means that the number of false positives can be quite large, so we need to filter the raw callset. GATK offers two ways to filter variants:  \n\n1. The variant quality score recalibration (VQSR) method uses machine learning to identify variants that are likely to be real. This is the best method if you have a lot of data, for example one whole genome sequence sample or several whole exome samples.  \n2. If you have less data you can use hard filters as described [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531112?id=2806#2). \n\nSince we have very little data we will use hard filters. The parameters are slightly different for SNVs and INDELs, so you need to first select all SNVs using [SelectVariants](https://gatk.broadinstitute.org/hc/en-us/articles/360037225432-SelectVariants) and filter them using [VariantFiltration](https://gatk.broadinstitute.org/hc/en-us/articles/360037226192-VariantFiltration) with the parameters suggested for SNVs. Then select all INDELs and filter them with the parameters suggested for INDELs. Finally merge the SNVs and INDELs to get all variants in one file using [MergeVCFs](https://gatk.broadinstitute.org/hc/en-us/articles/360037226612-MergeVcfs-Picard-).  \nAn explanation of what the hard filters do can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants).\n\nExample solution for filtering SNVs:\n\n```bash\ngatk --java-options -Xmx7g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include SNP \\\n  -O cohort.snvs.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.snvs.vcf \\\n  -O cohort.snvs.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD < 2.0\"  \\\n  --filter-name MQfilter --filter-expression \"MQ < 40.0\"  \\\n  --filter-name FSfilter --filter-expression \"FS > 60.0\"\n```\n\nExample solution for filtering INDELs:\n\n```bash\ngatk --java-options -Xmx7g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include INDEL \\\n  -O cohort.indels.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.indels.vcf \\\n  -O cohort.indels.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD < 2.0\" \\\n  --filter-name FSfilter --filter-expression \"FS > 200.0\"\n```\n\nExample solution for merging filtered SNVs and INDELs:\n\n```bash\ngatk --java-options -Xmx7g MergeVcfs \\\n    -I cohort.snvs.filtered.vcf \\\n    -I cohort.indels.filtered.vcf \\\n    -O cohort.filtered.vcf\n```\n\nOpen your filtered vcf with `less` and page through it. It still has all the variant lines, but the FILTER column that was blank before is now filled in, with PASS or a list of the filters it failed. Note also that the filters that were run are described in the header section.\n\n#### Precomputed files {-}\n\nIf you run out of time, please click below to get the path to precomputed bam and vcf files for the GATKâ€™s best practices section.\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('Path to intermediary and final bam files: ',bpbamdir,'\\n'))\ncat(paste0('Path to intermediary and final vcf files: ',bpvcfdir,'\\n'))\n```\n\n#### Questions {-}\n\n26. Check how  many variants in total that are present in the cohort.filtered.vcf file and how many that have passed the filters. Is the difference big?\n27. Look at the variants that did not pass the filters using `grep -v 'PASS' cohort.filtered.vcf`. Try to understand why these variants didn't pass the filter.\n\n# Clean up {-}\n\nWhen the analysis is done and you are sure that you have the desired output, it is a good practice to remove intermediary files that are no longer needed. This will save disk space, and will be a crucial part of the routines when you work with your own data. Please think about which files you need to keep if you would like to go back and look at this lab later on. Remove the other files.\n\n# Answers {-}\n\nWhen you have finished the exercise, please have a look at this document with [answers to all questions](lab_vc_answers.pdf), and compare them with your answers.\n\n# SBATCH scripts {-}\n\nThis section is supplementary material intended only for those of you who want to learn how to run all steps automatically in bash scripts. Please make sure that you have understood how all the individual steps work before you start with this.\nTo learn more about SLURM and SBATCH scripts please look the [SLURM user guide](https://www.uppmax.uu.se/support/user-guides/slurm-user-guide/) on UPPMAX website.\n\n## Variant calling in cohort {-}\n\nBelow is a skeleton script that can be used as a template for running [variant calling in a cohort](#jointvc). Please modify it to run all the steps in part two of this workshop.\n\n```bash\n#!/bin/bash\n#SBATCH -A sens2022-22-123\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 1:00:00\n#SBATCH -J jointGenotyping\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  #Fill in the code for running bwa-mem for each sample here\n  #Fill in the code for samtools index for each sample here\n  #Fill in the code for HaplotypeCaller for each sample here\ndone\n#Fill in the code for CombineGVCFs for all samples here\n#Fill in the code for GenotypeGVCFs here\n\n```\nPlease save the sbatch script in your UPPMAX folder and call it \"joint_genotyping.sbatch\" or similar. Make the script executable by this command:\n\n```bash\nchmod u+x joint_genotyping.sbatch\n```\n\nTo run the sbatch script in the SLURM queue, use this command:\n\n```bash\nsbatch joint_genotyping.sbatch\n```\n\nIf you have an active node reservation you can run the script as a normal bash script:\n\n```bash\n./joint_genotyping.sbatch\n```\n\nIf you would like more help with creating the sbatch script, please look at our example solution:\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('#!/bin/bash\n#SBATCH -A ',upid,'\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 2:00:00\n#SBATCH -J jointGenotyping\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  bwa mem -R \\\\\n  \"@RG\\\\tID:${sample}\\\\tPU:flowcellx_lanex\\\\tSM:${sample}\\\\tLB:libraryx\\\\tPL:illumina\" \\\\\n  -t 1 human_g1k_v37_chr2.fasta \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort > \"${sample}.bam\"\n\n  samtools index \"${sample}.bam\"\n\n  gatk --java-options -Xmx7g HaplotypeCaller \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -ERC GVCF -I \"${sample}.bam\" \\\\\n  -O \"${sample}.g.vcf\"\n\ndone\n\ngatk --java-options -Xmx7g CombineGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V HG00097.g.vcf \\\\\n-V HG00100.g.vcf \\\\\n-V HG00101.g.vcf \\\\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx7g GenotypeGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.g.vcf \\\\\n-O cohort.vcf'))\n```\n\n## GATK best practices {-}\n\nNow please try to incorporate the additional steps from [GATK's best practices](#gatk_bp) into the workflow. If you run out of time you can sneak peek at our example solution below.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('#!/bin/bash\n#SBATCH -A ',upid,'\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 2:00:00\n#SBATCH -J BestPractise\n\n## load modules\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\n# define path to reference genome\nref=\"/sw/courses/ngsintro/reseq/data/ref\"\n\n# make symbolic links\nln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_2.fq\n\n# index reference genome\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\nsamtools faidx human_g1k_v37_chr2.fasta\ngatk --java-options -Xmx7g CreateSequenceDictionary \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-O human_g1k_v37_chr2.dict\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: ${sample}\"\n  # map the reads\n  bwa mem \\\\\n  -R \"@RG\\tID:${sample}\\tPU:flowcellx_lanex\\tSM:${sample}\\tLB:libraryx\\tPL:illumina\" \\\\\n  -t 1 human_g1k_v37_chr2.fasta \\\\\n  \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort > \"${sample}.bam\"\n\n  samtools index $sample\".bam\"\n  # mark duplicates\n  gatk --java-options -Xmx7g MarkDuplicates \\\\\n  -I \"${sample}.bam\" \\\\\n  -O \"${sample}.md.bam\" \\\\\n  -M \"${sample}_mdmetrics.txt\"\n\n  # base quality score recalibration\n  gatk --java-options -Xmx7g BaseRecalibrator \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -I \"${sample}.md.bam\" \\\\\n  --known-sites \"${ref}/1000G_phase1.snps.high_confidence.b37.chr2.vcf\" \\\\\n  -O \"${sample}.recal.table\"\n\n  gatk --java-options -Xmx7g ApplyBQSR \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -I \"${sample}.md.bam\" \\\\\n  --bqsr-recal-file \"${sample}.recal.table\" \\\\\n  -O \"${sample}.recal.bam\"\n\n  # haplotypeCaller in -ERC mode\n  gatk --java-options -Xmx7g HaplotypeCaller \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -ERC GVCF \\\\\n  -I \"${sample}.bam\" \\\\\n  -O \"${sample}.g.vcf\"\ndone\n\n# joint genotyping\ngatk --java-options -Xmx7g CombineGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V HG00097.g.vcf \\\\\n-V HG00100.g.vcf \\\\\n-V HG00101.g.vcf \\\\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx7g GenotypeGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.g.vcf \\\\\n-O cohort.vcf\n\n# variant filtration SNPs\ngatk --java-options -Xmx7g SelectVariants \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.vcf \\\\\n--select-type-to-include SNP \\\\\n-O cohort.snvs.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.snvs.vcf \\\\\n--filter-name QDfilter --filter-expression \"QD < 2.0\" \\\\\n--filter-name MQfilter --filter-expression \"MQ < 40.0\" \\\\\n--filter-name FSfilter --filter-expression \"FS > 60.0\" \\\\\n-O cohort.snvs.filtered.vcf\n\n# variant filtration indels\ngatk --java-options -Xmx7g SelectVariants \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.vcf \\\\\n--select-type-to-include INDEL \\\\\n-O cohort.indels.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.indels.vcf \\\\\n--filter-name QDfilter --filter-expression \"QD < 2.0\" \\\\\n--filter-name FSfilter --filter-expression \"FS > 200.0\" \\\\\n-O cohort.indels.filtered.vcf\n\n# merge filtered SNPs and indels\ngatk --java-options -Xmx7g MergeVcfs \\\\\n-I cohort.snvs.filtered.vcf \\\\\n-I cohort.indels.filtered.vcf \\\\\n-O cohort.filtered.vcf'))\n```\n\n# Additional information {-}\n\n* Here is a technical documentation of [Illumina Quality Scores](https://www.illumina.com/documents/products/technotes/technote_understanding_quality_scores.pdf)\n* Tools used or referenced\n  * [BWA](http://bio-bwa.sourceforge.net/bwa.shtml)\n  * [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n  * [MultiQC](http://multiqc.info/)\n  * [Picard](https://broadinstitute.github.io/picard/command-line-overview.html)\n  * [GATK](https://software.broadinstitute.org/gatk/)\n  * [samtools](http://www.htslib.org/)\n\n<!--# Alternative cluster\nThe teachers may inform you that we will use the high performance computing center NSC instead of UPPMAX during this workshop. If so, please follow this link to [run the workshop on NSC](lab_vc_tetralith.html).-->\n\n\n**Thanks for today!**\n","srcMarkdownNoYaml":"\n\n```{r,eval=TRUE,include=FALSE}\nlibrary(yaml)\nlibrary(here)\n\nupid <- yaml::read_yaml(here(\"_quarto.yml\"))$uppmax_project\n```\n\n```{css,include=FALSE}\n.workLocally{\nbackground-color: red;\n}\n```\n\n```{r,include=FALSE}\n## VARIABLES\ndatadir <- \"/sw/courses/ngsintro/reseq/data\"\nfastqdir <- \"/sw/courses/ngsintro/reseq/data/fastq\"\nrefdir <- \"/sw/courses/ngsintro/reseq/data/ref\"\nbamdir <- \"/sw/courses/ngsintro/reseq/data/bam\"\nvcfdir <- \"/sw/courses/ngsintro/reseq/data/vcf\"\nbpbamdir <- \"/sw/courses/ngsintro/reseq/data/best_practise_bam\"\nbpvcfdir <- \"/sw/courses/ngsintro/reseq/data/best_practise_vcf\"\ncol_uppmax <- \"#f4f8e8\"\ncol_local <- \"#e5f4f8\"\n```\n\n# Introduction {-}\n\nWhole genome sequencing (WGS) is a comprehensive method for analyzing entire genomes. This workshop will take you through the process of calling germline short variants (SNVs and INDELs) in WGS data from three human samples.\n\n1. The first part of the workshop will guide you through a basic variant calling workflow in one sample. The goals are that you should get familiar with the bam and vcf file formats, and be able to interpret vcf files in Integrative Genomics Viewer (IGV).\n2. If you have time, the next part of the workshop will show you how to perform joint variant calling in three samples. The goals here are that you should be able to interpret multi-sample vcf files and explain the differences between the g.vcf and vcf file formats.\n3. If you have time, the last part of the workshop will take you through the GATK best practices for germline short variant detection in three samples. The goal here is that you should learn how to use GATK's documentation so that you can analyze your own samples in the future.\n\n::: {.callout-note}\n\n## General guide\n\n* You will work on the computing cluster Rackham at UPPMAX\n* If you change the node you are working on you will need to reload the tool modules.\n* Please type commands in the terminal instead of copying and pasting them which often result in formatting errors.\n* Use tab completion.\n* In paths, please replace `username` with your actual UPPMAX username.\n* In commands, please replace `parameter` with the correct parameter, for example your input file name, output file name, directory name, etc.\n* A line starting with `#` is a comment\n* Running a command without parameters will often return a help message on how to run the command.\n* After a command is completed, please check that the desired output file was generated and that it has a reasonable size (use `ls -l`).\n* Google errors, someone in the world has run into EXACTLY the same problem you had and asked about it on a forum somewhere.\n:::\n\n# Data description {-}\n## Samples  {-}\n\nThe 1000 Genomes Project ran between 2008 and 2015, creating the largest public catalogue of human variation and genotype data. In this workshop we will use low coverage whole genome sequence data from three individuals, generated in the first phase of the 1000 Genomes Project.\n\nSample        | Population | Sequencing technology\n------------- | ---------- | --------\nHG00097       | British in England and Scotland    | Low coverage WGS\nHG00100       | British in England and Scotland    | Low coverage WGS\nHG00101       | British in England and Scotland    | Low coverage WGS\n\n## Genomic region {#Genomicregion -}  \n\nThe *LCT* gene on chromosome 2 encodes the enzyme lactase, which is responsible for the metabolism of lactose in mammals.\nMost mammals can not digest lactose as adults, but some humans can.\nGenetic variants upstream of the *LCT* gene causes lactase persistence, which means that lactase is expressed also in adulthood and the carrier can continue to digest lactose.\nThe variant rs4988235, located at position chr2:136608646 in the GRCh37 reference genome, has been shown to lead to lactose persistence.\nThe alternative allele (A on the forward strand and T on the reverse strand) creates a new transcription factor binding site that enables continued expression of the gene after weaning.\n\nIn this workshop we will detect genetic variants in the region chr2:136545000-136617000 in the three samples listed above, and check if they carry the allele for lactase persistence.\n\nFor those interested in the details of the genetic bases for lactose tolerance, please read the first three pages of [Lactose intolerance: diagnosis, genetic, and clinical factors](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3401057/pdf/ceg-5-113.pdf) by Mattar et al. The variant rs4988235 is here referred to as LCT-13910C>T.  \n\n## Data folder on UPPMAX {#Data -}  \n\nAll input data for this exercise is located in this folder on Rackham:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(datadir))\n```\n\nThe fastq files are located in this folder:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(fastqdir))\n```\n\nReference files, such as the reference genome in fasta format, are located in this folder:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(refdir))\n```\n\n# Preparations {-}\n\n## Laptop {#preparelaptop -}\nThis lab will be done completely on UPPMAX and the instructions assume that you connect via ThinLinc.\nHowever, if you prefer to connect to UPPMAX via ssh you can instead copy some of the resulting files to your laptop and work on them there. , install IGV, and run all the IGV steps on your laptop.\nIf so, please create a local workspace on your laptop, for example a folder called *ngsworkflow* on your desktop. You need to have write permission in this folder.\nIf you connect to UPPMAX via ThinLinc you don't have to crete a local workspace.\n\n## UPPMAX {-}\n### Connect to UPPMAX {-}\n\nDuring this lab it is best to connect to UPPMAX with ThinLinc, which gives you a graphical remote desktop. Instructions for this is available at [**Connecting to UPPMAX**](topics/other/lab_connect.html). Please follow the instructions in section 1.2 Remote desktop connection.\n\n### Logon to a node {-}\n\nThis lab should be done on a compute node (not the login node). First check if you already have an active job allocation using this command:\n\n```bash\nsqueue -u username\n```\n\nWhere `username` should be replaced with your username.<br/>\nIf no jobs are listed you should allocate a job for this lab. If you already have an active job allocation please proceed to **Connect to the node** below.<br/>\n\nUse this code to allocate a job on day 1 of variant-calling:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('salloc -A ',upid,' -t 04:00:00 -p core -n 1 --no-shell'))\n```\n\nUse this code to allocate a job on day 2 of variant-calling:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('salloc -A ',upid,' -t 04:00:00 -p core -n 1 --no-shell'))\n```\n\nOnce your job allocation has been granted (should not take long) please check the allocation again using:\n\n```bash\nsqueue -u username\n```\n\nYou should now see that you have an active job allocation. The node name for your job is listed under the nodelist header.<br/>\n\n**Connect to the node:**\n```bash\nssh -Y nodename\n```\n\n### Workspace on UPPMAX {-}\n\nYou should work in your folder under the courseâ€™s nobackup folder, just like you have done during the previous labs.\n\nStart by going there using this command:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('cd /proj/',upid,'/nobackup/username'))\n```\n\nWhere `username` should be replaced with your real username.\nCreate a folder for this exercise and move into it:\n\n```bash\nmkdir ngsworkflow\ncd ngsworkflow\n```\n\nMake sure you are located in\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nfor the rest of this lab.\n\n### Symbolic links to data {-}\n\nThe raw data files are located in the [Data](#Data) folder described above.   \nCreate a symbolic link to the reference genome (in this case chromosome 2 in GRCh37) in your workspace:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('ln -s ',refdir,'/human_g1k_v37_chr2.fasta'))  \n```\n\nDo the same with the fastq files:  \n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('ln -s ',fastqdir,'/HG00097_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00097_2.fq\\n'))  \ncat(paste0('ln -s ',fastqdir,'/HG00100_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00100_2.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00101_1.fq\\n'))\ncat(paste0('ln -s ',fastqdir,'/HG00101_2.fq\\n'))\n```\n\n### Accessing programs {#Accessingprograms -}\n\nLoad the modules that are needed during this workshop. Remember that these modules must be loaded every time you login to Rackham, or when you connect to a new compute node.  \nFirst load the bioinfo-tools module:\n\n```bash\nmodule load bioinfo-tools\n```\n\nThis makes it possible to load the individual programs:\n\n```bash\nmodule load FastQC/0.11.8\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n```\n\nAlthough you don't have to specify which versions of the tools to use, it is recommended to do so for reproducibility if you want to rerun the  exact same analyses later.\nWhen loading the module GATK/4.1.4.1 you will get a warning message about the fact that GATK commands have been updated since the previous version of GATK. This is fine and you don't have to do anything about it.\n\n## Index the genome {-}\n\nTools that compare short reads with a large reference genome needs indexes of the reference genome to work efficiently. You therefore need to create index files for each tool.<br/>\n\nFirst check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nGenerate BWA index files:\n\n```bash\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\n```\n\nCheck that several new files have been created using `ls -l`. Then Generate a samtools index:\n\n```bash\nsamtools faidx human_g1k_v37_chr2.fasta\n```\n\nCheck to see what file(s) were created using `ls -lrt`. Then Generate a GATK sequence dictionary:\n\n```bash\ngatk --java-options -Xmx7g CreateSequenceDictionary -R human_g1k_v37_chr2.fasta -O human_g1k_v37_chr2.dict\n```\n\nAgain, check what file(s) were created using `ls -lrt`.\n\n# Variant calling in one sample\n\nNow let's start the main part of the workshop, which is variant calling in one sample. The workflow consists of aligning the reads with [BWA](http://bio-bwa.sourceforge.net) and detecting variants with [HaplotypeCaller](https://gatk.broadinstitute.org/hc/en-us/articles/360037225632-HaplotypeCaller) as illustrated below.\n\n![](assets/1_onesamplevc.png)\n\n## Aligning reads\n\n### BWA mem {#bwamem}\n\nYou should use `BWA mem` to align the reads to the reference genome.  \n\nIn the call to `BWA mem` you need to add something called a *read group*, which contains information about how the reads were generated.\nThis is required by HaplotypeCaller. Since we don't know exactly how the reads in the 1000 Genomes Project were generated we will\nassume that each pair of fastq files was generated from one library preparation (*libraryx*), derived from one biological sample (*HG00097*), and run on one lane (*lanex*) of a flowcell (*flowcellx*) on the Illumina machine, and define a toy read group with this information.\nThe code for adding this read group is<br/>\n\n**-R \"@RG\\\\tID:HG00097\\\\tPU:lanex_flowcellx\\\\tSM:HG00097\\\\tLB:libraryx\\\\tPL:illumina\"**.\n\n::: {.callout-note}\nWhen running BWA for another sample later on you have to replace *HG00097* in the read group with the new sample name. To learn more about read groups  please read [this article](https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups) at GATK forum.\n:::\n\nYou also need to specify how many threads the program should use (should be the same as the number of cores you have access to and is defined by the code `-t 1` below) and what reference genome file to use. The output from `BWA` should be parsed to `samtools sort`, which sorts the sam file according to chromosome position and then converts the sam file to the binary bam format. Finally, use a file redirect `>` so that the output ends up in a file and not on your screen.\n\nFirst make sure that you are standing in the workspace you created on UPPMAX for this lab:\n\n```bash\npwd\n```\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen use this command to align the reads, add the read group, sort the reads and write them to a bam file:\n\n```bash\nbwa mem \\\n-R \"@RG\\tID:readgroup_HG00097\\tPU:lanex_flowcellx\\tSM:HG00097\\tLB:libraryx\\tPL:illumina\" \\\n-t 1 human_g1k_v37_chr2.fasta HG00097_1.fq HG00097_2.fq | samtools sort > HG00097.bam\n```\n\nPlease check that the expected output file was generated and that it has content using `ls -lrt`.\n\nNext you need to index the generated bam file so that programs can randomly access the sorted data without reading the whole file. This command creates an index file with the same name as the input bam file, except with a .bai extension:\n\n```bash\nsamtools index HG00097.bam\n```\nPlease check what output file was generated this time.\n\n### Check bam with samtools\n\nThe bam file is binary so we cannot read it directly, but we can view it with the program `samtools view`. The header section of the bam file can be viewed separately with the `-H` flag:\n\n```bash\nsamtools view -H HG00097.bam\n```\n\n#### Question {-}\n\n1. The @SQ tag of the bam header contains information about reference sequence. What do you think SN:2 and LN:243199373 in this tag means?\n\nThe aligned reads can be viewed with `samtools view` without the `-H`. This will display the entire bam file which is quite large, so if you just want to look at the first 5 lines (for example) you can combine `samtools view` with `head`:\n\n```bash\nsamtools view HG00097.bam | head -n 5\n```\n\n#### Question {-}\n\n2. What is the leftmost mapping position of the first read in the bamfile?\n\nPlease have a look at the [Sequence Alignment/Map Format Specification](https://samtools.github.io/hts-specs/SAMv1.pdf) for more information about bam files.\n\n### Check bam in IGV\n\nTo use IGV on UPPMAX we recommend that you are connected via ThinLinc.\nAlternatively you can install IGV on your local computer, download the files using scp, and look at them locally.\nThe instructions below assume that you have logged in to UPPMAX via ThinLinc.\\\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nTo start IGV please type this in the terminal:\n\n```bash\nmodule load IGV/2.8.13\nigv.sh &\n```\n\nIn IGV:\\\nIn the upper left dropdown menu choose `Human hg19` (which is the same as GRCh37).\\\nIn the `File` menu, select `Load from File` and select `HG00097.bam`, which should then appear in the tracks window.\\\nZoom in to see the reads. You can either select a region by click and drag, or by typing a region or a gene name in the text box at the top. Remember that we have data for the region chr2:136545000-136617000.\\\nIGV can be closed by selecting exit in the `File` menu or by clicking `x` in the upper right corner of the IGV window, but you can keep it open for the rest of the lab.\n\n#### Questions {-}\n\n3. What is the read length?\n4. Approximately how many reads cover an arbitrary position in the genomic region we are looking at?\n5. Which RefSeq Genes are located within the region chr2:136545000-136617000?\n\n## Variant Calling\n\n### HaplotypeCaller {#haplotypecaller}\n\nNow we will detect short variants in the bam file using GATK's `HaplotypeCaller`. First use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen run:\n\n```bash\ngatk --java-options -Xmx7g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-I HG00097.bam \\\n-O HG00097.vcf\n```\n\nCheck what new files were generated with `ls -lrt`.  \n\n### Explore the vcf file\n\nNow you have your first vcf file containing the raw variants in the region chr2:136545000-136617000 in sample HG00097. Please look at the vcf file with `less` and try to understand its structure.  \n\nVcf files contains meta-information lines starting with *##*, a header line starting with *#CHROM*, and then data lines each containing information about one variant position in the genome. The header line defines the columns of the data lines, and to view the header line you can type this command:  \n\n```bash\ngrep '#CHROM' HG00097.vcf\n```\n\n#### Question {-}\n\n6.  What column of the VCF file contains genotype information for the sample HG00097?\n\nThe meta-information lines starting with *##INFO* defines how the data in the *INFO* column is encoded,\nand the meta-information lines starting with *##FORMAT* defines how the data in the *FORMAT* column is encoded.\\\nTo view the meta-information lines describing the *INFO* column use:\n\n```bash\ngrep '##INFO' HG00097.vcf\n```\n\nTo view the meta-information lines describing the *FORMAT* column use:\n\n```bash\ngrep '##FORMAT' HG00097.vcf\n```\n\n#### Question {-}\n\n7. What does *GT* in the *FORMAT* column of the data lines mean?\n8. What does *AD* in the *FORMAT* column of the data lines mean?\n\nTo look at the details of one specific genetic variant at position 2:136545844 use:\n\n```bash\ngrep '136545844' HG00097.vcf\n```\n\n#### Questions {-}\n\n9. What genotype does the sample HG00097 have at position 2:136545844?\n10. What are the allelic depths for the reference and alternative alles in sample HG00097 at position 2:136545844?\n\nThe following command can be used to count the data lines (i.e. number of lines that don't start with \"#\") in the vcf file:\n\n```bash\ngrep -v \"#\" HG00097.vcf | wc -l\n```\n\n#### Question {-}\n\n11. How many genetic variants was detected in HG00097?\n\nFor more detailed information about vcf files please have a look at [The Variant Call Format specification](https://samtools.github.io/hts-specs/VCFv4.2.pdf).\n\n### Check vcf in IGV {#vcfinigv}\n\nWe assume that you have logged in to UPPMAX via ThinLinc.\\\nIf you have closed IGV please open it again as described above.\\\nLoad the file HG00097.vcf into tracks window of IGV as you did with the HG00097.bam file earlier (load the bam file as well if it is not already loaded). You will now see all the variants called in HG00097.\\\nYou can view variants in the *LCT* gene by typing the gene name in the search box, and you can look specifically at the variant at position chr2:136545844 by typing that position in the search box.\\\nPlease use IGV to answer the questions below.\n\n#### Questions {-}\n\n12. Hover the mouse over the upper row of the vcf track. What is the reference and alternative alleles of the variant at position chr2:136545844?\n13. Hover the mouse over the lower row of the vcf track and look under \"Genotype Information\". What genotype does HG00097 have at position chr2:136545844? Is this the same as you found by looking directly in the vcf file in question 10?\n14. Look in the bam track and count the number of reads that have \"G\" and \"C\", respectively,  at position chr2:136545844. How is this information captured under \"Genotype Attributes\"? (Again, hoover the mouse over the lower row of the vcf track.)\n\n# Variant calling in cohort {#jointvc}\n\nIf you have time, you can now try joint variant calling in all three samples. Each sample has to be processed with `BWA mem` as above, and then with `HaplotypeCaller` with the flag `-ERC` to generate one g.vcf file per sample. The individual g.vcf files should subsequently be combined with GATK's `CombineGVCFs`, and translated into vcf format with GATK's `GenotypeGVCFs`. The workflow below shows how the three samples should be processed and combined.  \n\n![](assets/2_jointvc.png)\n\nIf you don't have time to complete all steps we have made precomputed intermediary file available. Please see links under each analysis step.\n\n## BWA mem {#bwa_joint}\n\nRun `BWA mem` for all three samples in the data set. `BWA mem` should be run exactly as above, but with the new sample names.\nYou also need to adjust the read group information so that it matches each new sample name. <br/>\n\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen use this command for every sample to align the reads, add the read group, sort the reads and write them to a bam file:\n\n```bash\nbwa mem -R \"@RG\\tID:readgroup_<sample>\\tPU:lanex_flowcellx\\tSM:<sample>\\tLB:libraryx\\tPL:illumina\" \\\n-t 1 human_g1k_v37_chr2.fasta sample_1.fq sample_2.fq | samtools sort > sample.bam\n```\n\nWhere `sample` should be replaced with the real samples name, i.e. HG00097, HG00100 and HG00101.\nPlease check that the expected output files were generated and have content using `ls -lrt`.\nYou also need to index each output bam file:\n\n```bash\nsamtools index <sample>.bam\n```\n\nPlease check what output file was generated this time.\nIf you run out of time you can click below to get paths to precomputed bam files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(bamdir,'/HG00097.bam\\n'))\ncat(paste0(bamdir,'/HG00100.bam\\n'))\ncat(paste0(bamdir,'/HG00101.bam\\n'))\n```\n\n## Generate g.vcf files {#generategvcf}\n\n`HaplotypeCaller` should also be run for all three samples, but this time the output for each sample needs to be in g.vcf format. This is accomplished with a small change in the `HaploteypCaller` command.<br/>\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen:\n\n```bash\ngatk --java-options -Xmx7g HaplotypeCaller \\\n-R human_g1k_v37_chr2.fasta \\\n-ERC GVCF \\\n-I sample.bam \\\n-O sample.g.vcf\n```\n\nPlease replace `sample` with the real sample names.\n\nIf you run out of time you can click below to get paths to the precomputed g.vcf files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(vcfdir,'/HG00097.g.vcf\\n'))\ncat(paste0(vcfdir,'/HG00100.g.vcf\\n'))\ncat(paste0(vcfdir,'/HG00101.g.vcf\\n'))\n```\n\n## Joint genotyping {#jointgenotyping}\n\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. To do this you first need to combine all individual .g.vcf files to one file using `CombineGVCFs`.<br/>\n\nFirst use pwd to check if you are standing in the correct directory:\n\n```bash\npwd\n```\n\nShould return\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0('/proj/',upid,'/nobackup/username/ngsworkflow'))\n```\n\nThen:\n\n```bash\ngatk --java-options -Xmx7g CombineGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V sample1.g.vcf \\\n-V sample2.g.vcf \\\n-V sample3.g.vcf \\\n-O cohort.g.vcf\n```\n\nPlease replace `sample1`, `sample2`, `sample3` with the real sample names.\nThen run GATK's `GenoteypeGVC` to generate a vcf file:\n\n```bash\ngatk --java-options -Xmx7g GenotypeGVCFs \\\n-R human_g1k_v37_chr2.fasta \\\n-V cohort.g.vcf \\\n-O cohort.vcf\n```\n\nIf you run out of time you can click below to get paths to the precomputed cohort.g.vcf and cohort.vcf files.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0(vcfdir,'/cohort.g.vcf\\n'))\ncat(paste0(vcfdir,'/cohort.vcf\\n'))\n```\n\n#### Questions {-}\n\n15. How many data lines do the cohort.g.vcf file have? You can use the Linux command `grep -v \"#\" cohort.g.vcf` to extract all lines in \"cohort.g.vcf\" that don't start with \"#\", then `|`, and then `wc -l` to count those lines.\n16. How many data lines do the cohort.vcf file have?\n17. Explain the difference in number of data lines.\n18. Look at the header line of the cohort.vcf file. What columns does it have?\n19. What is encoded in the last three columns of the data lines?\n\n## Check combined vcf file in IGV\n\nAgain we assume that you have logged in to UPPMAX via ThinLinc.\\\nIf you have closed IGV please open it again as described above.\\\nLoad the files cohort.vcf, HG00097.bam, HG00100.bam and HG00101.bam into IGV as described earlier.\\\nThis time lets look closer at the variant rs4988235, located at position chr2:136608646 in the HG19 reference genome. This is the variant that has been shown to lead to lactase persistence.\\\nPlease use IGV to answer the questions below.\n\n#### Questions {-}\n\n20. What is the reference and alternative alleles at chr2:136608646?\n21. What genotype do the three samples have at chr2:136608646? Note how genotypes are color coded in IGV.\n22. Should any of the individuals avoid drinking milk?\n23. Now compare the data shown in IGV with the data in the VCF file. Extract the row for the chr2:136608646 variant in the cohort.vcf file, for example using `grep '136608646' cohort.vcf`. What columns of the vcf file contain the information shown in the upper part of the vcf track in IGV?\n24. What columns of the vcf file contain the information shown in the lower part of the vcf track?\n25. Zoom out so that you can see the *MCM6* and *LCT* genes. Is the variant at chr2:136608646 locate within the LCT gene?\n\nIf you are interested in how this variant affects lactose tolerance please read the article by Mattar et al presented [above](#Genomicregion), or in [OMIM](https://www.omim.org/entry/601806#0001).\n\n# GATK's best practices {#gatk_bp}\n\nThe third part of this workshop will take you through additional refinement steps that are recommended in [GATKs best practices for germline short variant discovery](https://gatk.broadinstitute.org/hc/en-us/articles/360035535932-Germline-short-variant-discovery-SNPs-Indels-), illustrated in the flowchart below.\nThe additional steps in the best practice workflow was not covered in the variant-calling lecture on Wednesday afternoon. There will be a short lecture about this on Thursday morning at 9 am. However, if you reach this step earlier you can have a look at this [prerecorded video of the same lecture](https://youtu.be/b7VV6e5q6ss).  \n\n![](assets/3_best_practise.png){width=80%}\n\n## BWA mem\n\nThe first step in GATK's best pracice variant calling workflow is to run BWA mem for each sample exactly as you did in [Variant calling in cohort](#bwa_joint). You have already done this step, so please use the bam files that you generated in step 2.1 for the steps below.\n\n## Mark Duplicates\n\nSometimes the same DNA fragment is sequenced multiple times, which leads to multiple reads from the same fragment in the fastq file. This can occur due to PCR amplification in the library preparation, or if one read cluster is incorrectly detected as multiple clusters by the sequencing instrument.\nIf a duplicated read contains a genetic variant, the ratio of the two alleles might be obscured, which can lead to incorrect genotyping. It is therefore recommended (in most cases) to mark duplicate reads so that they are counted as one during genotyping.\n\nPlease read about Picard's `MarkDuplicates` [here](https://gatk.broadinstitute.org/hc/en-us/articles/360037225972-MarkDuplicates-Picard-). Picard's MarkDuplicates has recently been incorporated into the GATK suite, but the usage example in GATKs documentation describes how to call it via the stand alone Picard program. To learn how to use it as part of the GATK module, please call MarkDuplicates without input parameters like this:\n\n```bash\ngatk --java-options -Xmx7g MarkDuplicates\n```\n\nPlease run MarkDuplicates on all three bam files generated in step 2.1. Here is the code for running MarkDuplicates on the sample HG00097:\n\n```bash\ngatk --java-options -Xmx7g MarkDuplicates \\\n    -I HG00097.bam \\\n    -O HG00097.md.bam \\\n    -M HG00097_mdmetrics.txt\n```\n\n## Recalibrate Base Quality Scores\n\nAnother source of error is systematic biases in the assignment of base quality scores by the sequencing instrument. This can be corrected by GATK's [Base Quality Score Recalibration](https://gatk.broadinstitute.org/hc/en-us/articles/360035890531-Base-Quality-Score-Recalibration-BQSR-).\nIn short, you first use [BaseRecalibrator](https://gatk.broadinstitute.org/hc/en-us/articles/360037593511-BaseRecalibrator) to build a recalibration model, and then [ApplyBQSR](https://gatk.broadinstitute.org/hc/en-us/articles/360037225212-ApplyBQSR) to recalibrate the base qualities in your bam file.  \n`BaseRecalibrator` requires a file with known SNPs as input. This file is available in the data folder on UPPMAX:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"bash\"}\ncat(paste0(refdir,'/1000G_phase1.snps.high_confidence.b37.chr2.vcf'))\n```\n\nPlease recalibrate the base quality scores in all the bam files generated in the previous step. Below is our example solution for sample HG00097.\nFirst run BaseRecalibrator:\n\n```{r,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('gatk --java-options -Xmx7g BaseRecalibrator \\\\\n    -R human_g1k_v37_chr2.fasta \\\\\n    -I HG00097.md.bam \\\\\n    --known-sites ',refdir,'/1000G_phase1.snps.high_confidence.b37.chr2.vcf \\\\\n    -O HG00097.recal.table'))\n```\n\nThen run ApplyBQSR:\n\n```bash\ngatk --java-options -Xmx7g ApplyBQSR \\\n    -R human_g1k_v37_chr2.fasta \\\n    -I HG00097.md.bam \\\n    --bqsr-recal-file HG00097.recal.table \\\n    -O   HG00097.recal.bam\n```\n\n## Generate g.vcf files\n\n`HaplotypeCaller` should also be run for all three samples, and the output should be in g.vcf exactly as described [above](#generategvcf). This time use recalibrated bam files as input.\n\n## Joint genotyping\n\nOnce you have the g.vcf files for all samples you should perform joint genotype calling. This should be done with the commands `CombineGVCFs` and `GenotypeGVCFs` exactly as described  [above](#jointgenotyping), but you should use the g.vcf files generated from the recalibrated bam files as input.\n\n## Variant Filtering\n\nHaplotypeCaller is designed to be very sensitive, which is good because it minimizes the chance of missing real variants. However, it means that the number of false positives can be quite large, so we need to filter the raw callset. GATK offers two ways to filter variants:  \n\n1. The variant quality score recalibration (VQSR) method uses machine learning to identify variants that are likely to be real. This is the best method if you have a lot of data, for example one whole genome sequence sample or several whole exome samples.  \n2. If you have less data you can use hard filters as described [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531112?id=2806#2). \n\nSince we have very little data we will use hard filters. The parameters are slightly different for SNVs and INDELs, so you need to first select all SNVs using [SelectVariants](https://gatk.broadinstitute.org/hc/en-us/articles/360037225432-SelectVariants) and filter them using [VariantFiltration](https://gatk.broadinstitute.org/hc/en-us/articles/360037226192-VariantFiltration) with the parameters suggested for SNVs. Then select all INDELs and filter them with the parameters suggested for INDELs. Finally merge the SNVs and INDELs to get all variants in one file using [MergeVCFs](https://gatk.broadinstitute.org/hc/en-us/articles/360037226612-MergeVcfs-Picard-).  \nAn explanation of what the hard filters do can be found [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035890471-Hard-filtering-germline-short-variants).\n\nExample solution for filtering SNVs:\n\n```bash\ngatk --java-options -Xmx7g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include SNP \\\n  -O cohort.snvs.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.snvs.vcf \\\n  -O cohort.snvs.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD < 2.0\"  \\\n  --filter-name MQfilter --filter-expression \"MQ < 40.0\"  \\\n  --filter-name FSfilter --filter-expression \"FS > 60.0\"\n```\n\nExample solution for filtering INDELs:\n\n```bash\ngatk --java-options -Xmx7g SelectVariants \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.vcf \\\n  --select-type-to-include INDEL \\\n  -O cohort.indels.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\n  -R human_g1k_v37_chr2.fasta \\\n  -V cohort.indels.vcf \\\n  -O cohort.indels.filtered.vcf \\\n  --filter-name QDfilter --filter-expression \"QD < 2.0\" \\\n  --filter-name FSfilter --filter-expression \"FS > 200.0\"\n```\n\nExample solution for merging filtered SNVs and INDELs:\n\n```bash\ngatk --java-options -Xmx7g MergeVcfs \\\n    -I cohort.snvs.filtered.vcf \\\n    -I cohort.indels.filtered.vcf \\\n    -O cohort.filtered.vcf\n```\n\nOpen your filtered vcf with `less` and page through it. It still has all the variant lines, but the FILTER column that was blank before is now filled in, with PASS or a list of the filters it failed. Note also that the filters that were run are described in the header section.\n\n#### Precomputed files {-}\n\nIf you run out of time, please click below to get the path to precomputed bam and vcf files for the GATKâ€™s best practices section.\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('Path to intermediary and final bam files: ',bpbamdir,'\\n'))\ncat(paste0('Path to intermediary and final vcf files: ',bpvcfdir,'\\n'))\n```\n\n#### Questions {-}\n\n26. Check how  many variants in total that are present in the cohort.filtered.vcf file and how many that have passed the filters. Is the difference big?\n27. Look at the variants that did not pass the filters using `grep -v 'PASS' cohort.filtered.vcf`. Try to understand why these variants didn't pass the filter.\n\n# Clean up {-}\n\nWhen the analysis is done and you are sure that you have the desired output, it is a good practice to remove intermediary files that are no longer needed. This will save disk space, and will be a crucial part of the routines when you work with your own data. Please think about which files you need to keep if you would like to go back and look at this lab later on. Remove the other files.\n\n# Answers {-}\n\nWhen you have finished the exercise, please have a look at this document with [answers to all questions](lab_vc_answers.pdf), and compare them with your answers.\n\n# SBATCH scripts {-}\n\nThis section is supplementary material intended only for those of you who want to learn how to run all steps automatically in bash scripts. Please make sure that you have understood how all the individual steps work before you start with this.\nTo learn more about SLURM and SBATCH scripts please look the [SLURM user guide](https://www.uppmax.uu.se/support/user-guides/slurm-user-guide/) on UPPMAX website.\n\n## Variant calling in cohort {-}\n\nBelow is a skeleton script that can be used as a template for running [variant calling in a cohort](#jointvc). Please modify it to run all the steps in part two of this workshop.\n\n```bash\n#!/bin/bash\n#SBATCH -A sens2022-22-123\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 1:00:00\n#SBATCH -J jointGenotyping\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  #Fill in the code for running bwa-mem for each sample here\n  #Fill in the code for samtools index for each sample here\n  #Fill in the code for HaplotypeCaller for each sample here\ndone\n#Fill in the code for CombineGVCFs for all samples here\n#Fill in the code for GenotypeGVCFs here\n\n```\nPlease save the sbatch script in your UPPMAX folder and call it \"joint_genotyping.sbatch\" or similar. Make the script executable by this command:\n\n```bash\nchmod u+x joint_genotyping.sbatch\n```\n\nTo run the sbatch script in the SLURM queue, use this command:\n\n```bash\nsbatch joint_genotyping.sbatch\n```\n\nIf you have an active node reservation you can run the script as a normal bash script:\n\n```bash\n./joint_genotyping.sbatch\n```\n\nIf you would like more help with creating the sbatch script, please look at our example solution:\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('#!/bin/bash\n#SBATCH -A ',upid,'\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 2:00:00\n#SBATCH -J jointGenotyping\n\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: \"${sample}\n  bwa mem -R \\\\\n  \"@RG\\\\tID:${sample}\\\\tPU:flowcellx_lanex\\\\tSM:${sample}\\\\tLB:libraryx\\\\tPL:illumina\" \\\\\n  -t 1 human_g1k_v37_chr2.fasta \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort > \"${sample}.bam\"\n\n  samtools index \"${sample}.bam\"\n\n  gatk --java-options -Xmx7g HaplotypeCaller \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -ERC GVCF -I \"${sample}.bam\" \\\\\n  -O \"${sample}.g.vcf\"\n\ndone\n\ngatk --java-options -Xmx7g CombineGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V HG00097.g.vcf \\\\\n-V HG00100.g.vcf \\\\\n-V HG00101.g.vcf \\\\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx7g GenotypeGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.g.vcf \\\\\n-O cohort.vcf'))\n```\n\n## GATK best practices {-}\n\nNow please try to incorporate the additional steps from [GATK's best practices](#gatk_bp) into the workflow. If you run out of time you can sneak peek at our example solution below.\n\n```{r,accordion=TRUE,echo=FALSE,comment=\"\",class.output=\"sh\"}\ncat(paste0('#!/bin/bash\n#SBATCH -A ',upid,'\n#SBATCH -p core\n#SBATCH -n 1\n#SBATCH -t 2:00:00\n#SBATCH -J BestPractise\n\n## load modules\nmodule load bioinfo-tools\nmodule load bwa/0.7.17\nmodule load samtools/1.10\nmodule load GATK/4.1.4.1\n\n# define path to reference genome\nref=\"/sw/courses/ngsintro/reseq/data/ref\"\n\n# make symbolic links\nln -s /sw/courses/ngsintro/reseq/data/ref/human_g1k_v37_chr2.fasta\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00097_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00100_2.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_1.fq\nln -s /sw/courses/ngsintro/reseq/data/fastq/HG00101_2.fq\n\n# index reference genome\nbwa index -a bwtsw human_g1k_v37_chr2.fasta\nsamtools faidx human_g1k_v37_chr2.fasta\ngatk --java-options -Xmx7g CreateSequenceDictionary \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-O human_g1k_v37_chr2.dict\n\n## loop through the samples:\nfor sample in HG00097 HG00100 HG00101;\ndo\n  echo \"Now analyzing: ${sample}\"\n  # map the reads\n  bwa mem \\\\\n  -R \"@RG\\tID:${sample}\\tPU:flowcellx_lanex\\tSM:${sample}\\tLB:libraryx\\tPL:illumina\" \\\\\n  -t 1 human_g1k_v37_chr2.fasta \\\\\n  \"${sample}_1.fq\" \"${sample}_2.fq\" | samtools sort > \"${sample}.bam\"\n\n  samtools index $sample\".bam\"\n  # mark duplicates\n  gatk --java-options -Xmx7g MarkDuplicates \\\\\n  -I \"${sample}.bam\" \\\\\n  -O \"${sample}.md.bam\" \\\\\n  -M \"${sample}_mdmetrics.txt\"\n\n  # base quality score recalibration\n  gatk --java-options -Xmx7g BaseRecalibrator \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -I \"${sample}.md.bam\" \\\\\n  --known-sites \"${ref}/1000G_phase1.snps.high_confidence.b37.chr2.vcf\" \\\\\n  -O \"${sample}.recal.table\"\n\n  gatk --java-options -Xmx7g ApplyBQSR \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -I \"${sample}.md.bam\" \\\\\n  --bqsr-recal-file \"${sample}.recal.table\" \\\\\n  -O \"${sample}.recal.bam\"\n\n  # haplotypeCaller in -ERC mode\n  gatk --java-options -Xmx7g HaplotypeCaller \\\\\n  -R human_g1k_v37_chr2.fasta \\\\\n  -ERC GVCF \\\\\n  -I \"${sample}.bam\" \\\\\n  -O \"${sample}.g.vcf\"\ndone\n\n# joint genotyping\ngatk --java-options -Xmx7g CombineGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V HG00097.g.vcf \\\\\n-V HG00100.g.vcf \\\\\n-V HG00101.g.vcf \\\\\n-O cohort.g.vcf\n\ngatk --java-options -Xmx7g GenotypeGVCFs \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.g.vcf \\\\\n-O cohort.vcf\n\n# variant filtration SNPs\ngatk --java-options -Xmx7g SelectVariants \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.vcf \\\\\n--select-type-to-include SNP \\\\\n-O cohort.snvs.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.snvs.vcf \\\\\n--filter-name QDfilter --filter-expression \"QD < 2.0\" \\\\\n--filter-name MQfilter --filter-expression \"MQ < 40.0\" \\\\\n--filter-name FSfilter --filter-expression \"FS > 60.0\" \\\\\n-O cohort.snvs.filtered.vcf\n\n# variant filtration indels\ngatk --java-options -Xmx7g SelectVariants \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.vcf \\\\\n--select-type-to-include INDEL \\\\\n-O cohort.indels.vcf\n\ngatk --java-options -Xmx7g VariantFiltration \\\\\n-R human_g1k_v37_chr2.fasta \\\\\n-V cohort.indels.vcf \\\\\n--filter-name QDfilter --filter-expression \"QD < 2.0\" \\\\\n--filter-name FSfilter --filter-expression \"FS > 200.0\" \\\\\n-O cohort.indels.filtered.vcf\n\n# merge filtered SNPs and indels\ngatk --java-options -Xmx7g MergeVcfs \\\\\n-I cohort.snvs.filtered.vcf \\\\\n-I cohort.indels.filtered.vcf \\\\\n-O cohort.filtered.vcf'))\n```\n\n# Additional information {-}\n\n* Here is a technical documentation of [Illumina Quality Scores](https://www.illumina.com/documents/products/technotes/technote_understanding_quality_scores.pdf)\n* Tools used or referenced\n  * [BWA](http://bio-bwa.sourceforge.net/bwa.shtml)\n  * [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n  * [MultiQC](http://multiqc.info/)\n  * [Picard](https://broadinstitute.github.io/picard/command-line-overview.html)\n  * [GATK](https://software.broadinstitute.org/gatk/)\n  * [samtools](http://www.htslib.org/)\n\n<!--# Alternative cluster\nThe teachers may inform you that we will use the high performance computing center NSC instead of UPPMAX during this workshop. If so, please follow this link to [run the workshop on NSC](lab_vc_tetralith.html).-->\n\n\n**Thanks for today!**\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"kable","error":false,"eval":true,"cache":null,"freeze":"auto","echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"left","fig-pos":null,"fig-env":null,"code-fold":false,"code-overflow":"wrap","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":false,"wrap":"none","default-image-extension":"png","to":"html","filters":["../../assets/custom.lua","reveal-header","lightbox"],"include-in-header":["../../assets/fonts/head.html"],"toc":true,"toc-depth":4,"number-sections":true,"output-file":"lab_vc.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.433","bibliography":["../../references.bib"],"csl":"../../apa.csl","knitr":{"opts_chunk":{"results":"hold"}},"uppmax_project":"snic2023-xx-xxxx","nsc_project":"snic2023-xx-xxxx","uppmax_res_1":"snic2023-xx-xxxx_1","uppmax_res_2":"snic2023-xx-xxxx_2","uppmax_res_3":"snic2023-xx-xxxx_3","uppmax_res_4":"snic2023-xx-xxxx_4","uppmax_res_5":"snic2023-xx-xxxx_5","location":"uppsala","assistants":["AJ: Anna Johansson","BV: BjÃ¶rn Viklund","DA: Dag Ahren","FB: Franziska Bonath","JH: Jason Hill","JB: Joakim Bygdell","JA: Juliana Assis","KL: Katarina Lejonlid","KB: Kristina Benevides","LK: Linda KÃ¶hn","LV: Louella Vasquez","MD: Martin DahlÃ¶","MG: Maxime Garcia","ML: Malin Larsson","MM: Markus Mayrhofer","NN: Nina Norgren","OVP: Olga Vinnere Pettersson","PA: Prasoon Agarwal","PP: Paul Pyl","RF: Roy Francis","VVH: Vincent van Hoef","SD: Sebastian DiLorenzo"],"schedule_message":"Coffee breaks are planned for approximately 10:00 and 14:30 every day.","colors":{"primary":"#95b540","secondary":"#E9F2D1"},"packages":{"packages_cran_student":["BiocManager","remotes","dplyr","ggplot2","pheatmap","stringr","tidyr"],"packages_bioc_student":["DESeq2","edgeR","goseq","GO.db","org.Mm.eg.db","reactome.db"],"packages_github_student":null,"packages_cran_repo":["bookdown","captioner","here","htmlTable","knitr","leaflet","lubridate","markdown","pagedown","yaml"],"packages_bioc_repo":null,"packages_github_repo":null},"quarto-required":">=1.2.2","theme":"../../assets/css/custom.scss","smooth-scroll":true,"toc-location":"right","number-depth":4,"code-copy":true,"title-block-banner":"#E9F2D1","callout-icon":false,"date":"last-modified","date-format":"DD-MMM-YYYY","lightbox":{"match":"auto"},"title":"Variant Calling","subtitle":"From reads to short variants","author":"Malin Larsson"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html","revealjs"]}